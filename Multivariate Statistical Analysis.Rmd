---
title: "R for Multivariate Statistics"
author: "Han Jingfeng"
date: "`r Sys.Date()`"
output:
  html_document: 
    fig_height: 5.625
    fig_width: 10
    toc: yes
    toc_depth: 4
    number_sections: yes
    toc_float:
      collapsed: false
      smooth_scroll: true
  word_document: 
    toc: yes
    toc_depth: 4
  pdf_document: 
    fig_height: 5.625
    fig_width: 10
    toc: yes
    toc_depth: 4
    number_sections: yes
    latex_engine: xelatex
always_allow_html: true
editor_options:
  chunk_output_type: console
CJKmainfont: Microsoft YaHei
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	echo = TRUE,
	fig.align = "center",
	fig.showtext = TRUE,
	message = FALSE,
	warning = FALSE,
	comment = "",
	options(digits = 5)
)

```

**加载程序包**
  
```{r}

# 加载程序包    
  library(pacman)  
  p_load(tidyverse, patchwork, png, jpeg, grid, install = FALSE, update = FALSE)

```
 
**获取运行环境信息**  
 
```{r eval=FALSE}

# 获取运行环境信息：以列表形式返回结果
  sessionInfo(package = NULL)
  sessionInfo(package = "car")
  
```

**参考资料**

* Zelterman, D.(2015) Applied Multivariate Statistics with R, Springer

* Wolfgang Härdle, Léopold Simar(2015), Applied Multivariate Statistical Analysis, Springer

* Everitt, Brian and Hothorn, Torsten(2011) An Introduction to Applied Multivariate Analysis with R, Springer

* Brian Sidney Everitt(2004) An R and S-PLUS Companion to Multivariate Analysis, Springer.

# 多元统计分析概述 

## 多元统计分析的目标

边际方法（marginal approach）一次仅分析一个变量，而多元数据分析（multivariate data analysis）能够在考虑变量之间相关关系的条件下同时分析多个变量。

```{r}

# 读取数据
  data(Prestige, package = "carData")
  anyNA(x = Prestige)
  
# 绘制数据缺失图
  DescTools::PlotMiss(x = Prestige, col = "red", bg = "steelblue", clust = FALSE, 
                      main = "Plot for Missing Data")

```

```{r}

# 读取数据
  attach(what = Prestige)
  
# 计算变量 education 与 prestige 的均值置信区间：边际置信区间
  library(DescTools)
  apply(X = Prestige[c(1, 4)], MARGIN = 2, FUN = MeanCI, na.rm = TRUE, trim = 0, 
        method = "classic", conf.level = 0.95, sides = "two.sided")

# 计算变量 education 与 prestige 的相关系数
  cor(x = education, y = prestige, use = "pairwise.complete.obs", method = "pearson")

# 绘制变量 education 与 prestige 的联合置信区间
  library(MVA)

  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  bvbox(Prestige[c(1, 4)], method = "robust", pch = 20, col = "blue", panel.first = grid(),
        xlab = "Education", ylab = "Prestige")
  title(main = "Bivariate Boxplot")
  rect(xleft = 10, ybottom = 43, xright = 12, ytop = 50, border = "red")
  par(opar)
  
  detach(name = Prestige)
  
```

## 多元统计分析方法

多元统计分析主要解决四个方面的问题：

* 数据的降维与结构简化：我们必须在损失数据信息与降低数据复杂程度之间做出权衡（trade-off）。也就是说，我们必须在不损失或尽量少损失数据信息的条件下，对数据中的重复信息进行合并，以实现用尽量少的变量替代原有多个变量的目的，这就是降维。降维类似于对数据进行"健康减肥"。经典降维方法包括主成分分析（Principal component analysis, PCA）和因子分析（Factor analysis）。

  - 主成分分析主要用于构造"综合指标"，以实现将原始数据进行最大程度的区分。
  
  - 因子分析旨在找出导致原始高度相关的多个变量背后的变量（公因子）。

* 归类问题：根据数据特征构造归类模式，归类问题包括：

  - 分类问题：利用历史数据寻找分类规则，从而按照分类规则将观测样例划归至某一类（Classification）。

  - 聚类问题：根据多元数据中存在的相似性或差异性将"相似"的个体聚为一类，称为聚类（Cluster）。

* 预测问题：回归与相关性分析。

* 统计推断：多元均值向量的假设检验。

## 多元统计分析简史

多元统计分析起源于十九世纪末期 Francis Galton 和 Karl Pearson 对父代特征与子代特征量化关系的研究。在二十世纪初，Charles Spearman 对智商测验关系的研究奠定了因子分析（factor analysis）的基础。此后的二十多年中，Hotelling 和 Thurstone 进一步拓展了对因子分析的研究。

多元统计分析不仅受到心理学研究的驱动，二十世纪三十年代 Fisher 提出了线性判别函数分析（linear discriminant
function analysis）以对植物的进行分类。此后，基于 Bartlett 和 Roy 的工作，Fisher 提出的方差分析也被拓展至多元领域。

多元统计分析的早期发展受制于计算工具的落后进展缓慢，主要集中于数理领域的理论探索。在上世纪五十年代，随着电子计算机的发展，多元统计分析进入了快速发展时期并被广泛应用于各行各业。

* 因子分析：用于解释关于人类行为与能力的心理学理论。

* 主成分分析：用于分析学生在同一次考试、不同科目的学习成绩。

* 典型相关分析：分析两次考试中学生成绩的关联性。

一些多元统计分析的方法是由其他科学领域的问题所驱动产生的：

* 线性判别分析：基于多个植物特征的植物分类问题。

* 多元方差分析：农业试验。

* 回归分析：遗传学与行星轨道学。

## 多元统计分析的应用领域

* 市场营销：预测新购买趋势、锁定忠实顾客、发掘潜在顾客、市场细分、精准营销。

* 银行业：基于消费者特征的贷款政策评估、预测信用卡用户流失。

* 金融行业：确定金融学指标之间的关系、跟踪投资组合的变化、预测价格的拐点、预测高频股票交易的波动特征。

* 保险行业：识别新保险购买者的特征、发现异常出现情况、锁定“高风险顾客”。

* 医疗行业：疾病的早期警示、基于病人特征预测医生诊疗、精准医疗。

* 分子生物学：基因测序、分析 DNA 微阵列、描述生物学方程、预测蛋白质构造。

* 天文学：天体编录（命名为行星、星系等）、识别天体之间的关系。

* 法务会计：识别保险诈骗、信用卡诈骗、医疗诈骗、监控偷税行为、识别股票市场内幕交易。

* 运动训练：发掘最有效的训练策略。

用数据驱动价值：Data → Information → Knowledge → Wisdom。

# 矩阵代数

## 向量的定义与运算

### 向量的定义

标量（scalar）是一个单个数值，向量（vector）是一个按照一定次序排列的有序数值集合。

* $p$ 维列向量：$p$ 行 $1$ 列，即矩阵的列数退化为 $q=1$。

$$
\boldsymbol{a}=\left(\begin{array}{c}
a_{1} \\
a_{2} \\
\vdots \\
a_{p}
\end{array}\right)
$$

* $q$ 维行向量：$1$ 行 $q$ 列，即矩阵的行数退化为 $p=1$。

$$\boldsymbol{a}^{\prime}=\boldsymbol{a}^{T}=\left(a_{1}, a_{2}, \cdots, a_{q}\right)$$

### 向量的运算

* 如果向量 $\boldsymbol{x}$ 与向量 $\boldsymbol{y}$ 含有相同的元素数量并且各个元素都相等，则称向量 $\boldsymbol{x}$ 与向量 $\boldsymbol{y}$ 相等，表示为 $\boldsymbol{x} = \boldsymbol{y}$。

* 向量的加法
 
$$\boldsymbol{x}+\boldsymbol{y}=\boldsymbol{y}+\boldsymbol{x}=\left(\begin{array}{c}
x_{1}+y_{1} \\
x_{2}+y_{2} \\
\vdots \\
x_{n}+y_{n}
\end{array}\right)$$

```{r}

# 向量的加法
  set.seed(seed = 2631)
  rnorm(n = 3, mean = 0, sd = 1) + rnorm(n = 3, mean = 1, sd = 2)

```

* 向量与标量的乘法

$$c \boldsymbol{x}=\left(\begin{array}{c}
c x_{1} \\
c x_{2} \\
\vdots \\
c x_{n}
\end{array}\right)$$

```{r}

# 向量与标量的乘法
  2 * seq(3)  

```   

* 向量的内积（inner-product）：向量的内积是一个单一数值。
  
$$\boldsymbol{x}^{\prime} \boldsymbol{y}=\sum_{i=1}^{n} x_{i} y_{i}$$

$$\boldsymbol{x}^{\prime} \boldsymbol{y}=\boldsymbol{y}^{\prime} \boldsymbol{x}$$

$$\boldsymbol{x}^{\prime}(\boldsymbol{y}+\boldsymbol{z})=\boldsymbol{x}^{\prime} \boldsymbol{y}+\boldsymbol{x}^{\prime} \boldsymbol{z}$$

  - 在 R 中，计算向量的内积使用函数 crossprod(x, y)。

* 给定一组向量 $x_1,x_2,\ldots,x_n$，存在一组不全为零的标量 $c_1,c_2,\ldots,c_n$，使得 $c_{1} \boldsymbol{x}_{1}+c_{2} \boldsymbol{x}_{2}+\cdots+c_{n} \boldsymbol{x}_{n}=\mathbf{0}$，则称向量 $x_1,x_2,\ldots,x_n$ 线性相关（mutually independent）。否则，称向量 $x_1,x_2,\ldots,x_n$ 相互独立（mutually independent）。

  - 线性相关意味着至少有一个向量是其他向量的线性组合。

  - 彼此正交的向量必然相互独立，但是相互独立的两个向量不一定是彼此正交的。
  
    - 两个向量的正交意味着两个向量的内积为零，即 $x^{\prime}{y}=0$，在几何上表现为两个向量是彼此垂直的。
  
```{r}

# 向量的内积  
  set.seed(seed = 2631)
  x <- rnorm(n = 3, mean = 0, sd = 1)
  y <- rnorm(n = 3, mean = 1, sd = 2)
  crossprod(x = x, y = y)

```  
  
### 向量的几何意义

一个向量可以有两个方面的几何意义：

* 意义 1：向量空间中的一个坐标点。

* 意义 2：向量空间中的一个既有方向又有长度的量。

具体使用哪一个几何意义，需要根据实际情况具体问题具体分析（in the context of particular problem）。

```{r}

# 向量的几何意义
  img <- readPNG(source = "Pictures/向量的几何意义.png")
  grid.raster(image = img)
  
```  
  
* 向量 $a$ 的欧式长度（Euclidean1 length，简称长度或欧式模）：向量的长度是一个单一数值。
  
$$\|\boldsymbol{x}\|=\left(\boldsymbol{x}^{\prime} \boldsymbol{x}\right)^{1/2}=\sqrt{x_{1}^{2}+x_{2}^{2}+\cdots+x_{n}^{2}}$$

  - 在二维平面上，欧氏长度也称为距离（distance）或欧式模。
  
  - 单位向量：长度为 $1$ 的向量，即 $\|\boldsymbol{a}\|=1$。

### 特殊向量

* $\boldsymbol{0}$ 向量：元素取值均为 $0$ 的向量。

```{r}

# 创建 0 向量
  
  # 方法 1：数据类型为双精度浮点型  
  O1 <- rep(x = 0, 10)

  # 方法 2：数据类型为整型
  O2 <- integer(length = 10)  

  identical(x = O1, y = O2)

``` 
  
* $\boldsymbol{1_n}$ 向量：元素取值均为 $1$ 的向量。

```{r}

# 创建 1 向量
  I <- rep(x = 1, 10) 
  I

```

## 矩阵的定义与运算

### 矩阵的定义

* $p\times{q}$ 矩阵：$p$ 行 $q$ 列

$$
\mathbf{A}=\left(\begin{array}{ccccc}
{a_{11}} & {\cdots} & {a_{1j}} & {\cdots} & {a_{1q}} \\
{\vdots} & {} & {\vdots} & {} & {\vdots} \\
{a_{i1}} & {\cdots} & {a_{ij}} & {\cdots} & {a_{iq}} \\
{\vdots} & {} & {\vdots} & {} & {\vdots} \\
{a_{p1}} & {\cdots} & {a_{pj}} & {\cdots} & {a_{pq}}
\end{array}\right)
$$

### 特殊矩阵

* 上三角矩阵：对角线以下的元素均为 $0$ 的方阵。

$$\boldsymbol{A}=\left(\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 p} \\
0 & a_{22} & \cdots & a_{2 p} \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & a_{p p}
\end{array}\right)$$

```{r}

# 创建上三角矩阵
  A <- matrix(data = 1:9, nrow = 3, byrow = FALSE)
  
  # 方法 1
  matrixcalc::upper.triangle(x = A)

  # 方法 2
  A[lower.tri(x = A, diag = FALSE)] <- 0
  
# 检验是否为上三角矩阵
  Matrix::isTriangular(object = matrixcalc::upper.triangle(x = A), upper = TRUE)

```

* 下三角矩阵：对角线以上的元素为 $0$ 的方阵。

$$\boldsymbol{A}=\left(\begin{array}{cccc}
a_{11} & 0 & \cdots & 0 \\
a_{21} & a_{22} & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
a_{p 1} & a_{p 2} & \cdots & a_{p p}
\end{array}\right)$$

```{r}

# 创建下三角矩阵
  A <- matrix(data = 1:9, nrow = 3, byrow = FALSE)
  
  # 方法 1
  matrixcalc::lower.triangle(x = A)
  
  # 方法 2
  A[upper.tri(x = A, diag = FALSE)] <- 0

# 检验是否为下三角矩阵
  Matrix::isTriangular(object = matrixcalc::lower.triangle(x = A), upper = FALSE)
  
```

* 对角矩阵：仅是对角线上的元素不全为 $0$ 的矩阵，或者说非对角线元素全为零的矩阵，对角矩阵一定是个方阵。

  - $p$ 阶方阵：$p\times{p}$ 矩阵，$p$ 行 $p$ 列。  

$$\boldsymbol{A}=\left(\begin{array}{cccc}
a_{11} & 0 & \cdots & 0 \\
0 & a_{22} & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & a_{p p}
\end{array}\right)$$

```{r}

# 创建对角矩阵
  diag(x = 1:3) 

# 检验是否为对角阵
  Matrix::isDiagonal(object = diag(x = 1:3))

```

* 单位矩阵：对角矩阵中对角线上的元素全为 $1$ 的矩阵，记为 $\boldsymbol{I}$。

$$\boldsymbol{I}=\boldsymbol{I}_{p}=\left(\begin{array}{cccc}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & 1
\end{array}\right)$$

```{r}

# 创建单位阵
  diag(x = 3)
  Matrix::diag(x = 3)

```

* 零矩阵：矩阵中的全部元素均为 $0$ 的矩阵。

```{r}

# 创建零矩阵
  
  # 方法 1：数据类型为双精度浮点型
  Q1 <- matrix(data = rep(x = 1, 9), nrow = 3)
  
  # 方法 2：数据类型为整型
  Q2 <- matrix(data = integer(length = 9), nrow = 3)
  
```

* 对称矩阵：矩阵 $A$ 为方阵，满足 $A^{\prime}=A$，即矩阵的转置还是其本身。  

```{r}

# 创建对称矩阵
  A_sys <- matrix(data = c(2, 3, 5, 3, 0, 1, 5, 1, 1), nrow = 3, byrow = TRUE)

# 检验矩阵是否为对称矩阵

  # 方法 1
  identical(x = t(x = A_sys), y = A_sys)

  # 方法 2
  matrixcalc::is.symmetric.matrix(x = A_sys)

  # 方法 3
  Matrix::isSymmetric(object = A_sys)

```
  
* 创建 Hilbert 矩阵

```{r}

# 创建 Hilbert 矩阵

  # 方法 1
  Matrix::Hilbert(n = 3)
  
  # 方法 2
  matrixcalc::hilbert.matrix(n = 3)
  
  near(x = Matrix::Hilbert(n = 3), y = matrixcalc::hilbert.matrix(n = 3))

```

* 创建斐波那契矩阵

```{r}

# 创建斐波那契矩阵：维度为 n + 1 的方阵
  matrixcalc::fibonacci.matrix(n = 3)

```

### 矩阵的运算

* 如果矩阵 $A$ 和 $B$ 具有相同的维度，那么 $A+B$ 就是两个矩阵对应元素的相加。

* 常数 $c$ 与 $A$ 的积就是常数 $c$ 与矩阵 $A$ 中每个元素的乘积。

* 如果矩阵 $A$ 的列数等于矩阵 $B$ 的行数，那么矩阵 $A$ 与 $B$ 的乘法就等于：

$$\boldsymbol{AB}=\left(\sum_{k=1}^{q} a_{ik} b_{kj}\right)$$。

* 在 R 中，计算矩阵的乘法使用操作符 %*%。 
  
```{r}

# 矩阵的乘法
  A_sys %*% t(x = A_sys)

```

### 矩阵的运算法则

* $(\boldsymbol{A}+\boldsymbol{B})^{\prime}=\boldsymbol{A}^{\prime}+\boldsymbol{B}^{\prime}$

* $(\boldsymbol{A B})^{\prime}=\boldsymbol{B}^{\prime} \boldsymbol{A}^{\prime}$

* $\boldsymbol{A}\left(\boldsymbol{B}_{1}+\boldsymbol{B}_{2}\right)=\boldsymbol{A} \boldsymbol{B}_{1}+\boldsymbol{A} \boldsymbol{B}_{2}$

* $\boldsymbol{A}\left(\sum_{i=1}^{k} \boldsymbol{B}_{i}\right)=\sum_{i=1}^{k} \boldsymbol{A} \boldsymbol{B}_{i}$

* $c(\boldsymbol{A}+\boldsymbol{B})=c \boldsymbol{A}+c \boldsymbol{B}$  

### 正交矩阵

如果矩阵 $A$ 为方阵且 $A{A^\prime}=I$，那么矩阵 $A$ 就称为正交矩阵。

正交矩阵的三个等价定义：

* $$A{A^\prime}=I \iff {A^\prime}=A^{-1} \iff {A^\prime}A=I$$
 
* 正交矩阵的行列式要么等于 $1$，要么就等于 $-1$。

* 正交矩阵的每一行或每一列均为正交单位向量，即正交矩阵的行或列的长度均为 $1$。

当 $A$ 为正交矩阵时，$y=Ax$ 就称对矩阵 $x$ 做了正交变换，正交变换在几何上意味着对原 $p$ 维坐标系做了刚性旋转或称正交旋转。

```{r}

# 正交矩阵
  img <- readPNG(source = "Pictures/正交矩阵.png")
  grid.raster(image = img)

```

## 矩阵的行列式、逆和秩

### 行列式

* 矩阵 $A$ 必须为方阵才能计算该矩阵的行列式。

* 行列式的基本性质

  - 如果矩阵 $A$ 的某行（或列）为零，则 $\mid A\mid=0$。

  - $\mid A^\prime \mid=\mid A\mid$，矩阵转置后的行列式不变。
  
  - 如果矩阵 $A$ 的某一行（或列）是其他一些行（或列）的线性组合，则矩阵 $A$ 的行列式为零。
  
  - 如果矩阵 $A$ 为上三角矩阵、下三角矩阵或对角矩阵，则矩阵 $A$ 的行列式为其对角线元素之积。
  
  - 如果矩阵 $A$ 与矩阵 $B$ 均为 $p$ 阶方阵，则 $\mid{AB}\mid=\mid{A}\mid\mid{B}\mid$。
  
* 创建矩阵

  - 使用 base 中的函数 matrix。
  
  - 使用 base 中的函数 as.matrix 将 R 对象转换为矩阵。
  
  - 使用 base 中的函数 rbind 或 cbind 将向量按行或按列将向量合并为矩阵。
  
* 计算矩阵的行列式使用 base 中的函数 det。
  
```{r}

# 创建矩阵：使用函数 matrix
  A <- matrix(data = c(6, 0, 2, 3, 1, 4), nrow = 2, byrow = TRUE)

# 创建矩阵：使用函数 rbind
  A <- rbind(c(1, 2, 3, 4, 5), 
             c(2, 4, 7, 8, 9), 
             c(3, 7, 10, 15, 20), 
             c(4, 8, 15, 30, 20), 
             c(5, 9, 20, 20, 40))

# 计算矩阵 A 的行列式
  det(x = A)

```

### 矩阵的逆

如果矩阵 $A$ 为方阵且行列式不为零，就称矩阵 $A$ 为非退化矩阵或非奇异矩阵，否则就称矩阵 $A$ 为退化矩阵或奇异矩阵。

如果矩阵 $A$ 满足 $A{A^{-1}}=I$，就称矩阵 $A^{-1}$ 为矩阵 $A$ 的逆矩阵。

* 只有非退化矩阵或非奇异矩阵才有逆矩阵。

* 矩阵的逆矩阵是唯一的。

逆矩阵的基本性质

* $\boldsymbol{A} \boldsymbol{A}^{-1}=\boldsymbol{A}^{-1} \boldsymbol{A}=\boldsymbol{I}$

* $\left(A^{\prime}\right)^{-1}=\left(A^{-1}\right)^{\prime}$

* 若 $\boldsymbol{A}$ 和 $\boldsymbol{C}$ 均为 $p$ 阶非退化方阵, 则 $(AC)^{-1}=C^{-1} A^{-1}$

* $\left|\boldsymbol{A}^{-1}\right|=|\boldsymbol{A}|^{-1}$

* 若 $\boldsymbol{A}$ 是正交矩阵, 则 $A^{-1}=A^{\prime}$

* 若 $\boldsymbol{A}=\operatorname{diag}\left(a_{11}, a_{22}, \cdots, a_{p p}\right)$ 非退化（即 $a_{ii} \neq 0, i=1,2, \cdots, p$），则

$$\boldsymbol{A}^{-1}=\operatorname{diag}\left(a_{11}^{-1}, a_{22}^{-1}, \cdots, a_{p p}^{-1}\right)$$

计算矩阵的逆矩阵

* 使用 base 中的函数 solve。

* 使用 matrixcalc 中的函数 matrix.inverse。

```{r}

# 计算矩阵 A 的逆矩阵
  
  # 方法 1  
  solve(a = A)
  MASS::fractions(x = solve(a = A))

  # 方法 2
  matrixcalc::matrix.inverse(x = A)
  MASS::fractions(x = A %*% matrixcalc::matrix.inverse(x = A))

```

### 矩阵的秩

线性相关: 一组同维向量 $\boldsymbol{a}_{1}, \boldsymbol{a}_{2}, \cdots, \boldsymbol{a}_{n}$, 若存在不全为零的常数 $c_{1}, c_{2}, \cdots, c_{n}$，使得

$$c_{1} \boldsymbol{a}_{1}+c_{2} \boldsymbol{a}_{2}+\cdots+c_{n} \boldsymbol{a}_{n}=\mathbf{0}$$

线性无关：一组同维向量 $\boldsymbol{a}_{1}, \boldsymbol{a}_{2}, \cdots, \boldsymbol{a}_{n}$, 当且仅当常数 $c_{1}, c_{2}, \cdots, c_{n}$ 全为零时，使得

$$c_{1} \boldsymbol{a}_{1}+c_{2} \boldsymbol{a}_{2}+\cdots+c_{n} \boldsymbol{a}_{n}=\mathbf{0}$$

行秩: 矩阵 $\boldsymbol{A}$ 中线性无关的行向量的最大数目。

列秩: 矩阵 $\boldsymbol{A}$ 中线性无关的列向量的最大数目。

* 行秩和列秩必然相等, 统称为矩阵的秩, 记作 $\operatorname{rank}(\boldsymbol{A})$。

矩阵秩的基本性质：

* $\operatorname{rank}(\boldsymbol{A})=0 \Leftrightarrow \boldsymbol{A}=\mathbf{0}$。

* 若 $\boldsymbol{A}$ 为 $p \times q$ 矩阵, 且 $\boldsymbol{A} \neq \mathbf{0}$, 则 $1 \leq \operatorname{rank}(\boldsymbol{A}) \leq \min \{p, q\}$。

* $\operatorname{rank}(A)=\operatorname{rank}\left(A^{\prime}\right)$。

* 若 $\boldsymbol{A}$ 和 $\boldsymbol{C}$ 为非退化方阵, 则 

$$\operatorname{rank}(\boldsymbol{ABC})=\operatorname{rank}(\boldsymbol{B})$$

* $p$ 阶方阵 $\boldsymbol{A}$ 是非退化的 $\Leftrightarrow \operatorname{rank}(\boldsymbol{A})=p$ (称作 $\boldsymbol{A}$ 满秩）。

* $\operatorname{rank}\left(\boldsymbol{A} \boldsymbol{A}^{\prime}\right)=\operatorname{rank}\left(\boldsymbol{A}^{\prime} \boldsymbol{A}\right)=\operatorname{rank}(\boldsymbol{A})$。
  
计算矩阵的秩使用 matrixcalc 中的函数 matrix.rank。

```{r}

# 计算矩阵的秩
  matrixcalc::matrix.rank(x = A)  
  Matrix::rankMatrix(x = A)

```

## 矩阵的特征根、特征向量和迹

### 矩阵的特征根与特征向量

在多元统计分析中，常常需要对高维数据进行降维。

* 为了确定数据降维后原始数据信息保留了多少，这就涉及到了特征根。

* 在数据的降维过程中，为了确定数据的投影方向是什么，这就涉及到了特征向量。
 
设 $A$ 是一个 $p$ 阶方阵，若对于一个数 $\lambda$，存在 $x\neq 0$，使得 $Ax=\lambda x$，则称 $\lambda$ 为方阵 $A$ 的一个特征根，而称 $x$ 为 $A$ 的属于 $\lambda$ 的一个特征向量。

* 对于任意的方阵，如果该方阵存在特征根和特征向量，那么特征根与特征向量的个数就是无穷多。因此，一般情况下取特征向量 $x_i$ 为单位向量，即满足 $\rVert{x_i}\rVert=1$。

* $A$ 有 $p$ 个特征根（可能有相同的），记作 $\lambda_1,\lambda_2,\cdots,\lambda_p$（可以为复数），相应的特征向量分别为 $x_1,x_2,\cdots,x_p$。

特征根和特征向量的基本性质：

* $A$ 和 $A^\prime$ 有相同的特征根。

* 若 $A$ 和 $B$ 分别是 $p\times q$ 和 $q\times p$ 矩阵，则 $AB$ 和 $BA$ 有相同的非零特征根。

* 若 $A$ 为实对称矩阵，则 $A$ 的特征根全为实数，$p$ 个特征根按大小依次表示为 $\lambda_1\geq\lambda_2\geq,\cdots,\geq\lambda_p$。若 $\lambda_i\neq\lambda_j$，则相应的特征向量 $x_i$ 与 $x_j$ 必正交，即 $x_i^\prime x_j=0$。

* 若 $A$ 为对角阵，那么 $A$ 对角线上的元素就是 $A$ 的特征根，特征根所对应的特征向量分别为 $\boldsymbol{e}_{1}=(1,0, \cdots, 0)^{\prime}, \quad \boldsymbol{e}_{2}=(0,1,0, \cdots, 0)^{\prime}, \cdots, \boldsymbol{e}_{p}=(0, \cdots, 0,1)^{\prime}$

* $|\boldsymbol{A}|=\prod_{i=1}^{p} \lambda_{i}$，即矩阵的行列式等于矩阵特征根之积。

* 特征向量正交向量，由特征向量构成的矩阵是正交矩阵。

计算矩阵的特征根和特征向量使用程序包 base 中函数 eigen。

### 矩阵的迹

设 $A$ 为 $p$ 阶方阵，则 $A$ 的迹（trace）定义为 $A$ 的对角线元素之和。

* 特征根或特征根之和等于矩阵的迹，特征根或特征根之积等于矩阵的行列式。

* 若 $A$ 为投影矩阵，则 $\text {tr}(A)=\text {rank}(A)$。

  - 若方阵 $A$ 满足 $A^2=A$ 就称 $A$ 为幂等矩阵，对称的幂等矩阵就称为投影矩阵。
  
计算矩阵的迹使用程序包 matrixcalc 中的函数 matrix.trace。

```{r}

# 计算矩阵的各行之和
  rowSums(x = A, na.rm = TRUE)
  matrixStats::rowSums2(x = A, na.rm = TRUE, rows = 1:nrow(x = A))
  
# 计算矩阵的各列之和
  colSums(x = A, na.rm = TRUE)  
  matrixStats::colSums2(x = A, na.rm = TRUE, cols = 1:ncol(x = A))

# 计算矩阵中所有元素之和
  sum(A, na.rm = TRUE)
  matrixStats::sum2(x = A, na.rm = TRUE)

```

```{r}

# 计算矩阵的行均值
  
  # 计算矩阵中所有行的均值
  rowMeans(x = A, na.rm = TRUE)
  matrixStats::rowMeans2(x = A, na.rm = TRUE, rows = 1:nrow(x = A))  

  # 计算矩阵中第 1、3、5 行的均值
  matrixStats::rowMeans2(x = A, rows = c(1, 3, 5), na.rm = TRUE)

  # 计算分块矩阵中第 1、3、5 行和列的均值
  matrixStats::rowMeans2(x = A, rows = c(1, 3, 5), cols = c(1, 3, 5), na.rm = TRUE)

```  

```{r}

# 计算矩阵的列均值

  # 计算矩阵中所有列的均值  
  colMeans(x = A, na.rm = TRUE)
  matrixStats::colMeans2(x = A, na.rm = TRUE, rows = 1:nrow(x = A))

  # 计算矩阵中第 1、3、5 列的均值
  matrixStats::colMeans2(x = A, na.rm = TRUE, cols = c(1, 3, 5))

  # 计算分块矩阵中第 1、3、5 行和列的均值
  matrixStats::colMeans2(x = A, rows = c(1, 3, 5), cols = c(1, 3, 5), na.rm = TRUE)

```

```{r}

# 计算矩阵的维度
  dim(x = A)  

# 计算矩阵的行数
  nrow(x = A)  
  
# 计算矩阵的列数
  ncol(x = A)

# 计算矩阵的转置
  t(x = A)

# 判断矩阵是否为正交矩阵
  A %*% t(x = A)
  det(x = A)
  
```

```{r}

# 计算矩阵的特征根和特征向量
  eigens <- eigen(x = A, only.values = FALSE)

  # 提取矩阵的特征根
  eigens$values  

  # 提取矩阵的特征向量
  eigens$vectors

# 计算矩阵的迹
  matrixcalc::matrix.trace(x = A)
  sum(diag(x = A), na.rm = TRUE)

# 验证矩阵特征根的和等于矩阵的迹
  near(x = sum(eigens$values), y = matrixcalc::matrix.trace(x = A))

# 验证矩阵特征根的积等于矩阵的行列式
  near(x = prod(eigens$values, na.rm = TRUE), y = det(x = A))

# 验证特征向量是单位向量
  for (i in 1:nrow(x = eigens$vectors)) {
    print(x = sum(eigens$vectors[i, ]^2, na.rm = TRUE), digits = 4)
  }
  
  for (i in 1:ncol(x = eigens$vector)) {
    print(x = sum(eigens$vectors[, i]^2, na.rm = TRUE), digits = 4)
  }
 
# 验证特征向量之间彼此正交
  library(MASS)
  
  q <- ncol(x = eigens$vectors)
  
  for (i in 1:q) {
    for (j in (i + 1):q) {
      if (i + 1 > q) {
        print("finished")
      } else print(
        x = fractions(x = as.vector(x = t(eigens$vectors[, i]) %*% eigens$vectors[, j]))
        )
    }
  }

```

## 正定矩阵、半正定矩阵

设 $A$ 是对称矩阵，则定义：  

* 二次型：$x\prime{A}{x}$，其中 $x$ 是一个向量。

* 正定矩阵：如果对一切 $x\neq 0$，都有 $x^\prime{A}{x}>0$，就称 $A$ 为正定矩阵，记作 $A>0$。

* 半正定矩阵：如果对一切 $x$，都有 $x^\prime{A}{x}\geq 0$，就称 $A$ 为半正定矩阵，记作 $A\geq 0$。

正定矩阵的性质

* 如果 $A$ 为正定矩阵，那么 $A$ 的特征根均大于零、$A$ 是正交矩阵、$A^{-1}$ 也是正定矩阵。

* 如果 $A$ 为半正定矩阵，那么 $A$ 的秩等于 $A$ 的正特征根的个数。

```{r}

# 判断矩阵是否为正定矩阵
  matrixcalc::is.positive.definite(x = A)
  
# 判断矩阵是否为半正定矩阵
  matrixcalc::is.positive.semi.definite(x = A)  
  
# 判断矩阵是否为负定矩阵
  matrixcalc::is.negative.definite(x = A)  

# 判断矩阵是否为半负定矩阵
  matrixcalc::is.negative.semi.definite(x = A)  

# 验证矩阵是否为幂等矩阵
  all.equal(target = A %*% A, current = A)

```

## 向量和矩阵的几何理解

### 向量的距离

对于 $\boldsymbol{x}, \boldsymbol{y} \in \mathbb{R}^{p}$，若函数 $d(\boldsymbol{x}, \boldsymbol{y})$ 满足：

$$\begin{array}{l}
d(\boldsymbol{x}, \boldsymbol{y})>0, \quad \boldsymbol{x} \neq \boldsymbol{y}, \\
d(\boldsymbol{x}, \boldsymbol{y})=0, \quad \text { 当且仅当 } \boldsymbol{x}=\boldsymbol{y}, \\
d(\boldsymbol{x}, \boldsymbol{y}) \leq d(\boldsymbol{x}, \boldsymbol{z})+d(\boldsymbol{z}, \boldsymbol{y}), \quad \forall \boldsymbol{x}, \boldsymbol{y}, \boldsymbol{z}
\end{array}$$

就称 $d(\boldsymbol{x}, \boldsymbol{y})$ 为距离。

如果 $\boldsymbol{A}$ 为正定矩阵，称 

$$d(\boldsymbol{x}, \boldsymbol{y})=\sqrt{(\boldsymbol{x}-\boldsymbol{y})^{T} A(\boldsymbol{x}-\boldsymbol{y})}$$

为一个欧式距离。最简单情形是 $A=I_p$，此时 $d(\boldsymbol x, \boldsymbol y) = \sqrt{\sum_{j=1}^p (x_j - y_j)^2}$。

### 向量的模

$\|\boldsymbol x \| = \sqrt{\boldsymbol x^T \boldsymbol x}$ 对应的欧式距离为 $\|\boldsymbol x - \boldsymbol y\|$。

对正定阵 $\boldsymbol{A}$，$\| \boldsymbol x \|_A = \sqrt{\boldsymbol x^T A \boldsymbol x}$ 对应的欧式距离为 $\| \boldsymbol x - \boldsymbol y \|_A$。

### 向量的夹角

令 $\boldsymbol x \neq 0, \boldsymbol y \neq 0$，夹角 $\theta$ 为：

$$\theta = \cos^{-1} \frac{\boldsymbol x^T \boldsymbol y} {\| \boldsymbol x \| \, \| \boldsymbol y \|}$$

### 坐标轴的旋转变换

对于二维坐标

$$\Gamma = \left(\begin{array}{cc}
      \cos\theta & \sin\theta \\
      -\sin\theta & \cos\theta
    \end{array}\right)$$

逆时针旋转弧度 $\theta$ 的旋转变换为 $\boldsymbol y = \Gamma \boldsymbol x$。

对于 $\boldsymbol{x} \in \mathbb{R}^{p}$，$\Gamma$ 为正交阵，$\boldsymbol y = \Gamma \boldsymbol x$ 称为旋转和反射变换。

### 投影阵

令 $X$ 为 $n \times p$ 的矩阵，如果 $$P = X X^+, Q = I_n - P$$，则 $P,Q$ 是正交投影阵。

## R 矩阵运算常用函数汇总 
  
* 创建矩阵：matrix

* 矩阵转置: t

* 创建对角阵: diag

* 矩阵乘法: A %*% B

* 叉积 crossprod、外积 outer

* 行列式 det

* 逆矩阵与解线性方程组：solve

* 广义逆：MASS::ginv

* 特征值和特征向量：eigen

* 奇异值分解：svd

# 多元数据的图形方法

在多元统计分析中，我们常常使用统计图形回答以下问题：

* 某些分量是否比其它分量更为分散，有没有显示分组的分量，分量中有没有异常值，数据分布的正态性如何。

* 数据中是否存在线性组合。

* 查看单个变量的具体分布形态，如直方图、核密度图、盒形图、散点图、茎叶图等。

* 查看变量之间的两两散点图，如散点图矩阵。 

## 一元数据图形

### 一元数据散点图

```{r}

# 绘制一元数据散点图：方法 1
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1) 

  set.seed(seed = 2631)
  plot(rnorm(n = 100, mean = 0, sd = 1), type = "p", pch = 20, panel.first = grid(),
       col = "blue")
  par(opar)

```
  
```{r}

# 绘制一元数据散点图：方法 2
  set.seed(seed = 2631)
  mydata <- data.frame(x = 1:100, y = rnorm(n = 100, mean = 0, sd = 1))
      
  ggplot(data = mydata, mapping = aes(x = x, y = y)) +
    geom_point(color = "blue")

```  

### 茎叶图

以“茎”为坐标轴，每个“叶子”代表一个观测。

茎叶图主要查看数据的分布情况、检查数据中的错误以及识别数据中的异常值。

并列茎叶图（背靠背茎叶图）可用于比较两个变量的分布差异。

绘制茎叶图使用函数 stem。

```{r}

# 读取数据
  data("cd4", package = "boot")
  anyNA(x = cd4)
  attach(what = cd4)
  
# 绘制变量 baseline 的茎叶图
  stem(x = baseline)

  detach(name = cd4)

```

**并列茎叶图**

并列茎叶图又称背对背茎叶图，可以比较两个变量的分布差异。 

* 绘制并列茎叶图使用 aplpack 中的函数 stem.leaf.backback。

```{r}

# 读取数据
  attach(what = cd4)

# 绘制并列茎叶图
  aplpack::stem.leaf.backback(x = baseline, y = oneyear)

  detach(name = cd4)

```  

### 直方图

直方图能够反映连续型数据的分布形态，其特点是：

* 窗宽不同则图形也不同。

* 起始点不同图形可能不同。

* 分在同一窗体内的观测样例的具体区值不起作用，损失信息。

* 估计的密度形状是不连续的。

绘制直方图使用函数 hist。
  
```{r}

# 绘制直方图：方法 1
  set.seed(seed = 2631)
  xnorm <- rnorm(n = 100, mean = 0, sd = 1)

  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1) 
  hist(x = xnorm, breaks = "fd", col = "steelblue", border = "gray", panel.first = grid(),
       labels = TRUE, main = "Frequency Histogram")
  box()
  par(opar)

```

```{r}

# 绘制直方图：方法 2
  results <- hist(x = xnorm, breaks = "fd", plot = FALSE)  
  k <- results$breaks
  
  ggplot(mapping = aes(x = xnorm)) +
    geom_histogram(breaks = k, color = "black", fill = "darkgreen") +
    stat_bin(geom = "text", breaks = k, label = results$counts, color = "blue",
             vjust = -0.5) +
    labs(x = "Random Variable", y = "Frequency", title = "Histogram") +
    theme_bw()

```
 
```{r}

# 绘制直方图：方法 3
  library(simplevis)
  gg_histogram(data = data.frame(x = xnorm), x_var = x, pal = "blue", alpha_fill = 0.5, 
               alpha_line = 1, size_line = 1, x_expand = c(0.2, -0.2), 
               x_title = "Random Variable", y_title = "Frequency", title = "Histogram") +
    ylim(0, 15) +
    theme_bw()

```

```{r}

# 绘制直方图：方法 4
  library(lattice)
  histogram(x = xnorm, breaks = "fd", type = "count", col = "darkgreen", border = "blue",
            xlab = "Random Variable", ylab = "Frequency", main = "Histogram") 
  
```

```{r}

# 绘制直方图：方法 5
  library(lessR)
  mydata <- data.frame(x = xnorm)
  
  Histogram(x = x, data = mydata, breaks = "fd", color = "black", fill = "steelblue", 
            values = TRUE, xlab = "Random Variable", ylab = "Frequency", main = "Histogram")

```

### 核密度图

核密度图是连续光滑的曲线，核密度图能够反映连续型数据分布的位置、分散程度、偏斜方向、是否多峰、分布尾部等情况。

核密度图可以推广到多元的情况，但主要是二元和三元。更多变量时空间变得稀疏，估计很不稳定。

在 R 中，需要首先使用函数 density 计算出核密度估计值，然后使用函数 plot 将 density 计算的核密度估计值画成密度曲线。

```{r}

# 绘制核密度图：方法 1 
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  dens <- density(x = xnorm)  
  plot(dens, panel.first = grid(), zero.line = FALSE, col = "gray", 
       xlab = "Ramdom Variable", ylab = "Density", main = "Density Plot")
  polygon(dens, col = "steelblue")
  rug(x = xnorm, col = "blue")
  par(opar)

``` 
 
```{r}

# 绘制核密度图：方法 2
  ggplot(mapping = aes(x = xnorm)) +
    geom_density(fill = "steelblue") +
    labs(x = "Ramdom variable", y = "Density", title = "Density Plot")

```  

```{r}

# 绘制核密度图：方法 3
  library(simplevis)
  gg_density(data = as.data.frame(x = xnorm), x_var = xnorm, pal = "steelblue", 
             alpha_fill = 1, title = "Density Plot") +
    labs(x = "Ramdom Variable", y = "Density", title = "Density Plot")

```  

```{r}

# 绘制核密度图：方法 4
  library(car)

  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  densityPlot(x = xnorm, grid = TRUE, col = "blue", lty = 1, lwd = 2,
              method = "adaptive", xlab = "Random Variable", ylab = "Density",
              main = "Density Plot")
  par(opar)

``` 

```{r}

# 绘制核密度图：方法 5
  library(lattice)
  densityplot(x = xnorm, col = "blue", pch = 20, xlab = "Random Variable", 
              ylab = "Density", main = "Density Plot")

```

```{r}

# 绘制核密度图：方法 6
  GGally::ggally_densityDiag(data = mydata, mapping = aes(x = x), color = "black", 
                             fill = "steelblue") +
    xlim(-3, 5) +
    labs(x = "Random Variable", y = "Density", title = "Density Plot")

```

### 直方图 + 核密度图

```{r}

# 绘制直方图 + 核密度图：方法 1
  library(rafalib)
  
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  shist(z = xnorm, plotHist = TRUE, col = "blue", lty = 1, lwd = 2, unit = 0.5,
        panel.first = grid(), xlab = "Random Variable", ylab = "Frequency", 
        main = "Histogram & Density Plot")
  box()
  par(opar)

```

```{r}

# 绘制直方图 + 核密度图：方法 2
  ggplot(mapping = aes(x = xnorm)) +
    geom_histogram(mapping = aes(y = ..density..), binwidth = 0.5, color = "black",
                   fill = "steelblue") +
    geom_density(color = "blue", size = 1) +
    labs(x = "Random Variable", y = "Density", title = "Histogram & Density Plot")

```

```{r}

# 绘制直方图 + 核密度图：方法 3
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  hist(x = xnorm, breaks = "fd", freq = FALSE, col = "steelblue", border = "black", 
       panel.first = grid(), ylim = c(0, 0.45),
       xlab = "Random Variable", ylab = "Frequency", main = "Histogram & Density Plot")
  lines(x = dens, col = "blue", lwd = 2)
  box()
  par(opar)
  
```

### 箱线图

箱线图能够反映数据发分布的分散与集中情况，主要用于对多个组别数据分布情况的比较。

绘制箱线图使用函数 boxplot。

```{r}

# 读取数据
  attach(what = cd4)  

# 绘制箱线图：方法 1
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  boxplot(x = baseline, horizontal = TRUE, col = "darkgreen", range = 1.5, notch = FALSE, 
          outline = TRUE, xlab = "Value", ylab = "Baseline")
  par(opar)
  
  detach(name = cd4)

```

```{r}

# 读取数据
  attach(what = cd4)  

# 绘制箱线图：方法 2
  ggplot(data = cd4, mapping = aes(x = baseline)) +
    geom_boxplot(fill = "steelblue") +
    ylim(-4, 4) +
  theme_bw()

  detach(name = cd4)

```

```{r}

# 读取数据
  attach(what = cd4)  

# 绘制箱线图：方法 3
  library(lattice)
  bwplot(x = baseline, box.ratio = 0.2)  

  detach(name = cd4)
  
```

```{r}

# 读取数据
  attach(what = cd4)  

# 绘制箱线图：方法 4
  library(car)

  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  Boxplot(y = baseline, col = "steelblue", ylab = "Baseline")
  par(opar)
  
  detach(name = cd4)
  
```

```{r}

# 读取数据
  attach(what = cd4)  

# 绘制箱线图：方法 5
  library(lessR)
  lessR::Plot(x = baseline, data = cd4, vbs_plot = "b", xlab = "Baseline")
  
  detach(name = cd4)
  
```

分组箱线图  

```{r}

# 读取数据
  attach(what = iris)  

# 绘制分组箱线图：方法 1
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 0)
  boxplot(Sepal.Length ~ Species, data = iris, col = rainbow(n = 3), horizontal = TRUE,
          pch = 20, ylab = "Occupation")
  par(opar)
  
  detach(name = iris)

```
  
```{r}

# 读取数据
  attach(what = iris)  

# 绘制分组箱线图：方法 2    
  ggplot(data = iris, mapping = aes(x = Sepal.Length, fill = Species)) +
    geom_boxplot(outlier.colour = "red") +
    ylim(-0.5, 0.5)
  
  detach(name = iris)

```  

```{r}

# 读取数据
  attach(what = iris)  

# 绘制分组箱线图：方法 3    
  library(car)
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 0)
  Boxplot(Sepal.Length ~ Species, data = iris, col = "steelblue", pch = 20, id = TRUE, 
          xlab = "Species", ylab = "Sepal Length")
  par(opar)
  
  detach(name = iris)
  
```
  
```{r}

# 读取数据
  attach(what = iris)  

# 绘制分组箱线图：方法 4
  library(lattice)
  bwplot(Sepal.Length ~ Species, data = iris, box.ratio = 0.25, horizontal = FALSE, 
         pch = 20, xlab = "Species", ylab = "Sepal Length")  

  detach(name = iris)

```

## 二元数据图形

### 散点图

```{r}

# 读取数据
  attach(what = iris)

# 绘制散点图：方法 1
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  plot(x = Sepal.Length, y = Sepal.Width, type = "p", pch = 20, col = "blue",
       panel.first = grid(), xlab = "Sepal Length", ylab = "Sepal Width",
       main = "Scatterplot for Sepal Length vs. Sepal Width")
  par(opar)

  detach(name = iris)

```

```{r}

# 读取数据
  attach(what = iris)

# 绘制散点图：方法 2
  ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) +
    geom_point(color = "blue") +
    labs(x = "Sepal Length", y = "Sepal Width", 
         title = "Scatterplot for Sepal Length vs. Sepal Width")
  
  detach(name = iris)
  
```
  
散点图的曲线拟合

```{r}

# 读取数据
  attach(what = cd4)

# 绘制散点图 + 平滑曲线
  p <- ggplot(data = cd4, mapping = aes(x = baseline, y = oneyear)) +
    geom_point(color = "blue") 
  
  p1 <- p +
    geom_smooth(formula = y ~ x, method = "loess", se = TRUE, level = 0.9)
  
  p2 <- p +
    geom_smooth(formula = y ~ x, method = "loess", se = TRUE, level = 0.99)
  
  p1 + p2 + plot_layout(nrow = 2)

  detach(name = cd4)

```
  
### 二元盒形图
  
二元盒形图（bivariate boxplot）的实质就是散点图与凸壳（凸多边形）的结合。
  
二元盒形图主要用于展示两个连续型变量之间的相关关系以及识别数据中的潜在的异常值。

在二元盒型图中的两条直线都是回归线，其中：

* 由短线和点构成的直线是使用普通最小二乘法对所有观测样例拟合的回归线。

* 由点绘制的直线是剔除异常值后使用普通最小二乘法绘制的稳健回归线。

如果两个连续型变量服从联合二元正态分布的话，二元盒形图的内椭圆包含 $50\%$ 的数据点，外椭圆包含 $95\%$ 的数据点。    

绘制二元盒形图使用程序包 MVA 中的函数 bvbox，该函数能够绘制基于二元正态分布的二元盒形图。

```{r}

# 读取数据
  attach(what = cd4)

# 绘制二元盒形图
  library(MVA)

  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  bvbox(a = cd4, method = "robust", pch = 20, col = "blue", cex = 2, panel.first = grid(),
        xlab = "Baseline", ylab = "Oneyear", 
        main = "Bivariate Boxplot for Baseline vs. Oneyear")
  par(opar)
  
  detach(name = cd4)

```   

```{r}

# 读取数据
  data("USairpollution", package = "HSAUR2")
  anyNA(x = USairpollution)
  attach(what = USairpollution)
  
# 绘制二元盒形图
  library(MVA)
  library(car)

  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 0)
  
  bvbox(a = USairpollution[3:4],  pch = 20, col = "blue", cex = 1.5, panel.first = grid(),
        xlab = "Number of manufacturing enterprises", ylab = "Population size", 
        main = "Bivariate Boxplot")
  showLabels(x = manu, y = popul, labels = rownames(x = USairpollution), method = "x", 
             n = 4, col = "red", cex = 1, location = "lr")
  par(opar)
  
  detach(name = USairpollution)

```

### 凸壳图
  
凸壳图（convex hull）也称凸多边形图，有些类似于使用最少的纸张来包裹一个不规则的物体，物体在任何方向上的极值点决定了包裹的最终形状。在数学上，凸壳是指涵盖全部数据点的最小外凸多边形。

凸壳图也是用于展示两个连续型变量之间的相关关系以及识别数据中的潜在的异常值。

在 R 中，函数 chull 能够返回数据中在各个方向上的极值索引位置。

```{r}

# 读取数据
  attach(what = cd4)

# 绘制凸壳图
  library(aplpack)
  
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plothulls(cd4, pch = 20, col = "red", panel.first = grid(), cex = 1.5, lty = 1, lwd = 2,
            xlab = "Baseline", ylab = "Oneyear", main = "")
  title(main = "Convex Hull")
  par(opar)
  
  detach(name = cd4)

```

```{r}

# 读取数据
  attach(what = USairpollution)
  
# 绘制凸壳图
  library(aplpack)

  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 0)
  
  plothulls(x = manu, y = popul, pch = 20, col = "red", panel.first = grid(), cex = 1.5,
            xlab = "Number of manufacturing enterprises", ylab = "Population size", 
            main = "")
  title(main = "Convex Hull")

  detach(name = USairpollution)

```

### 洋葱图
  
对洋葱图（也称多层凸多边形）的最好理解就是将洋葱图视为一个多层蛋糕，位于内层的凸壳就是蛋糕的顶层，然后逐层向外扩展。面积越小、层次越高的凸壳，含有越大的数据密度。

```{r}

# 读取数据
  attach(what = cd4)

# 绘制洋葱图
  library(aplpack)
  
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  nlev <- 5
  mycolors <- heat.colors(n = 9)[3:(nlev + 2)]
  
  plothulls(x = cd4, type = "n", n.hull = nlev, col.hull = mycolors, lty.hull = 1:nlev, 
            density = NA, col = 0, panel.first = grid(), 
            xlab = "Baseline", ylab = "Oneyear", main = "")
  points(x = cd4, pch = 16, cex = 0.7, col = "blue")
  title(main = "Five peeling convex hulls")
  
  par(opar)
  
  detach(name = cd4)

``` 
  
```{r}

# 读取数据
  attach(what = USairpollution)

# 绘制洋葱图
  library(aplpack)
  
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 0)
  
  nlev <- 5
  mycolors <- heat.colors(n = 9)[3:(nlev + 2)]
  
  plothulls(x = USairpollution[3:4], type = "n", n.hull = nlev, col.hull = mycolors, 
            lty.hull = 1:nlev, density = NA, col = 0, panel.first = grid(), main = "",
            xlab = "Number of manufacturing enterprises", ylab = "Population size")
  points(x = USairpollution[3:4], pch = 16, cex = 1, col = "steelblue")
  title(main = "Five peeling convex hulls")
  par(opar)
  
  detach(name = USairpollution)

``` 

### 口袋图
  
在口袋图（bag plot）中含有二元数据的散点图，外层凸多边形（口袋）是剔除极端值后的凸壳，内层包络是含有 $50\%$ 观测值的凸壳。外壳上的数据点与二元中位数用直线相连，连线延伸至内壳。

口袋图能够识别数据中的潜在异常值。

绘制口袋图使用程序包 aplpack 中的函数 bagplot。

```{r}

# 读取数据
  attach(what = cd4)
  
# 绘制口袋图
  library(aplpack)

  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  bagplot(cd4, factor = 3, na.rm = TRUE, panel.first = grid(), col.loophull = "steelblue", 
          col.baghull = "lightblue", col.bagpoints = "blue", show.outlier = TRUE,
          xlab = "Baseline", ylab = "Oneyear", main = "Bag Plot for Baseline vs. Oneyear")
  box()
  par(opar)
  
  detach(name = cd4)

```  

```{r}

# 读取数据：模拟的服从多元正态分布数据
  library(mvtnorm)
  set.seed(seed = 2631)
  mydata <- rmvnorm(n = 100, mean = c(0, 0), sigma = rbind(c(1 ,0.7), c(0.7, 1)))

# 绘制口袋图
  library(aplpack)
  
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 0)
  
  bagplot(mydata, factor = 3, na.rm = TRUE, panel.first = grid(), col.loophull = "steelblue", 
          col.baghull = "lightblue", col.bagpoints = "blue", show.outlier = TRUE,
          xlab = "X", ylab = "Y", main = "Bag Plot for Simulated Data")
  box()
  par(opar)
  
``` 

### 二元联合密度图

二元数据作为二元随机向量的观测值，其联合密度估计是二元函数，需要使用曲面表示。

绘制二元联合密度曲面，使用 ggplot2 中的函数 stat_density_2d 或函数 geom_density2d。

```{r}

# 读取数据
  data("banknote", package = "mclust")
  anyNA(x = banknote)
  attach(what = banknote)

# 绘制二元联合密度曲面
  
  # 方法 1
  p <- ggplot(data = banknote, mapping = aes(x = Top, y = Diagonal)) +
    geom_point(color = "blue") +
    theme_bw()
  
  p1 <- p +
    stat_density_2d(color = "red") 

  # 方法 2
  p2 <- p +
    geom_density_2d(color = "red")
  
  p1 / p2

  detach(name = banknote)

```

程序包 KernSmooth 中的函数 bkde2D 可以对二元数据估计分布密度曲面，然后使用使用函数 contour 绘制曲面。

```{r}

# 读取数据
  attach(what = banknote)

# 估计二元数据的联合密度
  library(KernSmooth)
  mydata <- banknote[6:7]
  n <- nrow(x = mydata)
  dens <- bkde2D(x = mydata, 
                 bandwidth = 1.06 * c(sd(x = mydata[[1]]), sd(x = mydata[[2]])) * n^(-1/5))

# 绘制二元联合密度曲面
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 0)
  
  contour(x = dens$x1, y = dens$x2, z = dens$fhat, panel.first = grid(), 
          col = rainbow(n = 11), xlab = "Top", ylab = "Diagobal", main = "二元密度曲面")  
  
  par(opar)
  
  detach(name = banknote)
  
```

```{r}

# 读取数据
  attach(what = banknote)

# 绘制二元联合密度曲面
  ggplot(data = banknote, mapping = aes(x = Top, y = Diagonal)) +
    geom_density2d_filled(show.legend = FALSE) +
    xlim(8, 13) +
    ylim(137.5, 143) +
    labs(title = "二元密度曲面") +
    theme_minimal()

  detach(name = banknote)

```

### 蜂窝图

蜂窝图是一种二元直方图，它使用六边形来表示两个连续型变量所围成的区域，用颜色的深浅来表示六边形内数据点的密集程度。在蜂窝图中，六角形大小类似于直方图中的窗宽。

在蜂窝图中，由于已经占用了 $x$ 方向和 $y$ 方向，所以六边形内数据频数的大小可以用灰度深浅或者颜色渐变来表示。

绘制蜂窝图使用 ggplot2 中的函数 geom_hex，或使用程序包 hexbin 中的函数 hexbinplot。

```{r}

# 读取数据
  attach(what = diamonds)  

# 绘制蜂窝图：方法 1
  p1 <- ggplot(data = diamonds, mapping = aes(x = carat, y = price)) +
    geom_hex()

  p2 <- p1 + 
    scale_fill_gradient(low = "#9696F2", high = "#0A0A3D")
  
  p1 / p2
  
  detach(name = diamonds)

```
  
```{r}

# 读取数据
  attach(what = diamonds)

# 绘制蜂窝图：方法 2
  library(hexbin)  
  
  hexbinplot(price ~ carat, data = diamonds, style = "colorscale", colorkey = TRUE, 
             trans = sqrt, inv = function(x) x^2, aspect = 1, border = "gray",
             xlab = "Carat", ylab = "Price", main = "Hex plot for Diamond Carat vs. Price")
  
  hexbinplot(price ~ carat, data = diamonds, style = "nested.centroids", colorkey = TRUE, 
             trans = sqrt, inv = function(x) x^2, aspect = 1, border = NULL,
             xlab = "Carat", ylab = "Price", main = "Hex plot for Diamond Carat vs. Price")

  detach(name = diamonds)

```

## 三元变量的图形

### 三维散点图
  
程序包 rgl 支持三维动态图像，能够在独立的窗口中显示可以拖动旋转的三维散点图。

```{r}

# 读取数据
  attach(what = iris)

# 绘制三维散点图：方法 1
  library(rgl)
  plot3d(x = Sepal.Length, y = Sepal.Width, z = Petal.Length, type = "p", size = 5, 
         col = rainbow(n = 9)[4:6][as.numeric(x = factor(x = Species))], 
         xlab = "Sepal Length", ylab = "Sepal Width", zlab = "Petal Length", aspect = 1)
  
  detach(name = iris)

```

```{r}

# 读取数据
  attach(what = iris)

# 绘制三维散点图：方法 2
  library(car)
  scatter3d(x = Sepal.Length, y = Sepal.Width, z = Petal.Length, bg.col = "white", 
            surface.col = "gray", square.col = "black", grid.col = "orange",
            point.col = rainbow(n = 9)[4:6][Species], 
            xlab = "Sepal Length", ylab = "Sepal Width", zlab = "Petal Length")
  
  detach(name = iris)

```

```{r}

# 读取数据
  attach(what = Prestige)

# 三维散点图：方法 3
  library(scatterplot3d)
  p <- scatterplot3d(x = income, y = education, z = prestige, angle = 55, pch = 20,
                     color = rainbow(n = 9)[4:6][type], grid = TRUE, type = "p", 
                     col.grid = "steelblue", box = TRUE,  highlight.3d = FALSE, 
                     main = "3D Scatter Plot with Regression Plane",
                     xlab = "Income", ylab = "Education", zlab = "Prestige")
  model <- lm(prestige ~ income + education, data = Prestige)
  p$plane3d(model, col = "blue", draw_polygon = TRUE)

  detach(name = Prestige)

``` 

```{r}

# 读取数据
  attach(what = iris)

# 三维散点图：方法 4
  library(lattice)
  cloud(Sepal.Length ~ Petal.Length * Petal.Width, data = iris, pch = 20,
        groups = Species, screen = list(z = 20, x = -70, y = 2), pretty = TRUE,
        key = list(title = "Species", x = 0.05, y = 1, corner = c(0, 1), border = FALSE, 
                   points = Rows(x = trellis.par.get("superpose.symbol"), which = 1:3), 
                   text = list(levels(iris$Species)), cex = 0.8))
  
  detach(name = iris)
  
``` 

### 气泡图

```{r}

# 读取数据
  States <- tibble::rownames_to_column(.data = as.data.frame(x = state.x77),
                                       var = "states")
  States <- dplyr::rename(.data = States, "LifeExp" = "Life Exp")
  States <- mutate(.data = States, state.region = state.region)
  attach(what = States)

# 绘制气泡图：方法 1
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 0)
  
  plot(x = Income, y = LifeExp, type = "n", panel.first = grid(),
       xlab = "Income", ylab = "Life expectancy", main = "Bubble Plot for US States")
  
  colors <- c("olivedrab1", "orange", "green", "mediumturquoise", "mediumorchid2",
              "firebrick1")
  symbols(x = Income, y = LifeExp, circles = Population, add = TRUE, 
          bg = colors[state.region], fg = "gray50")
  showLabels(x = Income, y = LifeExp, labels = state.region, method = "mahal", n = 4, 
             cex = 1, col = "black", location = "lr")
  legend("topright", legend = levels(x = state.region), fill = colors[1:4], cex = 0.8, 
         ncol = 2, title = "Region")
  
  detach(name = States)
  
```

```{r}

# 读取数据
  attach(what = States)

# 绘制气泡图：方法 2
  library(DescTools)  
  library(RColorBrewer)

  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 0)
 
  colors <- c("olivedrab1", "orange", "green", "mediumturquoise", "mediumorchid2",
              "firebrick1")
  PlotBubble(x = Income, y = LifeExp, area = Population, cex = 0.0001, border = "gray50",
             col = colors[state.region], panel.first = grid(), xlab = "Income", 
             ylab = "Life expectancy", main = "Bubble Plot for US States")
  legend("topright", legend = levels(x = state.region), fill = colors[1:4], cex = 0.8, 
         ncol = 2, title = "Region")
  showLabels(x = Income, y = LifeExp, labels = state.region, method = "mahal", n = 4, 
             cex = 1, col = "black", location = "lr")

  detach(name = States)

``` 

```{r}

# 读取数据
  attach(what = USairpollution)

# 绘制气泡图：方法 3
  ggplot(data = USairpollution, mapping = aes(x = temp, y = wind, size = SO2)) +
    geom_point(alpha = 0.5) +
    labs(x = "Average annual temperature (Fahrenheit)", 
         y = "Average annual wind speed (m.p.h.)", 
         title = "Bubble Plot for US Air Pollution")

  detach(name = USairpollution)

```

```{r}

# 读取数据
  library(gapminder)
  mydata <- gapminder %>% 
    filter(year == 2007) %>% 
    dplyr::select(-year)

# 绘制气泡图：方法 4
  library(viridis)
  library(hrbrthemes)

  mydata %>%
    arrange(desc(x = pop)) %>%
    mutate(country = factor(country, country)) %>%
    ggplot(aes(x = gdpPercap, y = lifeExp, size = pop, fill = continent)) +
    geom_point(alpha = 0.5, shape = 21, color = "black") +
    scale_size(range = c(0.1, 24), name = "Population (M)") +
    scale_fill_viridis(discrete = TRUE, guide = FALSE, option = "A") +
    theme_ipsum() +
    theme(legend.position = "bottom") +
    ylab("Life Expectancy") +
    xlab("GDP per Capita") +
    theme(legend.position = "none")

```

R 中的数据集 quakes 是 Fiji 附近 1964年以来 $1000$ 个震级 $MB>4.0$ 的地震观测数据。

```{r}

# 读取数据
  data("quakes", package = "datasets")

# 绘制气泡图：方法 5
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  plot(x = quakes[["long"]], y = quakes[["lat"]], type = "n", panel.first = grid(),
       xlab = "Longitude",  ylab = "Latitude", main = "Bubble Plot for Fiji Earthquakes")
  symbols(x = quakes[["long"]], y = quakes[["lat"]], circles = exp(x = quakes[["mag"]]),
          inches = 0.2, add = TRUE, lwd = 1, fg = "orange", bg = "gray")
  par(opar)

``` 

### kriging 方法拟合曲面

kriging 方法以南非矿业工程师 Daniel Gerhardus Krige 的名字命名，他提出了 kriging 方法，该方法是一种数学平滑函数。

kriging 方法可用于曲面拟合。

```{r eval=FALSE}

# 读取数据
  attach(what = Jtemp)

# 用 Kriging 方法拟合三维曲面
  library(MASS)
  library(spatial)

  Longe <- Long - 180

  # 使用 spatial 中的函数 surf.ls 拟合多项式曲面，第一参数是多项式次数
  alt.kr <- surf.ls(np = 4, x = Longe, y = Lat, z = Alt)

  # 使用 spatial 中的函数 trmat 计算多项式的网格点：设置图形边界
  altsur <- trmat(obj = alt.kr, xl = -120, xu = -70, yl = 20, yu = 50, n = 50)
  
  # 使用 MASS 中的函数 eqscplot 产生纵横比为 1 的空图形
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  eqscplot(x = altsur, ratio = 1, type = "n", panel.first = grid(), cex.lab = 1.5,
           xlab = "Longitude, east",  ylab = "Latitude")
  
  # 使用函数 contour 向空图形叠加拟合曲面，即等高线
  contour(x = altsur, levels = c(0, 1000, 2000, 4000), col = "blue", add = TRUE)
  
  # 向图形中添加数据点
  points(x = Longe, y = Lat, pch = 16, col = "red")
  
  detach(name = Jtemp)
  
```    

## 多元数据的图形

### 散点图矩阵

```{r}

# 读取数据
  attach(what = iris)
  
# 绘制散点图矩阵：方法 1
  pairs(x = iris[, -5], pch = 20, col = "blue", cex.main = 0.8, cex.labels = 1,
        main = "Scatterplot Matrix for IRIS")
  
  detach(name = iris)

``` 

```{r}

# 读取数据
  attach(what = iris)

# 绘制散点图矩阵：方法 2
  library(car)
  
  opar <- par(no.readonly = TRUE)
  par(cex.main = 0.8)
  scatterplotMatrix(x = iris, smooth = FALSE, regLine = FALSE, groups = Species,
                    by.groups = TRUE, legend = TRUE, cex.labels = 0.8,
                    main = "Scatterplot Matrix for IRIS by Species")
  par(opar)
  
  detach(name = iris)
  
```

```{r}

# 读取数据
  attach(what = iris)

# 绘制散点图矩阵：方法 3
  library(scatterPlotMatrix)
  scatterPlotMatrix(data = iris[, -5])
  
  detach(name = iris)
  
```

```{r}

# 读取数据
  attach(what = iris)
  
# 综合图：使用程序包 GGally 中的函数 ggpairs
  library(GGally)
  ggpairs(data = iris,
          columns = 1:4, # 使用第 1 ~ 4 变量
          title = "Plots Matrix", # 可更改图片标题
          axisLabels = "show", # 默认 show, 展示坐标轴；还可以使用 none, internal
          columnLabels = c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width"), # 更改变量名
          aes(colour = Species, alpha = 0.9), # {ggplot2} 中 aes()，按 type 分类
          upper = list(continuous = wrap("cor", size = 3.5, displayGrid = TRUE)), # 相关系数
          lower = list(continuous = wrap("smooth", alpha = 0.3, size = 1.5)), # 平滑曲线
          diag = list(continuous = "density")) + # 对角线
    scale_color_manual(values = c("indianred", "darkgreen", "slateblue")) + # 自定义散点图的颜色
    scale_fill_brewer(palette = 1) + # 自定义对角线直方图的颜色
    theme_bw() # 主题
	
  detach(name = iris)
  
```

### 相关系数图

```{r}

# 读取数据
  attach(what = iris)

# 绘制相关系数图：方法 1
  library(corrplot)

  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4))
  corrplot(corr = cor(x = iris[-5]), method = "ellipse", type = "full", diag = TRUE,
           bg = "gray", addCoef.col = "red", addgrid.col = "white", number.cex = 0.8)
  par(opar)
  
  detach(name = iris)
  
```  

```{r}

# 读取数据
  attach(what = iris)

# 绘制相关系数图：方法 2
  library(corrgram)
  corrgram(x = iris[1:4], type = "data", cor.method = "pearson", cex.labels = 1, 
           lower.panel = panel.shade, upper.panel = panel.ellipse, text.panel = panel.txt,  
           diag.panel = panel.minmax, main = "相关系数图")
  
  corrgram(x = iris[1:4], type = "data", cor.method = "pearson", cex.labels = 1, 
           lower.panel = panel.shade, upper.panel = panel.pie, diag.panel = panel.density,
           text.panel = panel.txt, main = "相关系数图")
  
  detach(name = iris)
  
```

```{r}

# 读取数据
  attach(what = iris)

# 绘制相关系数的网络图：图中红线表示相关系数为负值
  library(qgraph)
  
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  corr.mat <- cor(iris[-5], use = "pairwise.complete.obs", method = "pearson")
  qgraph(input = corr.mat, cut = 0.3, details = TRUE, posCol = "steelblue")
  par(opar)
  
  detach(name = iris)
  
```
 
### 口袋图矩阵

绘制口袋图矩阵使用 aplpack 中的函数 bagplot.pairs。

```{r}

# 读取数据
  attach(what = iris)

# 绘制口袋图矩阵
  library(aplpack)
  invisible(x = bagplot.pairs(dm = iris, factor = 3, gap = 0, col.baghull = "green",
                              col.loophull = "steelblue", trim = 0, numeric.only = TRUE, 
                              cex = 0.8, pch = 20, col.looppoints = "blue", 
                              col.bagpoints = "black", main = ""))
  title(main = "Bagplot Matrix", outer = TRUE, line = -1, cex.main = 0.8)

  detach(name = iris)
  
``` 

### 协同图

协同图（coplot）展示的是在某变量的不同取值条件时，另外两个变量之间相关关系的变化情况。

绘制协同图使用函数 coplot。

```{r}

# 读取数据
  attach(what = quakes)  

# 绘制协同图
  coplot(lat ~ long | depth, data = quakes, pch = 20, col = "blue", columns = 3, 
         xlab = "Longitude", ylab = "Latitude")

  detach(name = quakes)

``` 

### Perspective Plots

曲面图选取适当视角可以直观表现二元函数曲面。 

在 R 中，绘制视角图使用 graphics 中的函数 persp。

```{r}

# 创建数据
  x <- seq(-1.95, 1.95, length.out = 30)
  y <- seq(-1.95, 1.95, length.out = 35)
  z <- outer(x, y, function(a, b) a*b^2)
  nrz <- nrow(x = z)
  ncz <- ncol(x = z)

# 生成颜色
  jet.colors <- colorRampPalette(colors = c("blue", "green"))
  nbcol <- 100
  mycolor <- jet.colors(n = nbcol)
  
# 计算 z 值
  zfacet <- z[-1, -1] + z[-1, -ncz] + z[-nrz, -1] + z[-nrz, -ncz]
  
# 根据 z 值排列颜色
  facetcol <- cut(zfacet, nbcol)
  
# 绘制视角图
  par(bg = "gray")
  persp(x, y, z, col = mycolor[facetcol], phi = 30, theta = -30)
  title(main = "Perspective Plot", cex = 0.8)

``` 

```{r}

# 创建数据
  x <- seq(-10, 10, length.out = 30)
  y <- x
  
  f <- function(x, y) {r <- sqrt(x^2 + y^2); 10 * sin(r)/r}
  z <- outer(x, y, f)
  z[is.na(z)] <- 1
  
  par(bg = "gray")
  persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = "lightblue", 
        main = "Perspective Plot")
  persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = "lightblue",
        ltheta = 120, shade = 0.75, ticktype = "detailed", xlab = "X", ylab = "Y", 
        zlab = "Sinc(r)", main = "Perspective Plot") -> res
  round(x = res, digits = 4)
  
``` 

### 星象图

星象图（star plots）是针对每个观察样例在每个变量上的观测数值进行绘图，如果数据框中含有 $n$ 个观察样例，那么就需要绘制 $n$ 幅星象图。在绘制星象图时，如果数据集有 $p$ 个数值型变量，星象图将圆面平分为角度相等的 $p$ 个扇形，然后用半径大小表示每个变量的大小。

* 星象图要求观测值的数据必须是非负的。

* 星象图与蛛网图和雷达图在本质是一致的。 

* 绘制星象图使用函数 stars(X, draw.segments = FALSE, key.loc = NULL,...)

```{r}

# 读取数据
  attach(what = mtcars)

# 绘制星象图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8)
  mydata <- mtcars[1:12, ]
  stars(x = mydata, full = TRUE, scale = TRUE, radius = TRUE, nrow = 3, xlim = c(1, 13), 
        ylim = c(1, 8), draw.segments = TRUE, labels = rownames(x = mydata), 
        key.loc = c(12, 4), key.xpd = TRUE, main = "Star Plots for Motor Cars")
  par(opar)
  
  detach(name = mtcars)

```

```{r}

# 读取数据
  expenditures <- rio::import(file = "Data/expenditures.xlsx")
  head(x = expenditures, n = 3)
  
# 对数据按照 Total 降序排序
  expenditures_new <- arrange(.data = expenditures, desc(x = Total))
  
# 绘制星相图 
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8)
  mydata <- expenditures_new[c(1:6, 26:31), ]
  stars(x = mydata[2:9], full = TRUE, scale = TRUE, radius = TRUE, nrow = 3, 
        xlim = c(1, 13), ylim = c(1, 8), draw.segments = TRUE, labels = mydata$Region, 
        key.loc = c(12, 4), key.xpd = TRUE, main = "Star Plots for Expenditures")
  par(opar)
  
```
 
### 雷达图

雷达图（radar plots）也是针对每个观察样例在每个变量上的观测数值进行绘图。

绘制雷达图时，首先把圆面均分为角度相等的 $p$ 个扇形，然后用 $p$ 个半径线段的长度表示每个变量的大小，并将相邻的半径端点相连。 

绘制雷达图也是使用函数 stars，但需要设置参数 draw.segments = FALSE。

```{r}

# 读取数据
  attach(what = USairpollution)

# 绘制雷达图：方法 1
  library(radarchart)
  chartJSRadar(scores = USairpollution, showLegend = TRUE, addDots = TRUE, labelSize = 10,
               labs = rownames(x = USairpollution), 
               main = "Radar Chart for US Air Pollution")
  
  detach(name = USairpollution)

```

```{r}

# 读取数据
  USairpollution_new <- add_column(.data = USairpollution, 
                                   states = rownames(x = USairpollution))
  attach(what = USairpollution_new)

# 绘制雷达图：方法 2
  library(ggiraphExtra)
  ggRadar(data = USairpollution_new[1:3, ], mapping = aes(group = states)) +
    ggtitle(label = "Radar Chart for US Air Pollution") +
    theme_bw()
  
  detach(name = USairpollution_new)

```

```{r}

# 读取数据
  attach(what = USairpollution_new)
   
# 绘制雷达图：方法 3
  library(radarBoxplot)
  radarBoxplot(states ~ ., data = USairpollution_new[1:6, ], use.ggplot2 = TRUE,
               mfrow = c(2, 1), medianLine = list(col = "red")) +
    ggtitle(label = "Radar Chart for US Air Pollution")
  
  detach(name = USairpollution_new)

```

```{r}

# 读取数据
  attach(what = USairpollution_new)
   
# 绘制雷达图：方法 4
  stars(x = USairpollution_new[1:9, ], draw.segments = FALSE, full = TRUE, nrow = 3, 
        key.loc = c(9.5, 5), mar = c(2, 0, 1, 2), col.stars = rainbow(n = 9), 
        main = "Radar Chart for US Air Pollution", cex.main = 0.8)
  
  detach(name = USairpollution_new)
  
``` 

无论是星象图、雷达图还是气泡图都属于符号图（symbol）或称 glyph plots，这类图形的显著特点是使用符号的参数来表示数据的取值。

* 例如，在气泡图中，气泡的大小就是由某个变量的取值决定的。

### 脸谱图
  
脸谱图是 Chernooff.H 于 1973 年首次提出的，它是将每个变量用人脸型的某一部位的形状或大小来表示，这样利用 $p$ 个变量的数值就可以勾画出一个人的脸谱，从而用于展示每个观测样例的数据分布特征。

通过分析不同观测样例脸谱图之间的差异，能够反映出各个观测样例之间存在的差异。

切尔诺夫脸谱图对核实以下两点很有帮助：

* 根据专业知识和直觉提出初始的聚类。

* 使用聚类算法得到最终的聚类。

绘制脸谱图使用程序包 aplpack 中的函数 faces。

数据中各列的取值分别表现在:

* 脸高度

* 脸宽度

* 脸型

* 嘴高度

* 嘴宽度

* 微笑形状

* 眼睛高度

* 眼睛宽度

* 头发高度

* 头发宽度

* 头发发型

* 鼻子高度

* 鼻子宽度

* 耳朵宽度

* 耳朵高度

各部分颜色由部分变量的均值决定:

* $7,8$ 变量的均值决定瞳孔的颜色

* $1,2,3$ 变量的均值决定嘴唇的颜色

* $14,15$ 变量的均值决定耳朵的颜色

* $12,13$ 变量的均值决定鼻子的颜色

* $9,10,11$ 变量的均值决定头发的颜色

* $1,2$ 变量的均值决定脸孔的颜色

```{r}

# 读取数据
  attach(what = expenditures_new)

# 绘制脸谱图
  opar <- par(no.readonly = TRUE)
  par(cex.main = 0.8)
  
  # 仅绘制脸谱线，不填充颜色
  aplpack::faces(expenditures_new[, -1], face.type = 0, fill = TRUE, ncol.plot = 6, 
                 labels = Region, cex = 0.8, 
                 main = "Chernoff Faces for Expenditures of China")
  
  # 绘制脸谱线并填充颜色
  aplpack::faces(expenditures_new[, -1], face.type = 1, fill = TRUE, ncol.plot = 6, 
                 labels = Region, cex = 0.8,
                 main = "Chernoff Faces for Expenditures of China")
  
  # 绘制填充颜色的圣诞老人
  aplpack::faces(expenditures_new[, -1], face.type = 2, fill = TRUE, ncol.plot = 6, 
                 labels = Region, cex = 0.8,
                 main = "Chernoff Faces for Expenditures of China")
  par(opar)
  
  detach(name = expenditures_new)
  
```
  
### Andrews 曲线
  
对于多元数据的每个观测样例，把观测样例的各个分量的值当作一个傅立叶级数的系数。各列一般应归一化到 $[0,1]$ 之间。 

设第 $i$ 个观测样例的各个分量为 $(X_{i,1}, X_{i,2}, \dots, X_{i,p})$，定义对应于第 $i$ 个观测样例的傅立叶级数函数为：
  
$$f_{i}(t)=\left\{\begin{array}{cc}
X_{i, 1} \frac{\sqrt{2}}{2}+X_{i, 2} \sin (t)+X_{i, 3} \cos (t) \\
+\cdots+X_{i, p-1} \sin \left(\frac{p-1}{2} t\right)+X_{i, p} \cos \left(\frac{p-1}{2} t\right), & \text { 当 } p \text { 为奇数 } \\
X_{i, 1} \frac{\sqrt{2}}{2}+X_{i, 2} \sin (t)+X_{i, 3} \cos (t) & \\
+\cdots+X_{i, p} \sin \left(\frac{p}{2} t\right), & \text { 当 } p \text { 为偶数 }
\end{array}\right.$$
  
Andrews 曲线的形状与分量的次序有关。此外，如果观测样例太多会导致图形非常杂乱，所以观测样例数最好控制在 $20$ 个之内。 

```{r}

# 读取数据
  attach(what = expenditures)  
  mydata <- expenditures[1:4, -c(1, 10)]
  
# 绘制 Andrews 曲线：方法 1，笛卡尔坐标系
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 0)
  MSG::andrews_curve(x = mydata, panel.first = grid(), col = 1:5, 
                     main = "Andrews' Curves")
  legend("topleft", legend = expenditures[1:4, 1], lty = 1, lwd = 1, col = 1:5, 
         title = "图例", ncol = 2, cex = 0.8)
  par(opar)
  
  detach(name = expenditures)  

```

```{r}

# 读取数据
  attach(what = expenditures)  

# 绘制 Andrews 曲线：方法 2，极坐标系
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 0)
  pracma::andrewsplot(A = as.matrix(x = mydata), f = 1:5, style = "pol", scaled = FALSE)
  legend("topleft", legend = expenditures[1:4, 1], lty = 1, lwd = 1, col = 1:5, 
         title = "图例", ncol = 2, cex = 0.8)  
  par(opar)
  
  detach(name = expenditures) 
  
```  

### 平行坐标图

平行坐标图（parallel coordinate plot，PCP）是对多元数据进行可视化展示的常用方法。

平行坐标图是针对多元数据中的 $n$ 个观测样例的 $p$ 个变量进行绘图，图中的每一条线表示一个观测样例，每一个轴表示一个变量。在绘制平行坐标图时不需要对变量进行标准化变换，但是会经常使用对数变换。

平行坐标图的缺点在于，在数据非常密集时可能会显得过于杂乱，导致难以辨认。因此，绘制平行坐标图时应该仅针对感兴趣的观测对象和变量进行绘图。

在平行坐标图中，平行线意味着正相关，相交线意味着负相关。

绘制平行坐标图使用 MASS 中的函数 parcoord 或 GGally 中的函数 ggparcoord。
  
```{r}

# 读取数据
  attach(what = iris)

# 平行坐标图：方法 1，使用程序包 MASS 中的函数 parcoord
  library(MASS)

  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 0)
  parcoord(x = iris[-5])
  par(opar)

  detach(name = iris)

```

```{r}

# 读取数据
  attach(what = iris)

# 平行坐标图：方法 2，使用程序包 GGally 中的函数 ggparcoord
  library(GGally)
  ggparcoord(data = iris, columns = 1:4, groupColumn = "Species", scale = "std", 
                   title = "Parallel Coordinate Plot for Iris")
  ggparcoord(data = iris, columns = 1:4, groupColumn = "Species", scale = "uniminmax", 
                   title = "Parallel Coordinate Plot for Iris")

  detach(name = iris)

```

# 多元分布及其推断
  
## 一元正态分布  

### 生成服从正态分布的随机数

```{r}

# 生成服从正态分布的随机数：以下三条命令完全等价

  # 方法 1
  set.seed(seed = 2631)
  xnorm.1 <- rnorm(n = 100, mean = 0, sd = 1)

  # 方法 2
  library(distributions3)
  set.seed(seed = 2631)
  xnorm.2 <- random(x = Normal(mu = 0, sigma = 1), n = 100)

  # 方法 3
  library(mosaic)
  set.seed(seed = 2631)
  xnorm.3 <- dpqrdist(dist = "norm", type = "r", n = 100, mean = 0, sd = 1)

```

### 一元正态分布的概率密度  

一元标准正态分布 $N(0,1)$ 的密度函数：$$\phi(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac12 x^2}, \ x \in \mathbb R$$

```{r}

# 绘制不同均值的正态分布概率密度曲线：方法 1
  library(pdplot2)
  k <- seq(-5, 5, len = 100)
  pdplot2(x = k, mean = c(-1, 0, 1), sd = 1, dist = "normal", type = "PDF", 
          color = 1:3, show_color = TRUE) +
    labs(x = "Random variable", y = "Probability Density", 
         title = "Probability Density Plot for Normal Distribution") +
    theme_bw()

```

```{r}

# 绘制不同均值的正态分布概率密度曲线：方法 2
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  curve(expr = dnorm(x = x, mean = 0, sd = 1), from = -5, to = 5, col = "red",
        panel.first = grid(), xlab = "Random variable", ylab = "Probability Density", 
        main = "Probability Density Plot for Normal Distribution")
  curve(expr = dnorm(x = x, mean = 1, sd = 1), from = -5, to = 5, col = "blue", 
        add = TRUE) 
  curve(expr = dnorm(x = x, mean = -1, sd = 1), from = -5, to = 5, col = "black", 
        add = TRUE) 
  legend("topright", legend = paste("mean = ", c(0, -1, 1), " sd = 1", sep = ""), 
         lty = 1, lwd = 1, col = c("red", "blue", "black"), title = "图例")
  par(opar)
  
```

```{r}

# 绘制不同均值的正态分布概率密度曲线：方法 3
  mydata <- data.frame(x = seq(-5, 5, len = 100))
  ggplot(data = mydata, mapping = aes(x = x)) + 
    stat_function(fun = dnorm, args = list(mean = 0, sd = 1), geom = "line", 
                  color = "red") +
    stat_function(fun = dnorm, args = list(mean = -1, sd = 1), geom = "line", 
                  color = "blue") +
    stat_function(fun = dnorm, args = list(mean = 1, sd = 1), geom = "line", 
                  color = "black") +
    labs(x = "Random variable", y = "Probability Density", 
         title = "Probability Density Plot for Normal Distribution") +
    theme_bw()
  
```

```{r}

# 绘制不同标准差的正态分布概率密度曲线：方法 1
  library(pdplot2)
  k <- seq(-5, 5, len = 100)
  pdplot2(x = k, mean = 0, sd = c(0.5, 1, 2), dist = "normal", type = "PDF", 
          color = 1:3, show_color = TRUE) +
    labs(x = "Random variable", y = "Probability Density", 
         title = "Probability Density Plot for Normal Distribution") +
    theme_bw()

```

```{r}

# 绘制不同标准差的正态分布概率密度曲线：方法 2
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  curve(expr = dnorm(x = x, mean = 0, sd = 1), from = -5, to = 5, col = "red", 
        ylim = c(0, 0.8), panel.first = grid(), 
        xlab = "Random variable", ylab = "Probability Density", 
        main = "Probability Density Plot for Normal Distribution")
  curve(expr = dnorm(x = x, mean = 0, sd = 0.5), from = -5, to = 5, col = "blue", 
        add = TRUE) 
  curve(expr = dnorm(x = x, mean = 0, sd = 2), from = -5, to = 5, col = "black", 
        add = TRUE) 
  legend("topright", legend = paste("mean = 0", "sd = ", c(1, 0.5, 2)), lty = 1, lwd = 1,
         col = c("red", "blue", "black"), title = "图例")
  par(opar)
  
```

```{r}

# 绘制不同标准差的正态分布概率密度曲线：方法 3
  mydata <- data.frame(x = seq(-5, 5, len = 100))
  ggplot(data = mydata, mapping = aes(x = x)) + 
    stat_function(fun = dnorm, args = list(mean = 0, sd = 1), geom = "line", 
                  color = "red") +
    stat_function(fun = dnorm, args = list(mean = 0, sd = 0.5), geom = "line", 
                  color = "blue") +
    stat_function(fun = dnorm, args = list(mean = 0, sd = 2), geom = "line", 
                  color = "black") +
    labs(x = "Random variable", y = "Probability Density", 
         title = "Probability Density Plot for Normal Distribution") +
    theme_bw()
  
```
   
### 一元正态分布的累积分布函数

一元标准正态分布 $N(0,1)$ 的累积分布函数：$$\Phi(x) = \int_{-\infty}^x \phi(t) \,dt$$  

```{r}

# 绘制不同均值的正态分布的累积分布函数曲线：方法 1
  library(pdplot2)
  k <- seq(-5, 5, len = 100)
  pdplot2(x = k, mean = c(-1, 0, 1), sd = 1, dist = "normal", type = "CDF", 
          color = 1:3, show_color = TRUE) +
    geom_vline(xintercept = 0, color = "gray", linetype = 2) +
    labs(x = "Random variable", y = "Cumulative Probability", 
         title = "Cumulative Probability Plot for Normal Distribution") +
    theme_bw()

```

```{r}

# 绘制不同均值的正态分布的累积分布函数曲线：方法 2
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  curve(expr = pnorm(q = x, mean = 0, sd = 1), from = -5, to = 5, col = "red",
        panel.first = grid(), xlab = "Random variable", ylab = "Cumulative Probability", 
        main = "Cumulative Probability Plot for Normal Distribution")
  curve(expr = pnorm(q = x, mean = 1, sd = 1), from = -5, to = 5, col = "blue", 
        add = TRUE) 
  curve(expr = pnorm(q = x, mean = -1, sd = 1), from = -5, to = 5, col = "black", 
        add = TRUE) 
  abline(v = 0, col = "gray", lty = 2)
  legend("topleft", legend = paste("mean = ", c(0, -1, 1), " sd = 1", sep = ""), 
         lty = 1, lwd = 1, col = c("red", "blue", "black"), title = "图例")
  par(opar)
  
```

```{r}

# 绘制不同均值的正态分布的累积分布函数曲线：方法 3
  mydata <- data.frame(x = seq(-5, 5, len = 100))
  ggplot(data = mydata, mapping = aes(x = x)) + 
    stat_function(fun = pnorm, args = list(mean = 0, sd = 1), geom = "line", 
                  color = "red") +
    stat_function(fun = pnorm, args = list(mean = -1, sd = 1), geom = "line", 
                  color = "blue") +
    stat_function(fun = pnorm, args = list(mean = 1, sd = 1), geom = "line", 
                  color = "black") +
    geom_vline(xintercept = 0, color = "gray", linetype = 2) +
    labs(x = "Random variable", y = "Cumulative Probability", 
         title = "Cumulative Probability Plot for Normal Distribution") +
    theme_bw()
  
```

```{r}

# 绘制不同标准差的正态分布的累积分布函数曲线：方法 1
  library(pdplot2)
  k <- seq(-5, 5, len = 100)
  pdplot2(x = k, mean = 0, sd = c(0.5, 1, 2), dist = "normal", type = "CDF", 
          color = 1:3, show_color = TRUE) +
    geom_vline(xintercept = 0, color = "gray", linetype = 2) +
    labs(x = "Random variable", y = "Cumulative Probability", 
         title = "Cumulative Probability Plot for Normal Distribution") +
    theme_bw()

```

```{r}

# 绘制不同标准差的正态分布的累积分布函数曲线：方法 2
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  curve(expr = pnorm(q = x, mean = 0, sd = 1), from = -5, to = 5, col = "red", 
        panel.first = grid(), xlab = "Random variable", ylab = "Cumulative Probability", 
        main = "Cumulative Probability Plot for Normal Distribution")
  curve(expr = pnorm(q = x, mean = 0, sd = 0.5), from = -5, to = 5, col = "blue", 
        add = TRUE) 
  curve(expr = pnorm(q = x, mean = 0, sd = 2), from = -5, to = 5, col = "black", 
        add = TRUE) 
  abline(v = 0, col = "gray", lty = 2)
  legend("topleft", legend = paste("mean = 0", "sd = ", c(1, 0.5, 2)), lty = 1, lwd = 1,
         col = c("red", "blue", "black"), title = "图例")
  par(opar)
  
```

```{r}

# 绘制不同标准差的正态分布概率密度曲线：方法 3
  mydata <- data.frame(x = seq(-5, 5, len = 100))
  ggplot(data = mydata, mapping = aes(x = x)) + 
    stat_function(fun = dnorm, args = list(mean = 0, sd = 1), geom = "line", 
                  color = "red") +
    stat_function(fun = dnorm, args = list(mean = 0, sd = 0.5), geom = "line", 
                  color = "blue") +
    stat_function(fun = dnorm, args = list(mean = 0, sd = 2), geom = "line", 
                  color = "black") +
    labs(x = "Random variable", y = "Probability Density", 
         title = "Probability Density Plot for Normal Distribution") +
    theme_bw()
  
```
  
### 一元正态分布的分位数函数

```{r}

# 绘制不同均值正态分布的分位数曲线：方法 1
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  curve(expr = qnorm(p = x, mean = 0, sd = 1, lower.tail = TRUE), from = 0, to = 1, 
        type = "l", col = "red", panel.first = grid(), xlab = "Cumulative Probability", 
        ylab = "Random variable", main = "Quantile Plot for Normal Distribution")
  curve(expr = qnorm(p = x, mean = -1, sd = 1, lower.tail = TRUE), col = "blue", 
        add = TRUE)
  curve(expr = qnorm(p = x, mean = 1, sd = 1, lower.tail = TRUE), col = "black", 
        add = TRUE)
  legend("topleft", legend = paste("mean = ", c(0, -1, 1), " sd = 1", sep = ""), 
         lty = 1, lwd = 1, col = c("red", "blue", "black"), title = "图例")
  par(opar)
  
```

```{r}

# 绘制不同均值正态分布的分位数曲线：方法 2
  mydata <- data.frame(x = seq(0, 1, len = 100))
  
  ggplot(data = mydata, mapping = aes(x = x)) +
    stat_function(fun = qnorm, args = list(mean = 0, sd = 1), color = "red") +
    stat_function(fun = qnorm, args = list(mean = -1, sd = 1), color = "blue") +
    stat_function(fun = qnorm, args = list(mean = 1, sd = 1), color = "black") +
    labs(x = "Cumulative Probability", y = "Random variable", 
         title = "Quantile Plot for Normal Distribution") +
    theme_bw()

```  

```{r}

# 绘制不同标准差正态分布的分位数曲线：方法 1
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  curve(expr = qnorm(p = x, mean = 0, sd = 1, lower.tail = TRUE), from = 0, to = 1, 
        type = "l", col = "red", panel.first = grid(), xlab = "Cumulative Probability", 
        ylab = "Random variable", main = "Quantile Plot for Normal Distribution")
  curve(expr = qnorm(p = x, mean = 0, sd = 0.5, lower.tail = TRUE), col = "blue", 
        add = TRUE)
  curve(expr = qnorm(p = x, mean = 0, sd = 2, lower.tail = TRUE), col = "black", 
        add = TRUE)
  legend("topleft", legend = paste("mean = 0", " sd = ", c(1, 0.5, 2), sep = ""), 
         lty = 1, lwd = 1, col = c("red", "blue", "black"), title = "图例")
  par(opar)
  
```
  
```{r}

# 绘制不同标准差正态分布的分位数曲线：方法 2
  mydata <- data.frame(x = seq(0, 1, len = 100))
  
  ggplot(data = mydata, mapping = aes(x = x)) +
    stat_function(fun = qnorm, args = list(mean = 0, sd = 1), color = "red") +
    stat_function(fun = qnorm, args = list(mean = 0, sd = 0.5), color = "blue") +
    stat_function(fun = qnorm, args = list(mean = 0, sd = 2), color = "black") +
    labs(x = "Cumulative Probability", y = "Random variable", 
         title = "Quantile Plot for Normal Distribution") +
    theme_bw()

```  

### 正态性变换  

对于明显呈偏态分布的数据可以通过使用 Box-Cox 变换使数据的分布接近于正态分布，Box-Cox 变换：

$$G(x)=\left\{\begin{array}{ll}
-x^{\lambda}, & \lambda<0 \\
\log (x), & \lambda=0 \\
x^{\lambda}, & \lambda>0
\end{array}\right.$$

```{r}

# 读取数据
  attach(what = Prestige)

# 绘制变量 income 的核密度图 + 直方图
  library(rafalib)
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  shist(z = income, plotHist = TRUE, col = "blue", lwd = 2, panel.first = grid(),
        xlab = "Income", ylab = "Frequncy", main = "Histogram and Density Plot for Income")
  box()
  par(opar)
  
  detach(name = Prestige)

```

```{r}

# 读取数据
  attach(what = Prestige)

# 计算 Box-Cox 变换的幂次
  library(car)
  p <- powerTransform(object = income)
  summary(object = p)

# 计算结果显示对数据进行对数变换
  library(rafalib)
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  shist(z = log(x = income), plotHist = TRUE, col = "blue", lwd = 2, panel.first = grid(), 
        xlab = "Income", ylab = "Frequncy", main = "Histogram and Density Plot for Income")
  box()
  par(opar)
  
  detach(name = Prestige)

```

```{r}

# 读取数据
  attach(what = Prestige)

# 检验变量 income 变换前后的正态性
  shapiro.test(x = income)
  shapiro.test(x = log(x = income))
  
  p <- ggplot(data = Prestige) 
  
  p1 <- p +
    geom_density(mapping = aes(x = income), color = "red") 
  
  p2 <- p +
    geom_density(mapping = aes(x = log(income)), color = "blue") 
  
  p1 / p2

  detach(name = Prestige)

```

### 正态分布的 Q-Q 图  

Q-Q 图是将正态分布的理论分位数与数据的实际分位数进行比较，如果数据服从正态分布，那么理论分位数应该与数据的实际分位数呈现一条直线。

```{r}

# 读取数据
  attach(what = Prestige)

# 绘制变量 income 变换前后的 Q-Q 图：方法 1
  opar <- par(no.readonly = TRUE)
  par(cex.main = 0.8, cex.axis = 0.8, cex.lab = 0.8, mfrow = c(1, 2), las = 0)
  qqnorm(y = income, pch = 20, col = "blue", panel.first = grid(),
         xlab = "Theoretical Quantiles", ylab = "Sample Quantiles",
         main = "Q-Q Plot Normal Distribution")
  qqline(y = income, distribution = qnorm, col = "red", lty = 1)
  
  qqnorm(y = log(income), pch = 20, col = "blue", panel.first = grid(),
         xlab = "Theoretical Quantiles", ylab = "Sample Quantiles",
         main = "Q-Q Plot Normal Distribution")
  qqline(y = log(income), distribution = qnorm, col = "red", lty = 1)
  
  par(opar)
  
``` 
  
```{r}

# 读取数据
  attach(what = Prestige)

# 绘制变量 income 变换前后的 Q-Q 图：方法 2
  library(car)
  opar <- par(no.readonly = TRUE)
  par(cex.main = 0.8, cex.axis = 0.8, cex.lab = 0.8, mfrow = c(1, 2), las = 0)
  qqPlot(x = income, pch = 20, col = "blue", distribution = "norm", cex = 0.8,
         id = list(n = 2, method = "y", labels = rownames(x = Prestige), cex = 0.8, 
                   col = "red", location = "lr"),
         xlab = "Theoretical Quantiles", ylab = "Sample Quantiles", 
         main = "Q-Q Plot Normal Distribution")  
  qqPlot(x = log(income), pch = 20, col = "blue", distribution = "norm", cex = 0.8,
         id = list(n = 2, method = "y", labels = rownames(x = Prestige), cex = 0.8, 
                   col = "red", location = "lr"),
         xlab = "Theoretical Quantiles", ylab = "Sample Quantiles", 
         main = "Q-Q Plot Normal Distribution")  
  par(opar)

  detach(name = Prestige)

```    
  
### 正态性检验  
  
正态性检验的原假设为：$H_0:\text{数据服从正态分布}$。

* Shapiro-Wilk Normality Test：使用函数 shapiro.test，检验统计量 $W$ 越大，数据越可能服从正态分布。

  - 检验统计量 $W$ 计算的是数据的实际分位数与理论分位数之间的相关性系数。

```{r}

# 读取数据
  attach(what = Prestige)

# 检验变量 income 变换前后是否服从正态分布：方法 1
  shapiro.test(x = income)
  shapiro.test(x = log(income))
  
  detach(name = Prestige)

```

* Kolmogorov-Smirnov 检验：比较经验分布函数与理论分布函数的最大差，使用程序包 fBasics 中的函数 ksnormTest。

  - 检验统计量 $D$ 计算的是经验分布函数与理论分布函数的最大差，$D$ 值越大，数据越不可能服从正态分布。

```{r}

# 读取数据
  attach(what = Prestige)

# 检验变量 income 变换前后是否服从正态分布：方法 2
  fBasics::ksnormTest(x = income)
  fBasics::ksnormTest(x = log(income))
  
  detach(name = Prestige)

``` 

* Jarque-Bera 偏度峰度检验：$\text{JB} = \frac{n}{6}( \text{偏度}^2 + \frac14 \text{峰度}^2 )$

  - 使用程序包 fBasics 中的函数 jbTest，检验统计量越大，数据越不可能服从正态分布。  
  
```{r}

# 读取数据
  attach(what = Prestige)

# 检验变量 income 变换前后是否服从正态分布：方法 3

  # 计算拉格朗日检验统计量
  fBasics::jbTest(x = income)
  
  # 计算卡方检验统计量
  fBasics::jarqueberaTest(x = income)
  
  # 计算拉格朗日检验统计量
  fBasics::jbTest(x = log(income))
  
  # 计算卡方检验统计量
  fBasics::jarqueberaTest(x = log(income))
  
  detach(name = Prestige)
  
```   

### 偏度和峰度
  
令 $EX=\mu$，$\text{Var}(X)=\sigma^2$：

* 偏度：$$\gamma_3 = E \left[ \left(\frac{X - \mu}{\sigma} \right)^3 \right]$$

* 峰度：$$\gamma_4 = E \left[ \left(\frac{X - \mu}{\sigma} \right)^4 \right]$$
  
  - 正态分布的峰度四阶中心矩为 $3$，由于正态分布被视为无峰分布，因此常常对峰度的四阶中心矩做减 $3$ 处理。

### 厚尾分布

厚尾分布也称重尾分布，密度在 $\pm\infty$ 处的期望和方差比与其相等的正态密度高的分布称为重尾分布。

常见的厚尾分布如 $t$ 分布、Laplace 分布以及柯西（Cauchy）分布。

### 混合模型分布密度

设 $p_k, k=1,2,\dots,K$ 是 $k$ 个密度函数，$w_k \in [0, 1]$，$\sum_{i=1}^K w_k=1$，则 

$$f(x) = \sum_{k=1}^K w_k p_k(x)$$  

是一个密度函数，称为混合模型分布密度。

如果 $p_k$ 的期望为 $\mu_k$，方差为 $\sigma_k^2$，则 $$f(x)=\sum_{k=1}^{K} w_{k} p_{k}(x)$$

期望都为零的 $k$ 个正态分布的混合密度函数为：

$$\begin{aligned}
\mu &=\sum_{k=1}^{K} w_{k} \mu_{k} \\
\sigma^{2} &=\sum_{k=1}^{K} w_{k}\left[\sigma_{k}^{2}+\left(\mu_{k}-\mu\right)^{2}\right]
\end{aligned}$$

* 该密度函数的均值为零，偏度为零。
  
$$\begin{aligned}
  \sigma^2 =& \sum_{k=1}^K w_k \sigma_k^2, \\
  \text{峰度} =& 3 \sum_{k=1}^K w_k (\sigma_k / \sigma)^4 
\end{aligned}$$
  
### 单样本均值的 $t$ 检验  

总体方差未知的一元正态总体的均值比较使用单样本 $t$ 检验。

* 注意：如果一元数据不服从正态分布，当样本容量较大时也可以使用 $t$ 检验。

```{r}

# 读取数据
  attach(what = Prestige)

# 检验变量 education 的均值是否对于 10

  # 方法 1
  t.test(x = education, mu = 10, alternative = "two.sided", conf.level = 0.95)  

  # 方法 2
  DescTools::MeanCI(x = education, trim = 0, method = "classic", conf.level = 0.95,
                    sides = "two.sided", na.rm = TRUE)

  detach(name = Prestige)

``` 

### 双样本均值的 $t$ 检验  

用于比较两个服从正态分布的总体在均值是否存在差异，使用函数 t.test。

```{r}

# 读取数据
  data("sleep", package = "datasets")
  attach(what = sleep)

# 双样本均值的 t 检验

  # 方法 1
  t.test(extra ~ group, data = sleep)  
  
  # 方法 2
  DescTools::MeanDiffCI(extra ~ group, data = sleep)
  
  detach(name = sleep)

```

## 二元正态分布

### 二元随机变量的概念 

二元随机变量 $X,Y$ 是直角坐标平面上的点。  

二元随机变量 $X,Y$ 之间的协方差为 $\text{Cov}(X, Y) = E[(X - EX)(Y - EY)]$

二元随机变量 $X,Y$ 之间的相关系数为 $\rho(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X) \text{Var}(Y)}}$

* 相关系数等于零称为不相关，二元随机变量彼此独立一定是不相关的。

* 相关系数度量的是二元随机变量之间的线性相关程度。
  
### 二元正态分布的概念 
  
二元正态分布 $\mu_X, \mu_Y, \sigma_X^2, \sigma_Y^2, \rho$ 表示为：  

$$\phi(\boldsymbol x | \boldsymbol\mu, \Sigma)
  = (2\pi)^{-1} \text{det}(\Sigma)^{-1/2}
  \exp \left( -\frac12 (\boldsymbol x - \boldsymbol\mu)^T \Sigma^{-1} 
    (\boldsymbol x - \boldsymbol\mu) \right)$$

二元正态分布的性质：

* 二元正态分布的边缘分布也服从正态分布。

  - 边缘分布：$p$ 维随机向量 $x=(x_1, x_2, \ldots, x_p)\prime$ 的任意子向量的分布。

  - 边缘分布可以是关于一个变量、两个变量、直至 $p−1$ 个变量的边缘分布。

* 二元正态分布的条件分布也服从正态分布。

  - 条件分布：给定一些已知条件下的随机变量或随机向量的概率分布。

* 二元正态分布的线性组合服从正态分布。

* 二元正态分布的标准化: 存在矩阵 $A$ 使得 $A A^T = \Sigma$，$\boldsymbol Z = A^{-1} (\boldsymbol X - \boldsymbol\mu)$ 服从标准二元正态分布。其中，$Z_1, Z_2$ 是独立的标准正态分布。  

### 二元正态分布的估计

```{r}

# 读取数据
  attach(what = swiss)
  anyNA(x = swiss)

# 查看数据的结构
  str(object = swiss)
  glimpse(x = swiss, width = 80)

# 计算摘要统计量
  summary(object = swiss)

  detach(name = swiss)

```
  
```{r}

# 读取数据
  attach(what = swiss) 

# 绘制变量 Agriculture 与 Examination 之间的散点图
  ggplot(data = swiss, mapping = aes(x = Agriculture, y = Examination)) +
    geom_point(color = "blue") +
    labs(x = "Percent of males involved in agriculture as occupation",
         y = "Percent draftees receiving highest mark on army examination") +
    theme_bw()
  
  detach(name = swiss)  

```

```{r}

# 读取数据
  attach(what = swiss) 

# 绘制变量 Agriculture 与 Examination 之间的二元盒型图
  library(MVA)

  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  bvbox(a = swiss[2:3], method = "robust", pch = 20, col = "red", panel.first = grid(),
        xlab = "Percent of males involved in agriculture as occupation",
        ylab = "Percent draftees receiving highest mark on army examination", 
        main = "Bivariate Boxplot")  
  
  par(opar)

  detach(name = swiss)

```

```{r}

# 读取数据
  attach(what = swiss) 

# 绘制变量 Agriculture 与 Examination 之间的洋葱图
  library(aplpack)
  
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  nlev <- 5
  mycolors <- heat.colors(n = 9)[3:(nlev + 2)]
  
  plothulls(x = swiss[2:3], type = "n", n.hull = nlev, col.hull = mycolors, 
            lty.hull = 1:nlev, density = NA, col = 0, panel.first = grid(), 
            xlab = "Percent of males involved in agriculture as occupation",
            ylab = "Percent draftees receiving highest mark on army examination",
            main = "")
  points(x = swiss[2:3], pch = 16, cex = 0.7, col = "blue")
  title(main = "Five peeling convex hulls")
  
  par(opar)

  detach(name = swiss)  

```

### 二元正态密度函数的绘图

```{r}

# 加载程序包
  library(fMultivar)
  library(mvtnorm)
  
  # 创建数据
  rho <- 0.8
  ng <- 25
  Sig <- rbind(c(1, rho), c(rho, 1))  
  
  x <- y <- seq(-2, 2, len = ng)
  z <- cbind(x = rep(x = x, each = ng), y = rep(x = y, ng))

  dens <- matrix(data = dmvnorm(x = z, sigma = Sig),
                 nrow = ng, ncol = ng, byrow = FALSE)

# 绘制二元正态分布概率密度的等高线图
  nlev <- 6
  
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  contour(x = x, y = y, z = dens, nlevels = nlev, panel.first = grid(), 
          drawlabels = TRUE, col = rainbow(n = 6), xlab = "x", ylab = "y", 
          main = "二元正态分布概率密度的等高线图")
  par(opar)
  
# 绘制二元正态分布概率密度的颜色图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  image(x = x, y = y, z = dens)
  par(opar)
  
# 绘制二元正态分布概率密度图
  persp(x = x, y = y, z = dens, theta = 30, phi = 30, col = "steelblue")

```  

```{r}

# 生成服从二元正态分布的随机向量
  x <- seq(-4, 4, by = 0.1)
  X <- grid2d(x = x)
  z <- dnorm2d(x = X$x, y = X$y, rho = 0.9)
  ZD <- list(x = x, y = x, z = matrix(data = z, ncol = length(x)))
  r <- rnorm2d(n = 5000, rho = 0.5)

# 绘制二元正态分布随机向量的散点图 + 概率密度等高线
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  plot(r, col = "steelblue", pch = 20, panel.first = grid(), xlab = "r1", 
       ylab = "r2", main = "二元正态分布随机向量的散点图 + 概率密度等高线") 
  contour(x = ZD, add = TRUE, lwd = 2, col = "red")
  par(opar)
   
# 绘制二元正态分布随机向量的六边形图 + 概率密度等高线
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  plot(x = hexBinning(x = r, bins = 50), col = terrain.colors(n = 12),
       panel.first = grid(), addRug = TRUE, xlab = "r1", ylab = "r2", 
       main = "二元正态分布随机向量的六边形图 + 概率密度等高线")
  contour(x = ZD, add = TRUE, lwd = 2, col = "blue")
   
```
 
### 二元变量的相关系数的检验

二元变量的相关系数的检验统计量为：$t=\hat{\rho} \sqrt{\frac{n-2}{1-\hat{\rho}^{2}}}$

* 二元变量的相关系数的检验的零假设为两个变量之间不相关。在零假设成立的条件下，上述检验统计量近似服从 $t$ 分布。
  
二元变量的相关系数的检验，使用函数 cor.test。
  
```{r}

# 读取数据
  attach(what = swiss)
  
# 二元变量的相关系数的检验
  cor.test(x = Agriculture, y = Examination, alternative = "two.sided", 
           method = "pearson", conf.level = 0.95)

  detach(name = swiss)

``` 

### 二元变量的联立置信椭圆区间  

设二元正态分布样本均值为 $\bar{\boldsymbol x}$，样本协方差阵为 $\hat{\Sigma}$，理论期望值为 $\mu$，则置信度为 $1-\alpha$ 的联合置信区间的边界为：

$$(\overline{\boldsymbol{x}}-\boldsymbol{\mu})^{T} \hat{\Sigma}^{-1}(\overline{\boldsymbol{x}}-\boldsymbol{\mu})=\frac{1}{n} \chi_{1-\alpha}^{2}(2)$$

其中， $\chi_{1-\alpha}^{2}(2)$ 是 $\chi^2(2)$ 的 $1-\alpha$ 的分位数。

```{r}

# 读取数据
  attach(what = swiss) 

# 绘制变量 Agriculture 与 Examination 之间的置信椭圆：方法 1，首选方法
  library(car)

  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  dataEllipse(x = Agriculture, y = Examination, pch = 20, col = "blue", center.cex = 1, 
              lwd = 1, center.pch = 7, levels = c(0.5, 0.95), plot.points = TRUE, 
              robust = TRUE, xlim = c(0, 110), ylim = c(-5, 40), fill = TRUE, 
              fill.alpha = 0.1, ellipse.label = paste("level = ", c(0.5, 0.95), sep = ""), 
              id = list(n = 3, labels = rownames(x = swiss), method = "mahal",
                        col = "red", cex = 0.8, location = "lr"), 
              xlab = "Percent of males involved in agriculture as occupation",
              ylab = "Percent draftees receiving highest mark on army examination",
              main = "Data Ellipses for Agriculture vs. Examination")
  
  par(opar)
  
  detach(name = swiss)  

```  

```{r}

# 读取数据
  attach(what = swiss)

# 绘制 90% Agriculture 与 Examination 之间的置信椭圆：方法 2
  library(ellipse)
  library(matrixStats)
  
  mydata <- as.matrix(x = swiss[2:3])
  
  ellipses_95 <- ellipse(x = cor(mydata, method = "pearson"), scale = colSds(x = mydata), 
                      centre = colMeans(x = mydata), level = 0.95, npoints = 100)  
  ellipses_50 <- ellipse(x = cor(mydata, method = "pearson"), scale = colSds(x = mydata), 
                      centre = colMeans(x = mydata), level = 0.5, npoints = 100)  
  
  opar <- par(no.readonly = TRUE)  
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(mydata, type = "n", xlim = c(-10, 110), ylim = c(-5, 40), panel.first = grid())
  points(mydata, type = "p", pch = 20, col = "blue")
  points(rbind(colMeans(x = mydata, na.rm = TRUE)), pch = 8, cex = 1, col = "red")
  lines(ellipses_95, col = "blue", type = "l")
  lines(ellipses_50, col = "blue", type = "l")

  par(opar)
  
  detach(name = swiss)

```

## 多元正态分布 

### 多元随机变量

* $\boldsymbol X = (X_1, X_2, \dots, X_p)^T$ 属于 $\mathbb R^p$。

* 期望向量 $E\boldsymbol X = (E X_1, E X_2, \dots, E X_p)^T$。

* 协方差阵 $\text{Var}(\boldsymbol X) = E[ (\boldsymbol X - E \boldsymbol X) (\boldsymbol X - E \boldsymbol X)^T ]$ 是 $p \times p$ 的半正定矩阵。其中，第 $(i,i)$ 元素等于 $\text{Var}(X_i)$，第 $(i,j)$ 元素等于 $\text{Cov}(X_i, X_j)$。 
  
设 $\boldsymbol X$ 为 $p$ 维随机向量，$\boldsymbol Y$ 为 $q$ 维随机向量，则  

$$\text{Cov}(\boldsymbol X, \boldsymbol Y)
= E[ (\boldsymbol X - \boldsymbol\mu_X)
(\boldsymbol Y - \boldsymbol\mu_Y)^T]
= [ \text{Cov}(\boldsymbol Y, \boldsymbol X) ]^T$$ 

$\text{Cov}(\boldsymbol X, \boldsymbol Y)$ 称为 $p$ 维随机向量 $X$ 与 $q$ 维随机向量 $Y$ 之间的协差阵。

### 多元概率分布函数
  
* 随机变量 $\boldsymbol X$ 的分布函数表示为：$$F(a)=P(\boldsymbol{X}\leq a)$$。

* 随机变量 $\boldsymbol X$ 与 $\boldsymbol Y$ 的联合分布函数表示为：

$$F(a_1, a_2)=P(\boldsymbol{X}\leq a_1, \boldsymbol{Y}\leq a_2)$$

* 随机向量 $\boldsymbol X = (X_1, X_2, \dots, X_p)^T$ 的分布函数表示为：

$$F(a_1, a_2, \ldots, a_p)=P(\boldsymbol{X_1}\leq a_1, boldsymbol{X_2}\leq a_2, \ldots, \boldsymbol{X_p}\leq a_p)$$

### 边缘分布

边缘分布：$p$ 维随机向量 $x=(x_1, x_2, \ldots, x_p)\prime$ 的任意子向量的分布。

* 边缘分布可以是关于一个变量、两个变量、直至 $p−1$ 个变量的边缘分布。

### 条件分布

条件分布：给定一些已知条件下的随机变量或随机向量的概率分布。

### 独立性

独立性：如果多个随机变量之间或随机向量的各个元素之间的取值彼此互不影响，就认为多个随机变量之间或随机向量的各个元素之间是相互独立的。

* 如果随机变量 $x_1,x_2,\ldots,x_n$ 之间是相互独立的，则有：$$f(x_1,x_2,\ldots,x_n)=f(x_1)f(x_2)\ldots f(x_n)$$

* 如果随机向量 $X_1,X_2,\ldots,X_n$ 之间是相互独立的，则有：$$f(X_1,X_2,\ldots,X_n)=f(X_1)f(X_2)\ldots f(X_n)$$

## 多元分布样本

设总体为 $p$ 元的随机向量 $\boldsymbol X$，此总体的一个简单随机样本为 $\boldsymbol x^{(i)}, i=1,2,\dots,n$, 其中 $\boldsymbol x^{(i)}$ 与 $\boldsymbol X$ 同分布。

这些数据一共有 $n \times p$ 个值，通常写成一个矩阵 $M = (x_{ij})_{n\times p}$ 的形式，矩阵的第 $i$行是 $(\boldsymbol x^{(i)})^T$，即第 $i$ 个观测样例（样品）的 $p$ 个分量，这样的矩阵称为观测数据集。  

### 样本均值 

```{r}

# 读取数据
  attach(what = swiss)
  
# 均值向量
  colMeans(x = swiss, na.rm = TRUE) 
  matrixStats::colMeans2(x = as.matrix(x = swiss), na.rm = TRUE)
  
  detach(name = swiss)

```  

### 样本的协方差阵  

总体协方差阵 $\Sigma=\text{Var}(\boldsymbol X)$ 可以用如下样本协方差阵估计：

$$\begin{aligned}
  S = \frac{1}{n} \sum_{i=1}^n (\boldsymbol x^{(i)} - \bar{\boldsymbol x}) (\boldsymbol x^{(i)} - \bar{\boldsymbol x})^T
\end{aligned}$$
 
这里用了 $1/n$ 的公式，多元分析中经常用这个公式。如果用 $1/(n-1)$ 的公式，得到的就是无偏估计。

```{r}

# 读取数据
  attach(what = swiss)

# 样本协方差阵
  cov(swiss, use = "pairwise.complete.obs", method = "pearson")
  
  detach(name = swiss)

```

### 观测数据集的中心化

观测数据集每列减去列平均值，称为中心化。 

```{r}

# 读取数据
  attach(what = swiss)

# 观测数据的中心化
  swiss_cen_1 <- scale(x = swiss, center = TRUE, scale = FALSE)
  head(x = swiss_cen_1, n = 3)
  
  swiss_cen_2 <- datawizard::center(x = swiss)
  head(x = swiss_cen_2, n = 3)

  near(x = swiss_cen_1, y = swiss_cen_2)

  detach(name = swiss)

```

### 观测数据集标准化  
  
观测数据每列减去列平均值，除以标准差估计值。

```{r}

# 读取数据
  attach(what = swiss)

# 观测数据的标准化
  swiss_std_1 <- scale(x = swiss, center = TRUE, scale = TRUE)  
  swiss_std_2 <- datawizard::standardize(x = swiss)
  near(x = swiss_std_1, y = swiss_std_2)
  
  detach(name = swiss)

```

## 多元正态分布数据统计量的分布 

### 均值和方差估计

设多元总体的简单随机样本 $\boldsymbol X$ 组成观测数据集 $\boldsymbol M$，$\bar{\boldsymbol x}$ 和 $S$ 为样本的均值和样本协方差阵，则

$$E \bar{\boldsymbol x} = \boldsymbol\mu,\quad ES = \frac{n-1}{n}\Sigma$$

当总体服从 $\boldsymbol X \sim \text{N}(\boldsymbol\mu, \Sigma)$ 时, $\bar{\boldsymbol X} \sim \text{N}(\boldsymbol\mu, \frac{1}{n} \Sigma)$。

### 多元均值的中心极限定理

设 $\boldsymbol X_i, i=1,2,\dots,n$ 独立同分布，共同期望为 $\boldsymbol\mu$，共同方差阵为 $\Sigma$，则 $\hat{\boldsymbol\mu} = \frac{1}{n} \sum_{i=1}^n \boldsymbol X_i$ 满足如下多元中心极限定理：  

$$\sqrt{n} (\hat{\boldsymbol\mu} - \boldsymbol\mu)
  \stackrel{d}{\rightarrow}
  N(\boldsymbol 0, \Sigma)
  \quad\text{当}\; n \to \infty$$  

若 $\hat\Sigma$ 是 $\Sigma$ 的一个相合估计，则有如下的中心极限定理：

$$\sqrt{n} \hat\Sigma^{-1/2} (\hat{\boldsymbol\mu} - \boldsymbol\mu)
  \stackrel{d}{\rightarrow}
  N(\boldsymbol 0, I)
  \quad\text{当}\; n \to \infty$$

### Wishart 分布

在一元正态总体的推断中，$\chi^2,\; t,\; F$ 分布起到重要作用。

在多元正态总体推断中，上述分布需要推广为 Wishart 分布和 Hotelling $T^2$ 分布。

设总体 $\boldsymbol{X}$ 服从 $N(\mathbf{0}, \Sigma)$ 正态分布，其中 $(\Sigma>0)$，矩阵 $M_{n \times p}$ 为 $\boldsymbol{X}$ 的观测数据集，每一行代表 $\boldsymbol{X}$  的一个样本点。令随机矩阵 $Z=M^{T} M$，则 $Z$ 服从 Wishart 分布 $W_{p}(\Sigma, n)$。

标准 Wishart 分布为 $W_{p}\left(I_{p}, n\right)$。

当 $p=1$ 时，总体服从 $X \sim \mathrm{N}\left(0, \sigma^{2}\right)$，$W_{1}\left(\sigma^{2}, n\right)$ 就服从 $\sigma^{2} \chi^{2}(n)$ 分布。

Wishart 分布的随机向量包括 $p(p+1)/2$ 个分量（矩阵的下三角部分），且需要满足正定约束。

多元正态总体样本方差之和服从 Wishart 分布。
  
### Hotelling $T^2$ 分布

设 $\boldsymbol{Y} \sim \mathrm{N}\left(\mathbf{0}, I_{p}\right)$，随机矩阵 $Z \sim W_{p}\left(I_{p}, n\right)$ 与  $\boldsymbol{Y}$ 独立，则 $\boldsymbol{Y}^{T}\left(\frac{1}{n} Z\right)^{-1} \boldsymbol{Y}$ 就服从 Hotelling $T_{p, n}^{2}$ 分布。 

这是 $t$ 分布的推广，一元 $(p=1)$ 的时候，Hotelling $T^2$ 分布相当于 $t(n)$ 分布随机变量的平方。

如果 $\boldsymbol{X} \sim \mathrm{N}(\boldsymbol{\mu}, \Sigma)$，随机矩阵 $Z \sim W_{p}(\Sigma, n)$ 与 $\boldsymbol{X}$ 独立，则

$$(\boldsymbol{X}-\boldsymbol{\mu})^{T}\left(\frac{1}{n} Z\right)^{-1}(\boldsymbol{X}-\boldsymbol{\mu}) \sim T_{p, n}^{2}$$

Hotelling $T_{p, n}^{2}$ 可以转化为 $F$ 分布：$$T_{p, n}^{2}=\frac{n p}{n-p+1} F_{p, n-p+1}$$。

### 多元正态分布均值的联立置信域

对于服从多元正态分布的总体来说，其均值的联立置信域为： 
 
* 二维时其边界是椭圆，三维时其边界是椭球。
  
## 多元正态单总体的均值检验

### 单样本均值的检验

设总体 $\boldsymbol{X} \sim \mathrm{N}(\boldsymbol{\mu}, \Sigma)$，$\boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)}, \ldots, \boldsymbol{x}^{(n)}$ 为该总体的一个样本。对给定的 $\boldsymbol{\mu}_{0}$，为检验

$$H_{0}: \boldsymbol{\mu}=\boldsymbol{\mu}_{0} \longleftrightarrow H_{a}: \boldsymbol{\mu} \neq \boldsymbol{\mu}_{0}$$

使用检验统计量

$$\left(\overline{\boldsymbol{x}}-\boldsymbol{\mu}_{0}\right)^{T}\left(\frac{1}{n-1} \frac{1}{n} M_{C}^{T} M_{C}\right)^{-1}\left(\bar{x}-\boldsymbol{\mu}_{0}\right) \stackrel{H_{0}}{\stackrel{ }{2}} T_{p, n-1}^{2}$$

或

$$\frac{n-p}{p(n-1)}\left(\overline{\boldsymbol{x}}-\boldsymbol{\mu}_{0}\right)^{T}\left(\frac{1}{n-1} \frac{1}{n} M_{C}^{T} M_{C}\right)^{-1}\left(\overline{\boldsymbol{x}}-\boldsymbol{\mu}_{0}\right) \stackrel{H_{0}}{\sim} F(p, n-p)$$

当检验统计量大于右侧的临界值 $F_{1-\alpha}(p, n-p)$ 时，就拒绝 $H_{0}$。

在 R 中，使用程序包 ICSNP 中的函数 HotellingsT2 或使用程序包 rrcov 中的函数 T2.test。  

```{r}

# 读取数据
  data("LASERI", package = "ICSNP")
  anyNA(x = LASERI)
  attach(what = LASERI)

# 绘制散点图矩阵
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 0)
  pairs(x = LASERI[25:28], col = "blue", pch = 20, main = "Scattplot Matrix for LASERI")
  par(opar)
  
  detach(name = LASERI)

```

```{r}

# 读取数据
  attach(what = LASERI)

# 检验平均变化的均值是否联合为零：使用单样本多元正态 Hotelling T2 检验

  # 方法 1：返回 F 检验统计量
  ICSNP::HotellingsT2(X = LASERI[25:28])
  
  # 方法 2：返回 T2 和 F 检验统计量
  rrcov::T2.test(x = LASERI[25:28])
  
  detach(name = LASERI)

```
 
### 独立双样本多元正态分布总体均值的比较  
 
独立双样本多元正态分布总体均值的比较，使用程序包 rrcov 中的函数 T2.test。  

### 独立多样本多元正态分布总体均值的比较 

独立多样本多元正态分布总体均值的比较，使用程序包 rrcov 中的函数 Wilks.test。  

## 多元总体的正态性检验 

### 多元总体的偏度与峰度检验  

在 R 中，使用程序包 psych 中的函数 mardia 执行多元总体的偏度与峰度检验并绘制广义 QQ 图。

```{r}

# 读取数据
  attach(what = LASERI)

# 计算各个变量的偏度系数
  psych::skew(x = LASERI[25:28], na.rm = TRUE, type = 3)
  
# 计算各个变量的峰度系数：已做减 3 处理
  psych::kurtosi(x = LASERI[25:28], na.rm = TRUE, type = 3)
  
# 多元总体的偏度与峰度检验
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  psych::mardia(x = LASERI[25:28], na.rm = TRUE, plot = TRUE)
  par(opar)

  detach(name = LASERI)

``` 
  
# 判别分析

## 判别分析概述

### 判别分析的概念

判别分析（discriminant analysis）属于“有监督的学习”方法。判别分析首先对“训练样本”进行分析计算得到一个判别规则，然后对新样例使用得到的判别规则判断该新样例所属的类别。在判别分析中，训练样本中既包含可有用于分类的解释变量（自变量），也包含真实的类别。

常用的判别分析方法有距离判别、Fisher 判别和 Bayes 判别，逻辑斯谛回归也经常用于判别分析中（尤其是对两类别的判别）， 分类树也是用于判别分析的方法。 
 
### 判别分析的目标

* 预测：也称分类或分配。预测是指在已知历史上用某些方法已把研究对象分成若干组别（也称类或总体）的情况下，来判定新观测样例应归属的组别。

  - 判别分析中的分类方法：距离判别、贝叶斯（Bayes）判别和费希尔（Fisher）判别等。其中，距离判别与贝叶斯判别仅适用于分类。
  
* 描述：也称分离。描述是指利用图形（通常是二维图形，有时也会是三维或一维图形，一般通过降维实现）方法或代数方法描述各组别中观测样例之间的差异性，从而最大限度地分离各个组别。

  - 判别分析中的分离方法：费希尔（Fisher）判别，该方法更多地是用于分离。
 
### 判别分类的原则

由于判别分类是在信息不完备情况下进行的，因此难免会发生误判，好的判别分类方法应使发生误判的概率尽可能的小。
  
### 判别分类的示例

* 有偿付力与无偿付力的财产责任保险公司。

  - 判别变量：总资产、股票与债券价值、股票与债券的市值、损失支出、盈余、签定的保费金额等。
  
* 新产品的速购者与迟购者。

  - 判别变量：教育、收入、家庭大小、过去更换品牌的次数等。
  
* 良好信用与不良信用风险。

  - 判别变量：收入、年龄、信用卡数量、家庭人口数量等。
  
**注意**：这里仅讨论判别变量为定量变量（间隔变量）的判别分析。   

## 距离判别  

距离判别的思想是计算每个样例与各个类别中心的距离，然后把样例归类为距离最近的一类。常用的距离为欧式距离（Euclidean Distance）与马氏距离（Mahalanobis distance）。

### 欧氏距离

欧式距离计算的是观测样例两两之间特征在空间上的直线距离。

$\boldsymbol{x}=\left(x_{1}, x_{2}, \cdots, x_{p}\right)^{\prime}$ 与 $\boldsymbol{y}=\left(y_{1}, y_{2}, \cdots, y_{p}\right)^{\prime}$ 之间的欧氏距离为：

$$d(\boldsymbol{x}, \boldsymbol{y})=\sqrt{\left(x_{1}-y_{1}\right)^{2}+\left(x_{2}-y_{2}\right)^{2}+\cdots+\left(x_{p}-y_{p}\right)^{2}}$$

平方欧氏距离为：

$$\begin{aligned}
d^{2}(\boldsymbol{x}, \boldsymbol{y}) &=\left(x_{1}-y_{1}\right)^{2}+\left(x_{2}-y_{2}\right)^{2}+\cdots+\left(x_{p}-y_{p}\right)^{2} \\
&=(\boldsymbol{x}-\boldsymbol{y})^{\prime}(\boldsymbol{x}-\boldsymbol{y})
\end{aligned}$$

如果各变量的单位不全相同，则上述欧氏距离是没有意义的。  

如果对各分量都作标准化变换，则各分量方差同为 $1$，于是：

* 平方和中各分量所起的平均作用都一样。

* 如果各分量的单位不全相同，则标准化可不受单位的影响。

计算欧氏距离使用 stats 中的函数 dist，该函数能够计算 "euclidean"、"maximum"、"manhattan"、"canberra"、"binary" 以及 "minkowski"。

* 函数 dist 返回一个由配对欧氏距离构成的矩阵。
  
```{r}
 
# 读取数据
  attach(what = iris)

# 计算各观测样例之间的配对欧氏距离
  dist(x = iris[1:6, -5], method = "euclidean", diag = TRUE, upper = FALSE)
  
  detach(name = iris)

``` 

### 马氏距离

欧氏距离经变量的标准化之后虽能够消除各变量的单位或方差不同的影响，但不能消除变量之间相关性带来的不利影响。

马氏距离（Mahalanobis distance）也称统计距离，它首先对随机向量进行正交变换，然后再对正交变换后的随机向量进行标准化变换，最后计算欧氏距离，这个欧氏距离就是马氏距离。
  
$\boldsymbol{x}=\left(x_{1}, x_{2}, \cdots, x_{p}\right)^{\prime}$ 与 $\boldsymbol{y}=\left(y_{1}, y_{2}, \cdots, y_{p}\right)^{\prime}$ 之间的平方马氏距离定义为：
  
$$\boldsymbol{d}^{2}(\boldsymbol{x}, \boldsymbol{y})=(\boldsymbol{x}-\boldsymbol{y})^{\prime} \Sigma^{-1}(\boldsymbol{x}-\boldsymbol{y})$$
  
$\boldsymbol{x}=\left(x_{1}, x_{2}, \cdots, x_{p}\right)^{\prime}$ 到总体 $\pi$ 的平方马氏距离定义为：

$$d^{2}(\boldsymbol{x}, \pi)\left(=d^{2}(\boldsymbol{x}, \boldsymbol{\mu})\right)=(\boldsymbol{x}-\boldsymbol{\mu})^{\prime} \Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu})$$ 
  
* 马氏距离将所有变量标准化为具有相同方差的变量，并且消除了变量之间的相关性。
  
* 马氏距离的实质就是两个经过"变换"后的变量之间的欧氏距离。
 
* 马氏距离是一个与随机变量中各分量的单位无关的无量纲纯数值。
  
* 当各分量不相关时马氏距离即为各分量经标准化变换后的欧氏距离。 

计算马氏距离使用 stats 中的函数 mahalanobis，该函数返回一个由平方马氏距离构成的向量。  

```{r}

# 读取数据
  attach(what = iris)

# 计算各个观测样例与质心之间的平方马氏距离 
  dist_mahal <- mahalanobis(x = iris[-5], center = colMeans(x = iris[-5], na.rm = TRUE), 
                            cov = cov(x = iris[-5], use = "pairwise.complete.obs"))
  DescTools::Some(x = dist_mahal, n = 3)
  length(x = dist_mahal)

# 绘制平方马氏距离的核密度图
  ggplot(mapping = aes(x = dist_mahal)) +
    geom_density(color = "gray", fill = "steelblue", size = 1) +
    xlim(-2, 15) +
    labs(x = "Squared Mahalanobis distances", y = "Density", 
         title = "Density Plot for Squared Mahalanobis distances") +
    theme_bw()

# 绘制平方马氏距离的 QQ 图
  library(car)
  
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4))
  qqPlot(x = dist_mahal, distribution = "chisq", df = 4, las = 1, pch = 20, col = "blue",
         envelope = list(col = "orange", level = 0.95), ylim = c(0, 18),
         id = list(n = 3, method = "mahal", labels = rownames(x = iris), col = "red", 
                   cex = 0.8, location = "lr"),
         xlab = "Chisquare Quantiles", y = "Mahalanobis Distances", 
         main = "Q-Q Plot for Mahalanobis Distances")
  par(opar)
  
# 检验平方马氏距离是否服从卡方分布
  ks.test(x = dist_mahal, y = pchisq, df = 4, alternative = "two.sided")
  
  detach(name = iris)

```  

### 误判概率
  
设 $\pi_1\sim N_p(\boldsymbol{\mu_1},\boldsymbol{\Sigma})$，$\pi_2\sim N_p(\boldsymbol{\mu_2},\boldsymbol{\Sigma})$，则误判概率为：

$$P(2\mid1)=P(1\mid2)=\Phi(-\Delta/2)$$

其中，$\Delta$ 是两个组别均值向量之间的马氏距离：

$$\Delta=\sqrt{(\boldsymbol{\mu_1-\mu_2})^\prime\boldsymbol{\Sigma^{-1}}(\boldsymbol{\mu_1-\mu_2})}$$

```{r}

# 误判概率图
  img <- readPNG(source = "Pictures/误判概率图.png")
  grid.raster(image = img)
  
``` 

由上图可见：

* 两个服从多元正态分布的随机向量越是分开（即 $\Delta$ 越大），分类效果也就越好，误判概率也就越小，判别效果也就越好。

* 当两个正态组很接近时，误判概率将会很大，此时作判别分析就没有什么实际意义了。
   
对两组均值向量之间是否过于接近的判定：
  
* 我们可设定原假设 $H_0: \mu_1=\mu_2$，备择假设 $H_1: \mu_1\neq\mu_2$，然后进行假设检验。

  - 如果假设检验结果不能拒绝原假设 $H_0$，就说明两组的均值向量之间无显著差异，此时作判别分析是徒劳的。
  
  - 如果假设检验结果拒绝了原假设 $H_0$，就说明两组的均值向量之间存在显著差异。但这种差异是否大到足以进行有效的判别分析还不能确定，此时还应看误判概率是否超过了一个合理的水平。

### 使用样本估计两组距离判别中的未知参数 
  
给定两个组别 $\boldsymbol\pi_1$ 和 $\boldsymbol\pi_2$ 服从多元正态分布，这时从两个组别中分别抽取容量为 $n_1$ 和 $n_2$ 的样本且 $n_1+n_2-2\geq{p}$，则有：
  
* $\mu_1$ 和 $\mu_2$ 使用样本均值向量作为无偏估计量。
 
* 协方差矩阵 $\boldsymbol\Sigma$ 的联合无偏估计量为 $$\S_p=\frac{(n_1-1)S_1+(n_2-1)S_2}{n_1+n_2-2}$$，其中：

$$S_i=\frac{1}{n_i-1}\Sigma_{j=1}^{n_i}(x_{ij}-\bar{x_i})(x_{ij}-\bar{x_i})^\prime,\quad i=1,2$$
  
* 使用样本估计误判概率 $\Phi(-\Delta/2)$ 时是有偏估计量，但大样本时偏差的影响是可以忽略的。  
  
### 误判概率的非参数估计

若两组不能假定为正态组，则 $P(2|1)$ 与 $P(1|2)$ 可以用样本中观测样例的误判比例来估计，通常有如下三种非参数估计方法：

* 回测法

  - 令 $n(2\mid1)$ 为样本中实为 $\pi_1$ 但却误判为 $\pi_2$ 的样例个数，$n(1\mid2)$ 为样本中实为 $\pi_2$ 但却误判为 $\pi_1$ 的样例个数，则误判概率为：
  
$$\hat{P}(2\mid1)=\frac{n(2\mid1)}{n_1}$$
  
$$\hat{P}(1\mid2)=\frac{n(1\mid2)}{n_2}$$

  - 该方法简单、直观且易于计算，但它给出的误判概率估计值通常偏低或称对误判概率的估计过于乐观。但当样本容量非常大时，偏低的影响可忽略。 
  
* 拆分样本

  - 将整个样本拆分为训练样本（用于构造判别函数）和验证样本（用于对该判别函数进行评估）。误判概率用验证样本的误判比例来估计，该估计是无偏的。该方法主要有两个缺陷：
  
    - 需要使用大样本。
  
    - 该方法构造的判别函数只用了部分样本数据，与使用全部样本数据构造的判别函数相比，损失了很多有价值的信息，其效用自然不如后者，表现为前者的误判概率通常高于后者，而后者的误判概率才是我们真正感兴趣的。该缺陷将随着样本容量的增大而逐渐减弱，甚至可基本忽略。

* 交叉验证法（或称刀切法）

  - 使用的交叉验证实际上就是留一交叉验证（leave-one-out）。
  
  - 从组 $\pi_1$ 中取出 $x_{1j}$，用该组的其余 $n_1−j$ 个观测值和组 $\pi_2$ 中的 $n_2$ 个观测值构造判别函数，然后对组 $\pi_1$ 中取出的 $x_{1j}$ 进行判别，$j=1,2,\cdots,n_1$。
  
  - 同样，从组 $\pi_2$ 中取出 $x_{2j}$，用该组的其余 $n_2−j$ 个观测值和组 $\pi_1$ 中的 $n_1$ 个观测值构造判别函数，然后对组 $\pi_2$ 中取出的 $x_{2j}$ 进行判别，$j=1,2,\cdots,n_2$。
  
  - 总验证次数为 $n_1+n_2$。
  
  - 令 $n^{*}(2\mid 1)$ 为样本中来自 $\pi_1$ 但却误判为 $\pi_2$ 的样例个数，令 $n^{*}(1\mid 2)$ 为样本中来自 $\pi_2$ 但却误判为 $\pi_1$ 的样例个数，则两个误判概率 $P(2\mid 1)$ 和 $P(1\mid 2)$ 的估计量为：
  
$$\hat{P}(2 \mid 1)=\frac{n^{*}(2 \mid 1)}{n_{1}}, \quad \hat{P}(1 \mid 2)=\frac{n^{*}(1 \mid 2)}{n_{2}}$$  

以上所述误判概率的这三种非参数估计方法同样适用于其它的判别方法或判别情形，并且可类似地推广到多组的情形。  

要注意的是，以上的简单判别规则是在假定两个组别的协方差阵相等时得到的。如果协方差阵不相等，判别函数就不再是线性的，分界点也不恰好在中点。 当只有两个组别时规则比较简单，可以得到一个判别函数。 如果有多个组别，就需要计算多个判别函数。
 
## 基于平方马氏距离的分类判别：两组情形

### 一般判别规则

问题：对 $i=1,2$，令组别分别为 $\boldsymbol\pi_i$ 的均值向量为 $\mu_i$，协差阵为 $\boldsymbol\Sigma_i$，$x$ 是一个新的 $p$ 维观测样例，现欲判断它属于哪一组。

基于马氏距离的一般判别规则：
 
$$\left\{\begin{array}{ll}
\boldsymbol{x} \in \pi_{1}, & \text { 若 }\; d^{2}\left(\boldsymbol{x}, \pi_{1}\right) \leq d^{2}\left(\boldsymbol{x}, \pi_{2}\right) \\
\boldsymbol{x} \in \pi_{2}, & \text { 若 }\; d^{2}\left(\boldsymbol{x}, \pi_{1}\right)>d^{2}\left(\boldsymbol{x}, \pi_{2}\right)
\end{array}\right.$$

其中，$d^2$ 表示平方马氏距离。 
 
### $\boldsymbol\Sigma_1=\boldsymbol\Sigma_2=\boldsymbol\Sigma$ 时的判别  
    
如果 $\boldsymbol\Sigma_1=\boldsymbol\Sigma_2=\boldsymbol\Sigma$，就有
  
$$d^2(x,\pi_1)-d^2(x,\pi_2)=-2a^\prime(x-\bar\mu),\quad \bar{\mu}=\frac{1}{2}(\mu_1+\mu_2),\quad a=\boldsymbol\Sigma^{-1}(\mu_1-\mu_2)$$

令 $W(x)=a^\prime(x-\bar\mu)$，则上述分类规则可简化为：

$$\left\{\begin{array}{ll}
\boldsymbol{x} \in \pi_{1}, & \text { 若 }\; W(\boldsymbol{x}) \geq 0 \\
\boldsymbol{x} \in \pi_{2}, & \text { 若 }\; W(\boldsymbol{x})<0
\end{array}\right.$$

* 其中，$W(x)$ 称为两组距离的线性判别函数，$a$ 称为判别系数向量。 

### $\boldsymbol\Sigma_1\neq \boldsymbol\Sigma_2$ 的情况  
  
当两个组别的协方差矩阵不相等时，不能使用上面的简化判别规则而只能使用一般判别规则，即

$$\left\{\begin{array}{ll}
\boldsymbol{x} \in \pi_{1}, & \text { 若 }\; d^{2}\left(\boldsymbol{x}, \pi_{1}\right) \leq d^{2}\left(\boldsymbol{x}, \pi_{2}\right) \\
\boldsymbol{x} \in \pi_{2}, & \text { 若 }\; d^{2}\left(\boldsymbol{x}, \pi_{1}\right)>d^{2}\left(\boldsymbol{x}, \pi_{2}\right)
\end{array}\right.$$

也可采用另一种形式，即选择马氏距离的二次判别函数： 

$$\begin{aligned}
W(\boldsymbol{x}) &=d^{2}\left(\boldsymbol{x}, \pi_{1}\right)-d^{2}\left(\boldsymbol{x}, \pi_{2}\right) \\
&=\left(\boldsymbol{x}-\boldsymbol{\mu}_{1}\right)^{\prime} \Sigma_{1}^{-1}\left(\boldsymbol{x}-\boldsymbol{\mu}_{1}\right)-\left(\boldsymbol{x}-\boldsymbol{\mu}_{2}\right)^{\prime} \Sigma_{2}^{-1}\left(\boldsymbol{x}-\boldsymbol{\mu}_{2}\right)
\end{aligned}$$

上式是关于 $x$ 的二次函数，相应的判别规则为：  

$$\left\{\begin{array}{ll}
\boldsymbol{x} \in \pi_{1}, & \text { 若 }\; W(\boldsymbol{x}) \leq 0 \\
\boldsymbol{x} \in \pi_{2}, & \text { 若 }\; W(\boldsymbol{x})>0
\end{array}\right.$$  

检验两个组别的协方差矩阵是否相同：

* 使用程序包 multiUS 中的函数 BoxMTest。

  - 如果任何一组的观测数量大于 $20$（足够大），将使用 $\chi^2$ 检验，否则使用 $F$ 检验。
  
  - 参数 test 可取 "F"、"ChiSq" 或 "any"，默认取值为 test = "any"。

* 使用程序包 biotools 中的函数 BoxM。
  
马氏距离判别：

* 使用程序包 MASS 中的函数 lda 或 qda。

* 使用程序包 biotools 中的函数 D2.disc。  

### 两组距离判别示例
 
对破产的企业收集它们在破产前两年的年度财务数据，同时对财务良好的企业也收集同一时期的数据。数据涉及四个变量：
  
* $x_1$：现金流量/总债务 

* $x_2$：净收入/总资产

* $x_3$：流动资产/流动债务

* $x_4$：流动资产/净销售额

```{r}

# 读取数据
  mydata <- rio::import(file = "Data/examp5.2.3.xlsx") 
  anyNA(x = mydata)
  
  mydata <- mydata %>% 
    mutate(groups = factor(x = g))
    
  head(x = mydata, n = 3)

  attach(what = mydata)

# 计算两个组别中观测样例的数量
  DescTools::Freq(x = groups)

  detach(name = mydata)

``` 

```{r}

# 读取数据
  attach(what = mydata)

# 检验两个组别是否具有协同的协方差阵：以下两条命令结果完全相同
  
  # 方法 1
  multiUS::BoxMTest(X = mydata[1:4], cl = groups, alpha = 0.05, test = "ChiSq")

  # 方法 2
  biotools::boxM(data = mydata[1:4], grouping = groups)

  detach(name = mydata)

``` 

**假设两个组别具有相同的协方差矩阵：使用线性判别规则**

说明：为了演示线性判别分析，尽管检验结果显示两个组别的协差阵显著不同，这里仍然将协差阵视为相同。

```{r}

# 读取数据
  attach(what = mydata)  
  
# 两组马氏距离的线性判别：方法 1，使用 MASS 中的线性判别函数 lda，以列表形式返回结果
  library(MASS)
  lda_1 <- lda(groups ~ x1 + x2 + x3 + x4, data = mydata)

# 提取数据中的组别
  lda_1$lev

# 提取数据中的总观测数量
  lda_1$N

# 提取各组中的观测样例数量 
  lda_1$counts

# 提取在变量 groups 的水平上各个变量的均值
  
  # 方法 1：直接提取
  lda_1$means
  
  # 方法 2：用于验证
  aggregate(cbind(x1, x2, x3, x4) ~ groups, data = mydata, FUN = mean, na.rm = TRUE, 
            trim = 0)

# 提取线性判别函数系数
  lda_1$scaling

# 计算回测法的预测值
  preds <- predict(object = lda_1)

# 计算回测法中各个观测样例的得分值
  scores <- preds$x
  slice_sample(.data = as.data.frame(x = scores), n = 3)

# 计算回测法中各个观测样例的所属类别
  classes <- preds$class
  DescTools::Some(x = classes, n = 3)

# 计算回测法的混淆矩阵
  DescTools::Conf(x = classes, ref = groups)

# 计算预测错误的样例
  misclassifications <- data.frame(cases = 1:nrow(x = mydata), 
                                   predictions = groups, responses = classes) %>% 
    filter(groups != classes)
  misclassifications  

# 利用得到的线性判别规则计算新样例所属类别：以列表形式返回结果
  results <- predict(object = lda_1, 
                     newdata = data.frame(x1 = -0.16, x2 = -0.10, x3 = 1.45, x4 = 0.51))
  results$x
  results$class

  detach(name = mydata)

``` 

```{r}

# 读取数据
  attach(what = mydata)
  
# 两组马氏距离的线性判别：方法 2，使用 biotools 中的线性判别函数 D2.disc，以列表形式返回结果
  library(biotools)
  lda_2 <- D2.disc(data = mydata[1:4], grouping = groups, pooled.cov = NULL)  
  
# 提取在结果变量 groups 的水平上各个变量的均值
  lda_2$means

# 提取联合协差阵
  lda_2$pooled
  
# 提取回测法中的预测值：以矩阵形式返回结果
# X1：各个观测样例之间的马氏距离
# X2：各个观测样例与所在组均值之间的马氏距离
  lda_2$D2

# 提取回测法的混淆矩阵
  
  # 方法 1
  lda_2$confusion.matrix

  # 方法 2
  DescTools::Conf(x = lda_2$D2$pred, ref = lda_2$D2$grouping)

# 利用得到的线性判别规则计算新样例所属类别：以列表形式返回结果
  results <- predict(object = lda_2,
                     newdata = data.frame(x1 = -0.16, x2 = -0.10, x3 = 1.45, x4 = 0.51))
  results$D2
  results$class

  detach(name = mydata)

```

**上面的检验结果显示两个组别具有不同的协方差矩阵：使用二次判别规则**。

```{r}

# 读取数据
  attach(what = mydata)

# 两组马氏距离的二次判别：使用 MASS 中的二次判别函数 qda，以列表形式返回结果  
  library(MASS)  
  qda_1 <- qda(groups ~ x1 + x2 + x3 + x4, data = mydata, method = "moment", CV = FALSE)  

# 提取数据中的组别
  qda_1$lev  
  
# 提取数据中的总观测数量
  qda_1$N 
  
# 提取各组中的观测样例数量 
  qda_1$counts  
  
# 提取在变量 groups 的水平上各个变量的均值
  qda_1$means   
 
# 提取每个组别中观测数据变换后的结果：这里的变换是指使得组内协差阵为椭球体的变换
  qda_1$scaling  
  
# 计算回测法中各个观测样例的所属类别
  preds <- predict(object = qda_1)
  classes <- preds$class
  DescTools::Some(x = classes, n = 3)
  
# 计算回测法的混淆矩阵
  DescTools::Conf(x = classes, ref = groups)  

# 计算预测错误的样例
  misclassifications <- data.frame(cases = 1:nrow(x = mydata), 
                                   predictions = classes, responses = groups) %>% 
    filter(predictions != responses)
  misclassifications

# 计算预测值
  results <- predict(object = qda_1, 
                     newdata = data.frame(x1 = -0.16, x2 = -0.10, x3 = 1.45, x4 = 0.51))
  results$class
  
  detach(name = mydata)

``` 
  
## 基于平方马氏距离的判别：多组情形

### 一般判别规则

设有 $k$ 个组别 $\boldsymbol{\pi_1},\boldsymbol{\pi_2},\ldots,\boldsymbol{\pi_k}$，它们的均值分别为 $\boldsymbol{\mu_1},\boldsymbol{\mu_2},\ldots,\boldsymbol{\mu_k}$，协方差矩阵分别为 $\boldsymbol{\Sigma_1},\boldsymbol{\Sigma_2},\ldots,\boldsymbol{\Sigma_k}$，则样例 $x$ 到总体 $\boldsymbol{\pi_i}$ 的平方马氏距离为：
  
$$d^{2}\left(\boldsymbol{x}, \pi_{i}\right)=\left(\boldsymbol{x}-\boldsymbol{\mu}_{i}\right)^{\prime} \Sigma_{i}^{-1}\left(\boldsymbol{x}-\boldsymbol{\mu}_{i}\right)$$

判别规则为（该判别规则不受变量单位的影响）：
  
$$\boldsymbol{x} \in \pi_{l}, \; \text { 若 }\; d^{2}\left(\boldsymbol{x}, \pi_{l}\right)=\min _{1 \leq i \leq k} d^{2}\left(\boldsymbol{x}, \pi_{i}\right)$$

### 各组协方差矩阵相等时分类规则的简化  

当各组协方差矩阵相等时，分类规则简化为：

如果 $\boldsymbol I^\prime_l{\boldsymbol x}+c_l=\text{min}(\boldsymbol I^\prime_i{\boldsymbol x}+c_i)$，那么 $\boldsymbol{x}\in\boldsymbol{\pi_l}$。  
  
其中：

* $\boldsymbol I_i=\boldsymbol\Sigma^{-1}\boldsymbol\mu_i$

* $c_i=-\frac{1}{2}\boldsymbol\mu^{\prime}_i\boldsymbol\Sigma^{-1}\boldsymbol\mu_i$ 
 
* $\boldsymbol I^\prime_i{\boldsymbol x}+c_i$ 为线性判别函数。

### 各组协方差矩阵不全相等时的分类规则
 
当各个组别的协方差矩阵不全相等时，不能使用上面的简化判别规则而只能使用一般判别规则。

### 多组距离判别示例  

* 电视机品牌调查分析

```{r}

# 读取数据  
  tvSales <- rio::import(file = "Data/sale.csv")
  tvSales <- mutate(.data = tvSales, groups = factor(x = Group))
  anyNA(x = tvSales)
  attach(what = tvSales)
  
# 计算各组中的观测数量
  DescTools::Freq(x = groups)
  
# 检验各组的协方差阵是否存在显著差异
  library(multiUS)  
  BoxMTest(X = tvSales[2:4], cl = groups, alpha = 0.05, test = "F")  
  
  detach(name = tvSales)

```

由于各组的协方差阵不存在显著差异，因此使用线性判别函数。

```{r}
  
# 读取数据
  attach(what = tvSales)

# 多组平方马氏距离判别：使用 MASS 中的线性判别函数 lda，以列表形式返回结果
  library(MASS)
  lda_1 <- lda(groups ~ quality + performance + price, data = tvSales)
  
# 提取数据的总观测数量
  lda_1$N

# 提取各组中的观测数量
  lda_1$counts

# 提取在组别 groups 的各个水平上各个变量的均值
  lda_1$means  

# 提取线性判别函数系数
  lda_1$scaling  
  
# 计算回测法的预测值
  preds <- predict(object = lda_1)  
  classes <- preds$class

# 计算回测法的混淆矩阵
  DescTools::Conf(x = classes, ref = groups)

# 计算回测法中预测错误的样例
  misclassifications <- data.frame(cases = 1:nrow(x = tvSales), 
                                   predictions = classes, responses = groups) %>% 
    filter(predictions != responses)
  misclassifications

# 计算预测值：以列表形式返回结果
  results <- predict(object = lda_1, 
                     newdata = data.frame(quality = 8, performance = 7.5, price = 65))
  results$x
  results$class
  
  detach(name = tvSales)

```

```{r}

# 读取数据
  attach(what = tvSales)

# 多组平方马氏距离判别：使用 biotolls 中的线性判别函数 D2.disc，以列表形式返回结果 
  library(biotools)
  lda_2 <- D2.disc(data = tvSales[2:4], grouping = groups, pooled.cov = NULL)
  
# 提取在变量 groups 的各个组别上各个变量的均值
  
  # 方法 1
  lda_2$means

  # 方法 2：用于验证
  aggregate(as.matrix(x = tvSales[2:4]) ~ groups, data = tvSales, 
            FUN = mean, na.rm = TRUE, trim = 0)

# 提取联合协方差矩阵
  lda_2$pooled

# 提取回测法的混淆矩阵
  lda_2$confusion.matrix
  
# 提取回测法的平方马氏距离、预测值、预测错误的样例
  lda_2$D2
  filter(.data = lda_2$D2, misclass == "*")
  
# 计算预测值：以列表形式返回结果
  results <- predict(object = lda_2, 
                     newdata = data.frame(quality = 8, performance = 7.5, price = 65))
  results$D2
  results$class
    
  detach(name = tvSales)

```

```{r}

# 读取数据
  attach(what = tvSales)  
  
# 多组平方马氏距离判别：使用 multiUS 中的线性判别函数 ldaPlus，以列表形式返回结果  
  library(multiUS)
  lda_3 <- ldaPlus(x = tvSales[2:4], grouping = groups, pred = TRUE, CV = TRUE)
  
# 提取数据的总观测数量
  lda_3$N
  
# 提取数据中的组别
  lda_3$lev  
  
# 提取数据中各组的观测数量
  lda_3$counts

# 提取变量 groups 的各个水平上各个变量的均值
  lda_3$means

# 提取线性判别函数系数
  lda_3$scaling
  
# 提取迹比例
  lda_3$eigModel[, 3]

# 提取回测法的组别预测值
  lda_3$pred$class

# 提取回测法的列联表：输出结果含原始数据列联表、预测值列联表、预测准确率
  lda_3$class
  
# 提取交叉验证表格：输出结果含原始数据列联表、预测值列联表、预测准确率
  lda_3$classCV  
  
# 计算预测值
  results <- predict(object = lda_3, 
                     newdata = data.frame(quality = 8, performance = 7.5, price = 65))
  results$x
  results$class

  detach(name = tvSales)

```

```{r}
  
# 读取数据
  attach(what = tvSales)

# 多组平方马氏距离判别：使用 MASS 中的二次判别函数 qda，以列表形式返回结果
  library(MASS)
  tvSales_qda <- qda(x = tvSales[2:4], grouping = groups, method = "moment", CV = FALSE)

# 提取数据的总观测数量
  tvSales_qda$N

# 提取数据中的组别
  tvSales_qda$lev

# 提取数据中各组的观测数量
  tvSales_qda$counts

# 提取变量 groups 的各个水平上各个变量的均值
  tvSales_qda$means

# 提取线性判别函数系数
  tvSales_qda$scaling

# 计算回测法的预测值
  preds <- predict(object = tvSales_qda)
  classes <- preds$class

# 计算回测法的混淆矩阵
  DescTools::Conf(x = classes, ref = groups)

# 计算回测法中预测错误的样例
  misclassifications <- data.frame(cases = 1:nrow(x = tvSales), 
                                   predictions = classes, responses = groups) %>% 
    filter(predictions != responses)
  misclassifications

# 计算预测值：以列表形式返回结果
  results <- predict(object = tvSales_qda, 
                     newdata = data.frame(quality = 8, performance = 7.5, price = 65))
  results$class
  
  detach(name = tvSales)

```

## 距离判别分析的总结

### 判别分类的前提条件

* 除非各组均值向量之间有明显的差异，否则就不适合作判别分类。

* 在各组数据满足一定的条件下，可先进行多元方差分析。

  - 如果检验结果表明各个组别之间的均值不存在显著差异，则停止进行判别分类。

  - 如果检验结果表明各个组别之间的均值存在显著差异菜考虑使用判别分类，但这并不意味着所作的判别一定有效，最终还得看一下误判概率。
  
### 采用线性判别函数还是二次判别函数
    
基于理论的策略：
  
* 当各组协方差矩阵相等时，采用线性判别函数。

* 当各组协方差矩阵不全相等时，采用二次判别函数。但需要注意：

  - 如果出现 $n_i<p$ 或 $n_i$ 略大于 $p$ 的情况时，应采用线性判别函数。
  
两个简单且实用的策略：

* 一般而言，如果各组的样本容量普遍较小，则选择线性判别函数。反之，如果各组的样本容量都非常大，则更倾向于采用二次判别函数。

* 如果对使用线性判别函数还是二次判别函数拿不准，可以同时采用这两种方法分别进行判别，然后用交叉验证法来比较其误判概率的大小，以确定到底采用哪种方法更为合适。

* 以上策略同样适用于其它的判别方法。

## Fisher 判别

### Fisher 判别的基本思想

Fisher 判别（Fisher discriminant analysis, FDA）也称典型判别（canonical discriminant），其基本思想是投影（或降维）。

* Fisher 判别是用 $p$ 维向量 $x=(x_1,x_2,\ldots,x_p)^{\prime}$ 中的少数几个变量的线性组合（称为费希尔判别函数或典型变量，一般 $r$ 明显小于 $p$）来代替原始的 $p$ 个变量，从而实现降维的目的，并根据这 $r$ 个判别函数 $y_1,y_2,\ldots,y_r$ 对样本中观测样例的归属作出判别分类或对各组别进行分离。

$$y_1=a_1^\prime {x},\;y_2=a_2^\prime {x},\;\ldots,\; y_r=a_r^\prime {x}$$
 
在对原始的 $p$ 维向量成功地降至二维或三维后，就可以针对降维后的数据（也称得分）绘制散点图，从直观的几何图形上区分各个组别。
  
Fisher 判别最终能够把多维数据投影到一维直线上，使得同类数据尽量接近异类数据尽量分开。从方差分析角度看，就是组内变差要小组间变差要大。

### 一个说明性的二维示例  

```{r}

# 两类别 Fisher 线性判别分析
  img <- readPNG(source = "Pictures/两类别FIsher判别分析.png")
  grid.raster(image = img)
  
```  

### Fisher 判别函数

设来自组 $\pi_i$ 的 $p$ 维观测值为 $x_{ij},\quad j=1,2,\ldots,n_i,\quad i=1,2,\ldots,k$，$k$ 是组别的数量。记：

* 各组之间的差异程度用组间矩阵表示：$$H=\sum_{i=1}^{k}n_i{(\overline{X_i} - \overline{X})} {(\overline{X_i} - \overline{X})^\prime}$$
  
* 组内观测样例之间的差异程度用组内矩阵表示：$$E=\sum_{i=1}^{k}{(n_i-1)}S_i$$  
 
* 协差阵 $\Sigma$ 的联合无偏估计为：$$S_p=\frac{E}{n-k}$$

* Fisher 判别需要假定各个组别之间具有相同的协差阵：$$\boldsymbol{\Sigma_1}=\boldsymbol{\Sigma_2}=\cdots=\boldsymbol{\Sigma}$$ 

  - 检验各个组别之间是否具有相同的协方差阵，使用程序包 multiUS 中的函数 BoxMTest 或程序包 biotools 中的函数 boxM。 

设 $E^{-1}H$ 的全部正特征根依次为 $\lambda_1\geq\lambda_2\geq\cdots\geq\lambda_s>0$，$s=\text{rank}(H)$ 且有 $s\leq\text{min}(k-1,p)$，相应的特征向量依次记为 $t_1,t_2,\cdots,t_s$，则称 $y_{i}=t_i^{\prime}x$ 为 Fisher 第 $i$ 线性判别函数或第 $i$ 典型变量，简称第 $i$ 判别函数。

* 通常情况下，取 $s=k-1$。
  
* $\lambda_i$ 表示 $y_i$ 对分离各组的贡献大小，也就是 $y_i$ 代表全部 $y_1,y_2,\cdots,y_s$ 进行判别的能力。$y_i$ 在所有 $s$ 个判别函数中的贡献率为：$$\lambda_i/\Sigma_{j=1}^s(\lambda_j)$$

  - 可见，$y_1,y_2,\cdots,y_s$ 对分离各组的贡献依次递减。
  
  - 在实际应用中，如果前 $r$ 个判别函数的累计贡献率已达到了一个较高的比例，则就采用这 $r$ 个判别函数进行判别。

### Fisher 判别函数的特点

* 各判别函数都具有单位（联合样本）方差。

* 各判别函数彼此之间不相关，也就是彼此之间的联合样本协方差为零。

* 判别函数在方向上并不是彼此正交的，但作图时仍将它们画成直角坐标系，虽有些变形，但通常并不严重。

* 判别函数不受变量度量单位的影响。
  
* 各个组别如能在前几个判别函数构成的低维空间中得到较好的分离，那么在原始变量的更高维空间中一般也会得到较好的分离，但反之未必。

* 费希尔判别虽是一种很好的降维投影方法，但该方法也有其不适用的场合。如，Fisher 判别函数不适用于非线性的情况。

```{r}

# Fisher 线性判别
  img <- readPNG(source = "Pictures/Fisher 不适合非线性.png")
  grid.raster(image = img)
  
```
    
### Fisher 判别规则：两组情形
 
**两组情形**

* 对于两组的判别，Fisher 判别函数只有一个。

* 两组的 Fisher 判别等价于协方差矩阵相等的距离判别。

* 如果两个组别服从多元正态分布，那么 Fisher 判别还等价于协方差矩阵相等且先验概率和误判代价也均相同的贝叶斯判别。

* 多组的 Fisher 判别规则在实践中很少采用。

### Fisher 判别规则：多组情形
  
有多个组别时，需要找出多个投影方向，分别建立多个判别函数。由于各判别函数都具有单位方差且彼此不相关，故此时的马氏距离等同于欧氏距离，因此可把各线性判别函数值看作是数据降维后的值，对降维后的数据使用距离判别法。
  
### Fisher 判别函数得分图

* 判别函数得分图既可用于分类也可用于分离，但主要用于分离。

* 为作图的目的，一般取 $r=2$，偶尔取 $r=3$。
  
* 当取 $r=2$ 时，可将各个观测对象的两个判别函数得分画成平面直角坐标系上的散点图，用目测法对新样品的归属进行辨别或对来自各组样品的分离情况及结构进行观测评估。
  
* 当 $r=3$ 时，可作（三维）旋转图从多角度来辨别新样品的归属或观测评估各组之间的分离效果，但其目测效果一般明显不如 $r=2$ 时清楚。
  
* 能够利用降维后生成的图形进行直观判别是 Fisher 判别的最重要应用，图中常常能清晰地展示出丰富的信息，如发现构成各组的结构、离群观测样例点或数据中的其他异常情况等。

在 R 中，计算 Fisher 线性判别分析可使用的方法：

* 程序包 MASS 中的函数 lda

* 程序包 multiUS 中的函数 ldaPlus

* 程序包 robCompositions 中的函数 daFisher：主要用于计算组间方差矩阵、组内方差矩阵、绘制典型变量得分图。

### Fisher 线性判别示例

Fisher 于 1936 年发表的鸢尾花（Iris）数据是对 $3$ 种鸢尾花：刚毛鸢尾花（第 Ⅰ 组）、变色鸢尾花（第 Ⅱ 组）和弗吉尼亚鸢尾花（第 Ⅲ组 ）各抽取一个容量为 $50$ 的样本，测量其花萼长（SL）、花萼宽（SW）、花瓣长（PL）、花瓣宽（PW），单位为 mm。

```{r}

# 读取数据
  attach(what = iris)

# 计算三种鸢尾花的观测数量
  DescTools::Freq(x = Species, useNA = "ifany")
  
# 检验各个组别之间是否具有相同的协方差
  multiUS::BoxMTest(X = iris[1:4], cl = Species, alpha = 0.05, test = "ChiSq")
  biotools::boxM(data = iris[1:4], grouping = Species)
  
  detach(name = iris)

``` 

```{r}

# 读取数据
  attach(what = iris)

# Fisher 线性判别分析：首选方法，以列表形式返回结果
  library(multiUS)
  lda_1 <- ldaPlus(x = iris[1:4], grouping = Species, pred = TRUE, CV = TRUE)
  lda_1
  
# 提取总观测数量
  lda_1$N
  
# 提取各个组别的观测数量
  lda_1$counts
  
# 提取各个组别上各个变量的均值
  lda_1$means
  
# 提取特征根与典型相关系数
  lda_1$eigModel
  
# 提取第一和第二线性判别系数
  lda_1$scaling
  
# 提取判别函数的组内标准化系数
  lda_1$standCoefWithin
  
# 提取判别函数的标准化系数
  lda_1$standCoefTotal

# 提取各个变量之间典型相关系数的显著性检验：原假设为各个变量之间典型相关系数为零
  lda_1$sigTest
  
# 提取不同组别的质心
  lda_1$centroids
  
# 提取回测预测结果：以列表形式返回结果
  preds <- lda_1$pred
  
  classes <- preds$class
  length(x = classes)
  DescTools::Some(x = classes, n = 3)
  
# 提取回测混淆矩阵
  lda_1$class  
  
# 提取交叉检验的混淆矩阵
  lda_1$classCV
  
# 计算回测法中预测错误的样例
  misclassifications <- data.frame(cases = 1:nrow(x = iris), 
                                   predictions = classes, responses = Species) %>% 
    filter(predictions != responses)
  misclassifications
  
# 绘制 Fisher 判别函数得分图：投影后的得分图 
  mydata <- data.frame(preds$x, preds$class, Species)
  
  ggplot(data = mydata, mapping = aes(x = LD1, y = LD2, color = preds.class)) +
    geom_point(mapping = aes(shape = Species), size = 2) +
    stat_ellipse() +
    labs(color = "Predictions", title = "Fisher 判别函数得分图") +
    theme_bw()
  
  detach(name = iris)

```

```{r}

# 读取数据
  attach(what = iris)
  
# 计算 Fisher 线性判别分析：方法 2，以列表形式返回结果
  library(MASS)
  lda_2 <- lda(Species ~ ., data = iris)
  print(x = lda_2)

# 计算回测法预测结果
  preds <- predict(object = lda_2)

# 计算回测法的混淆矩阵
  DescTools::Conf(x = preds$class, ref = Species)

# 绘制 Fisher 判别函数得分图：投影后的得分图
  mydata <- data.frame(preds$x, preds$class, Species)
  
  ggplot(data = mydata, mapping = aes(x = LD1, y = LD2, color = preds.class)) +
    geom_point(mapping = aes(shape = Species), size = 2) +
    stat_ellipse() +
    labs(color = "Predictions", title = "Fisher 判别函数得分图") +
    theme_bw()
  
# 绘制 Fisher 判别函数得分图：投影后的得分图，使用文本标签 
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  plot(lda_2, cex = 0.6, abbrev = FALSE, col = "blue")
  panel.first = grid()
  par(opar)
  
# 绘制 Fisher 判别函数得分图：投影后的得分图
  ldahist(data = preds$x[, 1], g = preds$class, h = 0.5, col = "steelblue", sep = TRUE,
          type = "both", bty = "n")
  
  detach(name = iris) 
  
```

```{r}

# 读取数据
  attach(what = iris)

# 计算组间方差矩阵、组内方差矩阵、绘制典型变量得分图：以列表的形式返回结果
  library(robCompositions)
  lda_3 <- daFisher(x = iris[1:4], grp = Species, method = "classical", plotScore = TRUE)
  
  is.list(x = lda_3)
  names(x = lda_3)

  lda_3
  summary(object = lda_3)

# 提取组间方差矩阵
  lda_3$B

# 提取组内方差矩阵
  lda_3$W

# 提取载荷矩阵
  lda_3$loadings

# 提取 Fisher 判别函数得分（典型变量得分）：以矩阵和数组形式返回结果
  scores <- lda_3$scores
  class(x = scores)
  
# 提取预测的组别
  lda_3$grppred
  
# 提取实际的组别
  lda_3$grp

# 提取混淆矩阵
  lda_3$mc

# 提取错误分类率
  lda_3$mcrate

# 提取各组别的总体均值
  lda_3$meanj

# 提取各组别的比例
  lda_3$pj

# 手动绘制典型变量得分图
  mydata <- data.frame(LD1 = lda_3$fdiscr[, 1], LD2 = lda_3$fdiscr[, 2],
                       predictions = lda_3$grppred, responses = lda_3$grp)
  
  ggplot(data = mydata, mapping = aes(x = LD1, y = LD2, color = responses)) +
    geom_point(mapping = aes(shape = predictions))

  detach(name = iris)

```
 
## 贝叶斯判别：最大后验概率法 

### 最大后验概率法的定义

设有 $k$ 个组 $\pi_i, i=1,2,\cdots,k$ 且组 $\pi_i$ 的概率密度为 $f_i(x)$，样例 $x$ 来自组 $\pi_i$ 的先验概率为 $p_i$，满足 $p_1+p_2+\cdots+p_k=1$。则样例 $x$ 属于 $\pi_i$ 的后验概率为： 

$$P\left(\pi_{i} \mid \boldsymbol{x}\right)=\frac{p_{i} f_{i}(\boldsymbol{x})}{\sum_{j=1}^{k} p_{j} f_{j}(\boldsymbol{x})}, \quad i=1,2, \cdots, k$$  
  
最大后验概率法的判别规则为：将样例判别为后验概率最大的组别。  
  
$$\boldsymbol{x} \in \pi_{l}, \quad \text { 若 } P\left(\pi_{l} \mid \boldsymbol{x}\right)=\max _{1 \leq i \leq k} P\left(\pi_{i} \mid \boldsymbol{x}\right)$$
### 皆为正态组的情形

设各个组别 $\boldsymbol{\pi_i}$ 均服从均值向量为 $\boldsymbol{\mu_i}$、协方差矩阵为 $\boldsymbol{\Sigma_i}$ 的多元正态分布，组 $\boldsymbol{\pi_i}$ 的概率密度为：

$$f_{i}(\boldsymbol{x})=(2 \pi)^{-p / 2}\left|\boldsymbol{\Sigma}_{i}\right|^{-1 / 2} \exp \left[-0.5 d^{2}\left(\boldsymbol{x}, \pi_{i}\right)\right]$$

其中，$d^2(x,\boldsymbol{\pi_i})$ 是样例 $x$ 到 $\boldsymbol{\pi_i}$ 的平方马氏距离。
  
$$d^{2}\left(\boldsymbol{x}, \pi_{i}\right)=\left(\boldsymbol{x}-\boldsymbol{\mu}_{i}\right)^{\prime} \Sigma_{i}^{-1}\left(\boldsymbol{x}-\boldsymbol{\mu}_{i}\right)$$

后验概率为：
  
$$P\left(\pi_{i} \mid \boldsymbol{x}\right)=\frac{p_{i} f_{i}(\boldsymbol{x})}{\sum_{j=1}^{k} p_{j} f_{j}(\boldsymbol{x})}=\frac{\exp \left[-\frac{1}{2} D^{2}\left(\boldsymbol{x}, \pi_{i}\right)\right]}{\sum_{j=1}^{k} \exp \left[-\frac{1}{2} D^{2}\left(\boldsymbol{x}, \pi_{j}\right)\right]}, \quad i=1,2, \cdots, k$$  

其中，$D^2(x,\boldsymbol{\pi_i})=d^2(x,\boldsymbol{\pi_i})+g_i+h_i$ 是样例 $x$ 到 $\boldsymbol{\pi_i}$ 的广义平方马氏距离。
  
$$g_{i}=\left\{\begin{array}{ll}
\ln \left|\Sigma_{i}\right|, & \text { 若 } \Sigma_{1}, \Sigma_{2}, \cdots, \Sigma_{k} \text { 不全相等 } \\
0, & \text { 若 } \Sigma_{1}=\Sigma_{2}=\cdots=\Sigma_{k}=\Sigma
\end{array}\right.$$

$$h_{i}=\left\{\begin{array}{ll}
-2 \ln p_{i}, & \text { 若 } p_{1}, p_{2}, \cdots, p_{k} \text { 不全相等 } \\
0, & \text { 若 } p_{1}=p_{2}=\cdots=p_{k}=\frac{1}{k}
\end{array}\right.$$

* 当 $p_1=p_2=\cdots=p_k=1/k$ 且各组的协方差矩阵 $\boldsymbol{\Sigma_i}$ 都相等时，上面的后验概率简化为：

$$P\left(\pi_{i} \mid \boldsymbol{x}\right)=\frac{\exp \left(\boldsymbol{I}_{i}^{\prime} \boldsymbol{x}+c_{i}+\ln p_{i}\right)}{\sum_{j=1}^{k} \exp \left(\boldsymbol{I}_{j}^{\prime} \boldsymbol{x}+c_{j}+\ln p_{j}\right)}, \quad i=1,2, \cdots, k$$

此时，最大后验概率法的判别规则等价于距离判别。

如果我们对样例 $x$ 来自哪一组的先验信息一无所知或难以确定，则一般可取 $p_1=p_2=\cdots=p_k=1/k$。

贝叶斯最大后验概率法的计算使用程序包 MASS 中的函数 lda 或 qda。

* 函数 lda 适用于等协差矩阵的情况，函数 qda 适用于协差矩阵不全相等的情况。

### 贝叶斯最大后验概率法示例

**假定两组皆为正态组，各个组别的协差矩阵全相等且先验概率也全相等的情况：使用线性贝叶斯判别**   

```{r}

# 读取数据
  mydata <- rio::import(file = "Data/examp5.2.3.csv")
  anyNA(x = mydata)
  attach(what = mydata)

# 计算各组别中的观测数量
  DescTools::Freq(x = factor(x = g), include.lowest = TRUE)
  
# 贝叶斯最大后验概率法：方法 2
  library(multiUS)
  bayes_1 <- ldaPlus(x = mydata[1:4], grouping = g, pred = TRUE, CV = TRUE, 
                     usePriorBetweenGroups = TRUE, prior = c(0.5, 0.5))  
  bayes_1

# 提取先验概率
  bayes_1$prior

# 提取回测法预测概率与预测类别
  preds <- bayes_1$pred
  
  post_probs <- preds$posterior
  DescTools::Some(x = post_probs, n = 3)

  classes <- preds$class
  classes

# 提取回测法的混淆矩阵
  bayes_1$class

# 提取回测法的交叉验证混淆矩阵
  bayes_1$classCV

# 计算预测错误的样例
  misclassifications <- data.frame(cases = 1:nrow(x = mydata), 
                                   predictions = classes, responses = g) %>% 
    filter(predictions != responses)
  misclassifications
  
# 计算新样例的预测类别
  results <- predict(object = bayes_1, prior = c(0.5, 0.5),
                     newdata = data.frame(x1 = -0.16, x2 = -0.10, x3 = 1.45, x4 = 0.51))
  results$class
  results$posterior

  detach(name = mydata)

```

```{r}

# 读取数据
  attach(what = mydata)
  
# 贝叶斯最大后验概率法：方法 2，不使用交叉验证
  library(MASS)
  bayes_2 <- lda(g ~ ., data = mydata, prior = c(0.5, 0.5), method = "moment")
  bayes_2

# 计算回测法的混淆矩阵
  preds <- predict(object = bayes_2)
  DescTools::Conf(x = preds$class, ref = g) 
  
# 计算预测错误的样例
  misclassifications <- data.frame(cases = 1:nrow(x = mydata), 
                                   presictions = preds$class, responses = g) %>% 
    filter(presictions != responses)
  misclassifications
  
# 计算新样例的预测类别
  results <- predict(object = bayes_1, prior = c(0.5, 0.5),
                     newdata = data.frame(x1 = -0.16, x2 = -0.10, x3 = 1.45, x4 = 0.51))
  results$class
  results$posterior
  
  detach(name = mydata)

``` 

**假定两组皆为正态组，各个组别的协差阵全相等且使用给定的先验概率：使用线性贝叶斯判别**  

```{r}
  
# 读取数据
  attach(what = mydata)  
 
# 贝叶斯最大后验概率法
  library(MASS)
  bayes <- lda(g ~ ., data = mydata, prior = c(0.1, 0.9))
  
# 提取先验概率
  bayes$prior

# 计算新样本的预测值
  results <- predict(object = bayes, 
                     newdata = data.frame(x1 = -0.16, x2 = -0.10, x3 = 1.45, x4 = 0.51))
  results$posterior
  results$class
  
  detach(name = mydata)

```

**假定两组皆为正态组，各个组别的协差阵全相等且使用样本比例作为先验概率：使用线性贝叶斯判别**  

```{r}
  
# 读取数据
  attach(what = mydata)
   
# 贝叶斯最大后验概率法
  library(MASS)
  bayes <- lda(g ~ ., data = mydata, method = "moment")
  
# 提取先验概率
  bayes$prior

# 计算新样本的预测值
  results <- predict(object = bayes, 
                     newdata = data.frame(x1 = -0.16, x2 = -0.10, x3 = 1.45, x4 = 0.51))
  results$posterior
  results$class
  
  detach(name = mydata)

```

**假定两组皆为正态组，各个组别的协差阵不全相等但先验概率全相等的情况：使用二次贝叶斯判别**  

```{r}

# 读取数据
  attach(what = mydata)
   
# 贝叶斯最大后验概率法：先验概率相等但协差阵不全相等
  library(MASS)
  bayes_qda <- qda(g ~ ., data = mydata, prior = c(0.5, 0.5))

# 提取先验概率
  bayes_qda$prior

# 计算新样本的预测值
  results <- predict(object = bayes_qda, 
                     newdata = data.frame(x1 = -0.16, x2 = -0.10, x3 = 1.45, x4 = 0.51)) 
  results$posterior
  results$class
  
  detach(name = mydata)

```  

**假定两组皆为正态组，各个组别的协方差矩阵不全相等并且使用给定的先验概率：使用二次贝叶斯判别**  

```{r}

# 读取数据
  attach(what = mydata)

# 贝叶斯最大后验概率法：使用给定的先验概率但协方差矩阵不全相等
  library(MASS)
  bayes_qda <- qda(g ~., data = mydata, prior = c(0.1, 0.9))  

# 提取先验概率
  bayes_qda$prior  
  
# 计算新样本的预测值
  results <- predict(object = bayes_qda, 
                     newdata = data.frame(x1 = -0.16, x2 = -0.10, x3 = 1.45, x4 = 0.51)) 
  results$posterior
  results$class  
  
  detach(name = mydata)
  
```

**假定两组皆为正态组，各个组别的协方差矩阵不全相等并且使用样本比例做先验概率：使用二次贝叶斯判别**  

```{r}

# 读取数据
  attach(what = mydata)
  
# 贝叶斯最大后验概率法：使用样本比例做先验概率但协方差矩阵不全相等
  library(MASS)
  bayes_qda <- qda(g ~., data = mydata)  

# 提取先验概率
  bayes_qda$prior  
  
# 计算新样本的预测值
  results <- predict(object = bayes_qda, 
                     newdata = data.frame(x1 = -0.16, x2 = -0.10, x3 = 1.45, x4 = 0.51)) 
  results$posterior
  results$class  
  
  detach(name = mydata)
  
```  

## 贝叶斯判别：最小期望误判代价法

### 最大后验概率法不适用的示例  
  
令 $\pi_1$ 为合格的药，$\pi_2$ 为不合格的药，对于新样例 $x$ 有：

$$P\left(\pi_{1} \mid \boldsymbol{x}\right)=0.6, \quad P\left(\pi_{2} \mid \boldsymbol{x}\right)=0.4$$
  
该问题中，两种误判造成的损失明显是不同的，仅仅根据后验概率的大小进行判别是不合适的。

### 两组的一般情形

设组 $\boldsymbol{\pi_1}$ 和 $\boldsymbol{\pi_2}$ 的概率密度分别为 $f_1(x)$ 和 $f_2(x)$，组 $\boldsymbol{\pi_1}$ 和 $\boldsymbol{\pi_2}$ 的先验概率分别为 $p_1$ 和 $p_2$ 且 $p_1+p_2=1$。误判代价矩阵为：

```{r}

# 误判代价矩阵
  img <- readPNG(source = "Pictures/误判代价矩阵.png")
  grid.raster(image = img)
  
``` 

* 在按给定的判别规则对新样例 $x$ 进行判别时，其误判代价 $c(l\mid i)$ 是一个随机变量，因为 $c(l\mid i)$ 是随机变量 $i$ 和 $l$ 的函数。

* 平均或期望误判代价(expected cost of misclassification)，记为 ECM：

$$\text{ECM}=E[c(l\mid i)]=c(2\mid 1){p_1}{P(2\mid 1)}+c(1\mid 2){p_2}{P(1\mid 2)}$$

  - 给定了判别规则，误判概率 $P(2|1)$ 和 $P(1|2)$ 就可算出，由于先验概率 $p_1$ 和 $p_2$ 是事先给定的，从而就可计算得出 ECM。

* 最小期望误判代价法采用的是使 ECM 达到最小的判别规则：

$$\left\{\begin{array}{ll}
\boldsymbol{x} \in \pi_{1}, & \text { 若 }\; \frac{f_{1}(\boldsymbol{x})}{f_{2}(\boldsymbol{x})} \geq \frac{c(1 \mid 2) p_{2}}{c(2 \mid 1) p_{1}} \\
\boldsymbol{x} \in \pi_{2}, & \text { 若 }\; \frac{f_{1}(\boldsymbol{x})}{f_{2}(\boldsymbol{x})}<\frac{c(1 \mid 2) p_{2}}{c(2 \mid 1) p_{1}}
\end{array}\right.$$  
  
### 两组的一些特殊情形

* 当 $p_1=p_2=0.5$ 时，判别规则可简化为：
 
$$\left\{\begin{array}{ll}
\boldsymbol{x} \in \pi_{1}, & \text { 若 } \frac{f_{1}(\boldsymbol{x})}{f_{2}(\boldsymbol{x})} \geq \frac{c(1 \mid 2)}{c(2 \mid 1)} \\
\boldsymbol{x} \in \pi_{2}, & \text { 若 } \frac{f_{1}(\boldsymbol{x})}{f_{2}(\boldsymbol{x})}<\frac{c(1 \mid 2)}{c(2 \mid 1)}
\end{array}\right.$$  

  - 实践中，如果先验概率难以给出，则它们通常被取成相等。  

* 当 $c(1|2)= c(2|1)$ 时，判别规则可简化为：

$$\left\{\begin{array}{ll}
\boldsymbol{x} \in \pi_{1}, & \text { 若 }\; P\left(\pi_{1} \mid \boldsymbol{x}\right) \geq P\left(\pi_{2} \mid \boldsymbol{x}\right) \\
\boldsymbol{x} \in \pi_{2}, & \text { 若 }\; P\left(\pi_{1} \mid \boldsymbol{x}\right)<P\left(\pi_{2} \mid \boldsymbol{x}\right)
\end{array}\right.$$

  - 实践中，若误判代价比无法确定则它们通常被取成相等，即取比值为 $1$。此时，最小期望误判代价法等价于最大后验概率法。
  
* 当 $p_1=p_2=0.5$ 且 $c(1|2)= c(2|1)$ 时，判别规则可简化为：
  
$$\left\{\begin{array}{ll}
\boldsymbol{x} \in \pi_{1}, & \text { 若 }\; f_{1}(\boldsymbol{x}) \geq f_{2}(\boldsymbol{x}) \\
\boldsymbol{x} \in \pi_{2}, & \text { 若 }\; f_{1}(\boldsymbol{x})<f_{2}(\boldsymbol{x})
\end{array}\right.$$
 
  - 此时，只需要比较样例在两个组别上的概率密度的大小即可。   
  
### 两个正态组的情形

* 在两组皆为正态组且协差阵相等的情形下，最小期望误判代价法等价于协方差矩阵相等的距离判别规则，且可使两个误判概率之和或平均误判概率达到最小。

### 多组的情形

对于多组别的情况，使 ECM 达到最小的判别规则为：
  
$$\boldsymbol{x} \in \pi_{l} \text {, 若 } \sum_{\substack{j=1 \\ j \neq l}}^{k} p_{j} c(l \mid j) f_{j}(\boldsymbol{x})=\min _{\substack{1 \leq i \leq k}} \sum_{\substack{j=1 \\ j \neq i}}^{k} p_{j} c(i \mid j) f_{j}(\boldsymbol{x})$$

* 如果所有组别的误判代价都是相同的，则此时判别规则可简化为：
  
$$\boldsymbol{x} \in \pi_{l}, \quad \text { 若 } p_{l} f_{l}(\boldsymbol{x})=\max _{1 \leq i \leq k} p_{i} f_{i}(\boldsymbol{x})$$
  
  - 它等价于最大后验概率法的判别规则。
 
* 如果所有组别的误判代价以及先验概率都是相同的，则此时判别规则可简化为：
  
$$\boldsymbol{x} \in \pi_{l}, \quad \text { 若 } f_{l}(\boldsymbol{x})=\max _{1 \leq i \leq k} f_{i}(\boldsymbol{x})$$ 

### 贝叶斯判别与距离判别的比较

* 贝叶斯判别考虑到了先验概率和误判代价，而距离判别没有此考虑，也不涉及密度。前者较为精细，且可给出最优判别规则；而后者相对粗糙，但主要优点是简单、直观，且没什么假定。

* 在不考虑先验概率和误判代价的情形下（相当于都取成相等），当各组协差阵相同时，各组正态性假定下的贝叶斯判别等价于距离判别。

* 在判别分类中，无论在理论上还是应用上，贝叶斯判别都居主导地位，其重要性明显在距离判别之上。 

```{r}

# 读取数据
  data("response", package = "CustomerScoringMetrics")
  class(x = response)
  names(x = response)
  attach(what = response)
  
# 计算期望误判代价 ECM：以列表形式返回结果
  library(CustomerScoringMetrics)  
  
  # 使用 0.5 作为划分类别的阈值
  emc_1 <- expMisclassCost(predTest = response$test[, 2], depTest = response$test[, 1],
                           costType = "costRatio", costs = 5, cutoff = 0.5)
  print(x = emc_1$EMC)
  
  # 使用交叉验证方法动态确定阈值
  emc_2 <- expMisclassCost(predTest = response$test[, 2], depTest = response$test[, 1],
                         costType = "costRatio", costs = 5, dyn.cutoff = TRUE, 
                         predVal = response$val[, 2], depVal = response$val[, 1])
  print(x = emc_2$EMC)
  
  detach(name = response)
  
```       

```{r}

# 双总体最小 ECM 贝叶斯判别函数
  discriminant.bayes <- function(TrnX1, TrnX2, rate = 1, TstX = NULL,
                                 var.equal = FALSE){
    if (is.null(TstX) == TRUE) TstX <- rbind(TrnX1,TrnX2)
    if (is.vector(TstX) == TRUE) TstX <- t(as.matrix(TstX))
    else if (is.matrix(TstX) != TRUE)
      TstX <- as.matrix(TstX)
    if (is.matrix(TrnX1) != TRUE) TrnX1 <- as.matrix(TrnX1)
    if (is.matrix(TrnX2) != TRUE) TrnX2 <- as.matrix(TrnX2)
  
  nx <- nrow(TstX)
  blong <- matrix(rep(0, nx), nrow = 1, byrow = TRUE, 
                  dimnames = list("blong", 1:nx))
  mu1 <- colMeans(TrnX1); mu2 <- colMeans(TrnX2)
  
  if (var.equal == TRUE || var.equal == T){
    S <- var(rbind(TrnX1, TrnX2))
    beta <- 2 * log(rate)   # W 前面的1/2乘到 beta 上面去了
    W <- mahalanobis(TstX,mu2,S) - mahalanobis(TstX, mu1, S)
    }
  else{
    S1 <- var(TrnX1); S2 <- var(TrnX2)
    beta <- 2*log(rate) + log(det(S1)/det(S2))
    W <- mahalanobis(TstX, mu2, S2) - mahalanobis(TstX, mu1, S1)
    }
  
  for (i in 1:nx){
    if (W[i] > beta)
      blong[i] <- 1
    else
      blong[i] <- 2
  }
  
  blong
  }

```

说明：

* 参数 TrnX1 与 Trnx2 分别表示 X1 类、X2 类训练样本，其传入格式是数据框或矩阵（样本按行输入）。

* 参数 $\text{rate}=\frac{L(1|2)}{L(2|1)} \cdot \frac{p_1}{p_2}$，默认设置为 1。

* 参数 TstX 是待测样本，其传入格式是数据框，或矩阵（样本按行输入），或向量（一个待测样本）。

  - 如果不输入 TstX（默认设置），则待测样本为两个训练样本之和，即计算训练样本的回测值。
  
* 参数 var.equal 是逻辑变量，var.equal = TRUE 表示认为两总体的协方差阵是相同的，var.equal = FALSE（默认设置）表示认为两总体的协方差阵是不同的。

* 函数的输出是由"1"和"2"构成的的一维矩阵，"1"表示待测样本属于 X1 类，"2"表示待测样本属于 X2 类。

```{r}

# 多总体最小 ECM 准则贝叶斯分类函数
  distinguish.bayes <- function(TrnX, TrnG, p = rep(1, length(levels(TrnG))),
                                TstX = NULL, var.equal =FALSE){
    if (is.factor(TrnG) == FALSE){
      mx <- nrow(TrnX); mg <- nrow(TrnG)
      TrnX <- rbind(TrnX, TrnG)
      TrnG <- factor(rep(1:2, c(mx, mg)))
    }
    
    if (is.null(TstX) == TRUE) TstX <- TrnX
    if(is.vector(TstX) == TRUE) TstX <- t(as.matrix(TstX))
    else if (is.matrix(TstX) != TRUE)
      TstX <- as.matrix(TstX)
    if (is.matrix(TrnX) != TRUE) TrnX <- as.matrix(TrnX)
    
    nx <- nrow(TstX)
    blong <- matrix(0, nrow = 1,ncol = nx, dimnames = list("blong", 1:nx))
    g <- length(levels(TrnG))
    mu <- matrix(0, nrow = g, ncol = ncol(TrnX))
    
    for (i in 1:g)
      mu[i, ] <- colMeans(TrnX[TrnG == i, ])
    D <- matrix(0, nrow = g, ncol = nx)  
    
    if (var.equal == TRUE || var.equal == T){
      for (i in 1:g){
        d2 <- mahalanobis(TstX, mu[i, ], var(TrnX))
        D[i, ] <- d2 - 2*log(p[i])
      }
    }
    
    else{
      for (i in 1:g){
        S <- var(TrnX[TrnG == i, ])
        d2 <- mahalanobis(TstX, mu[i, ], S)
        D[i,] = d2 - 2*log(p[i]) - log(det(S))
      }
    }
    
    for (j in 1:nx){
      dmin <- Inf
      for (i in 1:g){
        if (D[i,j] < dmin){
          dmin <- D[i,j]; blong[j] <- i
        }
      }
      }
    blong
  }

```

说明：

* 函数分别考虑了总体协方差阵相同和协方差阵不同的情况。

* 参数 Trnx表示训练样本，其传入格式是矩阵（样本按行输入），或数据框。

* 参数 TrnG 是因子型变量，表示训练样本的分类情况。

* 参数 $p$ 是先验概率，缺省值均为 $1$。

* 参数 TstX 是待测样本，其传入格式是矩阵（样本按行输入），或数据框，或向量（一个待测样本）。
  
  - 如果不传入 TstX（默认设置），则待测样本为训练样本。
  
* 参数 var.equal 是逻辑变量，var.equal = TRUE 表示认为总体协方差阵是相同的，var.equal = FALSE（默认设置）表示认为总体协方差阵不是全部相同的。

* 函数的输出是由数值构成的的一维矩阵，数值表示相应的类。

# 聚类分析

## 概述

### 聚类分析的概念

* 聚类分析将分类对象分成若干类，相似的对象归为同一类。

* 聚类分析常常用于探索寻找“自然的”分类，并且这样的分类应是对所研究的问题有意义的。

针对分类对象的不同，聚类分析可划分为：

* $Q$ 型聚类：分类对象为观测样例。

* $R$ 型聚类：分类对象为变量。  
  
从数学上看，上述两种聚类没有本质的差别，因为只要把观测矩阵转置就转换为另一种聚类问题。

聚类分析在机器学习领域又称为无监督（无指导）学习，这是因为数据中并没有现成的类别可供参考。而回归分析、判别分析方法则称为有监督学习。

常用的聚类方法包括：

* 系统（谱系）聚类法（hierarchical clustering），该方法从每类只有单个元素开始，每次合并两类。

* $k$ 均值聚类法（k-means clustering），该方法要求首先设定类别的个数，然后迭代地调整元素的类别使同类的元素接近而异类元素分离。

### 聚类分析的例子

* $Q$ 型聚类  

  - 在商务上，市场分析人员希望将客户划分为不同的客户群，并且用购买模式来刻画不同客户群的特征。

  - 在生物学上，对动植物分类和对基因分类，以获得对种群中固有结构的认识。

  - 在经济学中，根据人均国民收入、人均工农业产值和人均消费水平等多项指标对世界上所有国家的经济发展状况进行分类。
  
* $R$ 型聚类： 
  
  - 对人体体型指标进行测量，可对所有这些测量的指标进行聚类，一般可分成“纵向”指标（如身高、手臂长、上肢长和下肢长等）和“横向”指标（如体重、颈围、胸围和胸宽等）两类。  

### 聚类分析和判别分类的区别

* 聚类分析和判别分类是两种不同的分类方法：

  - 在判别分类中，组（或类）的数目是已知的，组是事先已定义好了的，我们将样品分配给其中一个组。

  - 在聚类分析中，无论是类的数目还是类本身在事先都是未知的。
  
  - 例如，我们判断某公司的职员来自哪个部门的，这属于判别分类的问题；而我们根据该公司各职员的一些能力和特点进行分类，则属于聚类分析的问题。
  
## 距离和相似系数  

### 变量的测量尺度

* 变量的测量尺度：间隔、有序和名义尺度。

* 间隔变量：变量用连续的量来表示。

  - 如长度、重量、速度、温度等。

* 有序变量：变量度量时不用明确的数量表示，而是用等级来表示。
  
  - 如某产品分为一等品、二等品、三等品等有次序关系。
  
  - 如顾客售后评价从非常满意到非常不满意之间进行选择。
  
* 名义变量：变量用一些类表示，这些类之间既无等级关系也无数量关系。

  - 如性别、职业、产品的型号等。
  
* 间隔变量也称为定量变量，有序变量和名义变量统称为定性变量或属性变量或分类变量。

## 相似性度量

相似性度量使用距离和相似系数这两类指标。

* 对距离指标来说，距离越小表示相似性越强。

* 对相似系数指标来说，相似系数越大表示相似性越强。
  
* 距离指标常用于度量样例之间的相似性，相似系数常用于度量变量之间的相似性。

### 距离的定义

距离：设 $x=(x_1,x_2,\cdots,x_p)^\prime$ 和 $y =(y_1,y_2,\cdots,y_p)^\prime$ 为两个观测样例，则所定义的距离 $d(x,y)$ 一般应满足如下三个条件：

* 非负性，也称正定性：$d(x, y)\geq 0$，$d(x, y)=0$ 当且仅当 $x=y$

* 对称性：$d(x, y) = d(y, x)$

* 三角不等式：$d(x, y)\leq d(x,z) + d(z, y)$
  
**欧氏（Euclidean）距离**
  
$$d(x, y)=\sqrt{(x-y)^\prime(x-y)}$$

* 欧氏距离的范数为 $2$。

* 欧氏距离是聚类分析中最常用的一个距离。
  
* 当各变量的单位不同或虽单位相同但各变量的变异性相差较大时，可考虑先对各变量的数据作标准化处理。
  
**曼哈顿（Manhattan）距离，也称绝对值距离**

$$d(x,y)=\sum_{i=1}^{p}|x_i-y_i|$$

* 曼哈顿距离的范数为 $1$。
  
* 绝对值距离常被形象地称作“城市街区”距离。

* 当我们对某城市（需考虑彼此之间路程）的位置点进行聚类时，一般使用绝对值距离。

```{r}

# 绝对值距离
  img <- readPNG(source = "Pictures/绝对值距离.png")
  grid.raster(image = img)
  
```

**闵可夫斯基（Minkowski）距离，也称闵氏距离**  
  
$$d(x, y)=[\sum_{i=1}^p{\mid{x_i-y_i\mid}}^q] ^{1/q},\quad q\geq 1$$
  
* 闵可夫斯基距离的范数为 $\alpha$。

* 当 $q=1$ 时，明氏距离就是绝对值距离。

* 当 $q=2$ 时，明氏距离就是欧式距离。

* 当 $q=\infty$ 时，明氏距离称为切比雪夫距离。

**切比雪夫距离，也称最大距离或上确界距离**

* $x$ 与 $y$ 的两个分量之间的最长距离。

* 切比雪夫距离的范数为无穷。
 
**兰氏（Lance）距离，也称堪培拉（Canberra）距离**  

* 当所有的数据皆为正时，可以定义 $x$ 与 $y$ 之间的兰氏距离为：

$$d(x,y)=\sum_{i=1}^{p}\frac{|x_i-y_i|}{x_i+y_i}$$  
 
* 该距离与各变量的单位无关，且适用于高度偏斜或含异常值的数据。

**马氏（Mahalanobis）距离**
  
$$d(x, y)=\sqrt{(x-y)^\prime \boldsymbol{S}^{-1}(x-y)},\quad \boldsymbol{S}\;\text{是样本协方差矩阵}$$
  
* 马氏能够消除量纲与变量之间相关性的影响（相关性又与多元观测取值的聚集方向有关）。 

* **注意**：由于聚类过程中的类一直变化着，样本协差阵 $S$ 一般难以确定，除非有关于不同类的先验知识。因此，在实际聚类分析中，马氏距离一般不是理想的距离。

**二值（binary）名义变量距离，也称杰卡德（Jaccard）距离** 

* 杰卡德距离（Jaccard Distance）用于衡量两个集合的差异性。

某高校举办一个培训班，从学员的资料中得到这样六个变量：$x_1$ 表示性别（男，女），$x_2$ 表示外语语种（英语，非英语），$x_3$ 表示专业（统计，非统计），$x_4$ 表示职业（教师，非教师），$x_5$ 表示居住处（校内，校外），$x_6$ 表示学位（硕士，学士）。现有两名学员：

$x$ =(男，英语，统计，非教师，校外，学士)′

$y$ =(女，英语，非统计，教师，校外，硕士)′

我们用 $m_1$ 表示匹配的变量数，$m_2$ 表示不匹配的变量数，则 $x$ 与 $y$ 之间的距离可定义为：

$$d(x,y)=\frac{m_2}{m_1+m_2}$$ 

按上述定义，本例中 $x$ 与 $y$ 之间的距离为 $2/3$。
 
在上述的各种距离中，观测样例的各个分量都是数值型的。实践中，数据中的分量有些是数值型的，有些是分类数据。对此，就需要使用高尔（Gower）距离。

令 Gower 距离的观测数据矩阵为 $M=(x_{ij})_{n\times p}$，矩阵 $M$ 中每行表示一个观测样例，每列表示一个变量或特征（feature），则第 $s$行 与第 $t$ 行的 Gower 距离定义为：  

$$d(s, t) = \frac{1}{p} \sum_{j=1}^p d_j(s, t, M)$$ 

在 R 中：

* 计算距离使用程序包 stats 中的函数 dist。

* 计算马氏距离使用 stats 中的函数 mahalanobis，该函数返回一个由平方马氏距离构成的向量。  

* 计算 Gower 距离使用程序包 cluster 的函数 daisy，该函数返回一个由各个观测样例彼此之间的不相似性系数构成的矩阵。

```{r}

# 计算距离
  set.seed(seed = 2631)
  x <- matrix(data = rnorm(n = 100, mean = 0, sd = 1), nrow = 5)

# 欧式距离
  dist(x = x, method = "euclidean", diag = FALSE, upper = FALSE)  
  
# 曼哈顿（Manhattan）距离，也称绝对值距离
  dist(x = x, method = "manhattan", diag = FALSE, upper = FALSE)

# 闵可夫斯基（Minkowski）距离，也称闵氏距离
  dist(x = x, method = "minkowski", diag = FALSE, upper = FALSE, p = 1) # 绝对值（曼哈顿）距离
  dist(x = x, method = "minkowski", diag = FALSE, upper = FALSE, p = 2) # 欧氏距离

# 切比雪夫距离，也称最大距离或上确界距离
  dist(x = x, method = "maximum", diag = FALSE, upper = FALSE)

# 兰氏（Lance）距离，也称堪培拉（Canberra）距离
  dist(x = x, method = "canberra", diag = FALSE, upper = FALSE)

# 二值名义距离，也称杰卡德（Jaccard）距离：编码时，匹配的元素必须编为 1
  d <- rbind(x = c(1, 1, 1, 0, 1, 0), y = c(0, 1, 0, 1, 1, 1))
  dist(x = d, method = "binary", diag = FALSE, upper = FALSE)

# 马氏距离：返回一个由平方马氏距离构成的向量，向量长度等于观测对象的数量或矩阵的行数
  set.seed(seed = 2631)
  x <- matrix(data = rnorm(n = 100), ncol = 4)
  mahalanobis(x = x, center = colMeans(x = x, na.rm = TRUE), 
              cov = cov(x = x, method = "pearson"))

```

```{r}

# 读取数据
  data(agriculture, package = "cluster")
  class(x = agriculture)
  anyNA(x = agriculture)
  dim(x = agriculture)
  
# 计算 Gower 距离
  library(cluster)
  d.gower <- daisy(x = agriculture, metric = "gower", stand = FALSE)
  as.matrix(x = d.gower)

```
  
### 相似性
  
* 对于间隔变量，变量之间最常用的相似系数是相关系数。

  - 相似系数（或其绝对值）越大，变量之间的相似性程度就越高，反之则相似性程度越低。

  - 变量之间的相似性度量，在一些应用中要看相似系数的大小，而在另一些应用中要看相似系数绝对值的大小。

* 聚类时，将相似程度高的变量归为一类。

* 对于连续型变量，变量之间最常用的相似系数是相关系数。  

```{r}

# 读取数据
  attach(what = iris)

# 计算相关系数矩阵
  
  # 方法 1
  cor(x = iris[, 1:4], method = "pearson", use = "pairwise.complete.obs")

  # 方法 2
  Hmisc::rcorr(x = as.matrix(x = iris[, 1:4]), type = "pearson")
  
  detach(name = iris)

```

为了度量两个变量中所有观测值之间的距离，更多地使用相似性的概念。设数据含有 $n$ 个观测，每个观测有 $p$ 个变量， 第 $i$ 个观测为 $(x_{i1}, x_{i2}, \dots, x_{ip})$，则第 $k$ 个变量的 $n$ 个观测值与第 $j$ 个变量的 $n$ 个观测值的相近程度可以用样本相关系数度量:

$$\begin{aligned}
  r_{kj} = \frac{\sum_{i=1}^n (x_{ik} - \bar x_{\cdot k}) (x_{ij} - \bar x_{\cdot j}) }
  {\sqrt{ \sum_{i=1}^n (x_{ik} - \bar x_{\cdot k})^2 \;  \sum_{i=1}^n (x_{ij} - \bar x_{\cdot j})^2}},
  \ k, j = 1,2,\dots,p .
\end{aligned}$$

$r_{kj}$ 越大相似度越高。在仅关心两个变量关系的强弱而不关心关系是正向还是反向的时候，也可以用 $|r_{kj}|$ 度量相似度。

将上面的相关系数转化为距离：$$d_{kj} = \sqrt{2[1 - r_{kj}]}$$

如果将第 $k$ 个变量的 $n$ 个观测值与第 $j$ 个变量的 $n$ 个观测值视为 $\mathbb R^n$ 中的两个向量，用这两个向量的夹角余弦度量其相近程度：

$$\begin{aligned}
  \cos\theta_{kj} = \frac{\sum_{i=1}^n x_{ik} x_{ij}}
  {\sqrt{ \sum_{i=1}^n x_{ik}^2 \; \sum_{i=1}^n x_{ij}^2 }}
\end{aligned}$$

此余弦取值位于 $[-1, 1]$ 之间，余弦取值越大表明第 $k$ 和第 $j$ 个变量之间的相似性越高。

对其它更复杂的对象往往也需要计算相异度或者距离。

* 程序包 igraph 中的函数 shortest.paths 能够计算图中各节点之间的距离。

* 程序包 xkcd 中的函数 cophenetic 能够计算一棵树的叶结点之间的距离。

* 程序包 distory 中的函数 dist.multiPhylo 能够计算两棵树之间的距离。

两个图之间的 Jaccard 距离也可以有定义。假设两个图具有相同的节点，则只要比较共同的边。据此定义 Jaccard 距离。

* 程序包 igraph 中的函数 similarity 能够计算两个图之间的 Jaccard 距离。

距离和相异度还可以用于比较图像、音频、地图、文档等复杂对象。谨慎设计的距离定义可以包含学科知识，并可以将包含许多异质数据的困难问题简化。 

* 例如，对新闻文本，经常先切分词语再计算词语频数作为数据。对数据的相似度或相异度的理解可以引发对数据的更好的呈现。 

## 系统聚类法

### 系统聚类法的概念

* 系统聚类法（也称层次聚类法）是通过一系列相继的合并或相继的分割来进行的，分为聚集系统法与分割系统法两种。

  - 系统聚类法适用于样例数目 $n$ 不是很大的情形。  

$Q$ 型系统聚类法的基本步骤：

* 第一步：将每个样例视为单独的 $n$ 个类。

* 第二步：将最接近的两个样例合并成一类，形成总计 $n-1$ 个类。

* 第三步：重复第二步，从 $n-1$ 个类中找到最相近的两个类将其合并，形成总计 $n-2$ 个类。

* 最后：上述步骤，直至所有的样例都合并为一个大类为止。 

$R$ 型系统聚类法的步骤类似，只不过是针对变量进行合并。

系统聚类算法需要考虑两个问题：如何衡量两个类的相近程度，以及如何选择最终分成多少个类。

分割系统法的聚类步骤与聚集系统法正相反。

聚集系统法最为常用，实践中最常使用其中的五种方法，包括：最短距离法、最长距离法、类平均法、重心法、离差平方和法（也称 Ward 方法）。所有这些聚类方法的区别仅仅在于类与类之间距离的定义不同。

### 最短距离法（single linkage）
 
定义类与类之间的距离为两类最近样例间的距离，使用最短距离法的递推公式进行计算。

* 最短距离法的聚类结果可使用树形图进行展示。

* 最短距离法有一种挑选长链状聚类的倾向，称为链接倾向，这是最短距离法的一个不足。

* 最短距离法不适合用于对分离得很差的群体进行聚类。

### 最长距离法（complete linkage）

类与类之间的距离定义为两类最远样例间的距离。

* 最长距离法与最短距离法的并类步骤完全相同，只是递推公式不同。

* 最长距离法容易被异常值严重地扭曲。

### 类平均法  

有两种定义
  
* 定义 1：两类中各个观测样例距离的平均值。
  
* 定义 2：两类中各个观测样例平方距离的平均值。

* 类平均法较好地利用了所有样例之间的信息，在很多情况下它被认为是一种比较好的系统聚类法。

### 重心法（centroid linkage）

设类 $G_K$ 和 $G_L$ 之间的重心（均值）分别为 $\bar{x_K}$ 和 $\bar{x_L}$，则 $G_K$ 与 $G_L$ 之间的平方距离定义为两类重心之间的距离。

$$D_{K L}^{2}=d_{\bar{x}_{K} \bar{x}_{L}}^{2}=\left(\overline{\boldsymbol{x}}_{K}-\overline{\boldsymbol{x}}_{L}\right)^{\prime}\left(\overline{\boldsymbol{x}}_{K}-\overline{\boldsymbol{x}}_{L}\right)$$

* 与其他系统聚类法相比，重心法在处理异常值方面更稳健，但是在其他的方面一般不如类平均法或离差平方和法的效果好。

### 离差平方和法（Ward 方法）

类内离差平方和：$G_K$ 与 $G_L$ 之间的平方距离定义为

$$\begin{array}{c}
D_{K L}^{2}=\frac{n_{K} n_{L}}{n_{M}}\left(\overline{\boldsymbol{x}}_{K}-\overline{\boldsymbol{x}}_{L}\right)^{\prime}\left(\overline{\boldsymbol{x}}_{K}-\overline{\boldsymbol{x}}_{L}\right) \\
\frac{n_{K} n_{L}}{n_{M}}=\frac{n_{K} n_{L}}{n_{K}+n_{L}}=\frac{1}{1 / n_{L}+1 / n_{K}}
\end{array}$$
 
* 对固定的类内样例数，它们反映了各自类内样例的分散程度。

* 离差平方和法使得两个大的类倾向于有较大的距离，因而不易合并。相反，两个小的类却因倾向于有较小的距离而易于合并。这往往符合我们对聚类的实际要求。

* 离差平方和法在许多场合下被认为是一种比较好的系统聚类法，但该方法对异常值很敏感。

对上述五种方法来说，样本观测数量越小，五种方法的分类结果越近似甚至完全相同。

## 一般的理想聚类结果

单从统计角度来看，理想的聚类结果一般应是：

* 类的个数适当。

* 类之间较为分开而类内相近。

* 未出现不合理的过大的类。

在满足了上述要求之后，还需要检验分类结果的经济意义。从经济意义角度来看，理想的聚类结果应是：

* 类之间的特征明显不同。

* 类内的特征彼此接近。  

### 几点说明

* 如果只使用一个变量进行聚类，一般应采用按大小的排序后再按某种规则或主观确定若干个分界点的方法，不宜采用通常的正规聚类方法。
  
* 如果聚类的目标是从全面的角度划分类别，则应使用正规聚类方法，因为正规聚类方法的聚类过程既能够反映出数据中的整体性差异又能够反映出各个样例之间的个体差异。

* 对聚类的结果必须能够给出合理的现实性解释。

* 对于各种聚类结果进行比较可使用平行图（也称轮廓图）进行直观展示。

## 系统聚类示例

由于聚类分析具有很强的探索性质，因此在进行聚类分析中应该首先基于研究目的和专业领域的知识进行理论上的考虑，然后再尝试多种聚类方法，从中得出最合理的聚类结果。

系统聚类的 R 语言步骤：

* 计算距离阵: dist

* 进行系统聚类: hclust

* 绘制聚类图: plot

* 添加聚类框: rect.hclust

* 确认聚类结果: cutree

系统聚类法使用程序包 stats 中的函数 hclust，其中参数 method 的取值为：

* "single"：最短距离法

* "complete"：最长距离法，默认设置

* "average"：类平均法

* "centroid"：重心法

* "median"：中间距离法

* "ward.D"：离差平方和法

函数 dist 用于计算距离，其中参数 method 的取值为：

* "euclidean"：欧氏距离，默认取值

* "manhattan"：曼哈顿距离，即绝对值距离

* "minkowski"：明氏距离，伴随的参数 $p$ 为明氏距离的幂，$p=2$ 即为欧氏距离

* "canberra"：堪培拉距离，即兰氏距离  

数据：1999 年全国 31 个省、直辖市和自治区的城镇居民家庭平均每人全年消费性支出的八个主要变量数据。

### 使用最小距离法聚类

```{r}

# 读取数据
  mydata <- rio::import(file = "Data/examp6.3.3.xlsx")
  pander::pander(x = head(x = mydata[, 1:5], n = 3))

# 数据的标准化
  mydata <- datawizard::standardize(x = mydata)
  pander::pander(x = head(x = mydata[, 1:5], n = 3))

# 计算欧式距离矩阵
  dist_euc <- dist(x = mydata[, -1], method = "euclidean", diag = FALSE, upper = FALSE)

# 系统聚类：最小距离法，以列表形式返回结果
  clust_min <- hclust(d = dist_euc, method = "single")

# 绘制系统聚类图（Cluster Dendrogram）
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(x = clust_min, labels = mydata$地区, check = TRUE, 
       xlab = "Euclidean Distance", ylab = "Height", sub = "",
       main = "Cluster Dendrogram with cluster = 5")
  rect.hclust(tree = clust_min, k = 5, which = 1:5, border = rainbow(n = 5), 
              cluster = cutree(tree = clust_min, k = 5, h = 5))
  par(opar)

# 查看聚类结果
  table(cutree(tree = clust_min, k = 5))
  
```

```{r}

# 绘制系统聚类图（Cluster Dendrogram）：普通树形图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mfrow = c(2, 4), las = 1)
  
  for (i in 2:9) {
    plot(x = clust_min, labels = mydata$地区, hang = 0.1, check = TRUE, 
         xlab = "Euclidean Distance", ylab = "Height", sub = "", 
         main = paste("Cluster Dendrogram with cluster = ", i, sep = ""))
    rect.hclust(tree = clust_min, k = i, which = 1:i, border = rainbow(n = i),
                cluster = cutree(tree = clust_min, k = i, h = i))
  }

  par(opar)

# 查看聚类结果
  for (i in 2:9) {
    print(x = table(cutree(tree = clust_min, k = i)))
  }
  
``` 

```{r}
  
# 绘制系统聚类图（Cluster Dendrogram）
  library(ape)

  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mfrow = c(3, 2), las = 1)
  
  plot(x = clust_min, check = TRUE, xlab = "Euclidean Distance", ylab = "Height", 
       sub = "", main = "Plain dendrogram")
  
  plot(x = as.dendrogram(object = clust_min), type = "triangle", 
       main = "Triangle branches")
  
  # 以下两条命令绘制相同类型的系统图
  plot(x = as.phylo(x = clust_min), type = "unrooted", main = "Unrooted tree")
  plot(as.phylo(x = clust_min), type = "u", cex = 0.8, main = "Unrooted tree")
  
  # 以下两条命令绘制相同类型的系统图
  plot(x = as.phylo(x = clust_min), type = "fan", main = "Leaves spread in a circle")
  plot(as.phylo(x = clust_min), type = "r", cex = 0.8, main = "Leaves spread in a circle")
  
  par(opar)
  
```

### 使用最大距离法聚类

```{r}
  
# 读取数据
  mydata <- rio::import(file = "Data/examp6.3.3.xlsx")
  mydata <- datawizard::standardize(x = mydata)

# 计算欧式距离矩阵
  dist_euc <- dist(x = mydata[-1], method = "euclidean")
  
# 系统聚类：最大距离法，以列表形式返回结果
  cluster_max <- hclust(d = dist_euc, method = "complete")

# 绘制系统聚类图（Cluster Dendrogram）
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)

  plot(cluster_max, labels = mydata$地区, check = TRUE, 
       xlab = "Euclidean Distance", ylab = "Height", sub = "", 
       main = "Cluster Dendrogram with cluster = 5")
  rect.hclust(tree = cluster_max, k = 5, which = 1:5, border = rainbow(n = 5))
  
  par(opar)
  
# 查看聚类结果
  table(cutree(tree = cluster_max, k = 5, h = 5))  

```

### 使用重心法聚类

```{r}
  
# 读取数据
  mydata <- rio::import(file = "Data/examp6.3.3.xlsx")
  mydata <- datawizard::standardize(x = mydata)  
  
# 计算欧式距离矩阵
  dist_euc <- dist(x = mydata, method = "euclidean")  
  
# 系统聚类：重心法，以列表形式返回结果
  cluster_cen <- hclust(d = dist_euc, method = "centroid")

# 绘制系统聚类图（Cluster Dendrogram）
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)

  plot(cluster_cen, labels = mydata$地区, check = TRUE, 
       xlab = "Euclidean Distance", ylab = "Height", sub = "", 
       main = "Cluster Dendrogram with cluster = 5")
  rect.hclust(tree = cluster_cen, k = 5, which = 1:5, border = rainbow(n = 5))

  par(opar)
  
# 查看聚类结果
  table(cutree(tree = cluster_cen, k = 5))

```

### 使用离差平方和法聚类

```{r}

# 读取数据
  mydata <- rio::import(file = "Data/examp6.3.3.xlsx")
  mydata <- datawizard::standardize(x = mydata)  

# 计算欧式距离矩阵
  dist_euc <- dist(x = mydata, method = "euclidean")  
  
# 系统聚类：离差平方和法，首选方法，以列表形式返回结果
  cluster_ward <- hclust(d = dist_euc, method = "ward.D2")
  
# 绘制系统聚类图（Cluster Dendrogram）
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(cluster_ward, labels = mydata$地区, check = TRUE, 
       xlab = "Euclidean Distance", ylab = "Height", sub = "", 
       main = "Cluster Dendrogram with cluster = 5")
  rect.hclust(tree = cluster_ward, k = 5, which = 1:5, border = rainbow(n = 5))
    
  par(opar)

# 查看聚类结果
  table(cutree(tree = cluster_ward, k = 5))
  
```

## 聚类中的若干问题

### 系统聚类法的单调性

令 $D_i$ 是系统聚类法中第 $i$ 次并类时的距离，如果一种系统聚类法能满足 $D_1 \leq D_2 \leq \cdots$，则称该系统聚类方法具有单调性。

* 这种单调性符合系统聚类法的思想，先合并较相似的类，后合并较疏远的类。

* 最短距离法、最长距离法、类平均法、离差平方和法都具有单调性，但重心法不具有单调性。

### 空间的浓缩与扩张

设有两种系统聚类法，它们在第 $i$ 步的距离矩阵分别为 $A_i$ 和 $B_i$，如果 $A_i>B_i$ 则称第一种方法比第二种方法使空间扩张，或第二种方法比第一种方法使空间浓缩。

* 太浓缩的方法不够灵敏，太扩张的方法可能因灵敏度过高而容易失真。

* 类平均法比较适中，它既不太浓缩也不太扩张，因此它在这方面是比较理想的。

* 最短距离法是一种非常浓缩的方法，容易出现链接倾向。

### 使用图形作主观的聚类

* 当 $p=2$ 时，可以直接在散点图上进行主观的聚类，其效果未必逊于、甚至好于正规的聚类方法，特别是在寻找“自然的”类和符合我们实际需要的类方面。

* 当 $p=3$ 时，我们可使用统计软件产生三维旋转图，通过三维旋转从各个角度来观测散点图，作直观的聚类。但由于其视觉效果及易操作性远不如平面散点图，故实践中很少采用，除非样品数很少。

* 当 $p\geq 3$时，还可采用主成分分析（这里允许不对主成分给出解释）或因子分析（一般只在对因子的解释感兴趣时使用，实践中很少采用）的技术将维数降至 $2$ 维（偶尔 $3$ 维），然后再生成散点图（或旋转图），从直觉上进行主观的聚类。

### 使用图形对聚类效果的评估

经聚类分析已将类分好之后，常常希望从统计的角度看一下聚类的效果：不同类之间是否分离得较好，同一类内的样品（或变量）是否彼此相似。通常可通过构造图形作直观的观测，所使用的图形有如下两种：

* 将 $p$ 维数据画于平面图上，方法有平行（坐标）图、星形图、切尔诺夫脸谱图和安德鲁曲线图等，这些图都不太适合样品数很大的场合。
  
* 使用 Fisher 判别的降维方法，将 $p$ 维数据降至 $2$ 维或 $3$ 维再构造散点图或三维旋转图。Fisher 降维方法不一定能够保证成功，但如果使用 Fisher 判别的降维方法能够成功，则往往更值得推荐，尤其在样品数很大的场合下。
 
### 对变量的聚类

* 最短距离法、最长距离法和类平均法都属于连接方法，它们既可以用于样品的聚类，也能够用于变量的聚类。

* 不是所有的系统聚类方法都适用于对变量的聚类。 

数据：对 305 名女中学生测量八个体型指标，这八个体型指标分别为：

* 身高（$x_1$），手臂长（$x_2$），上肢长（$x_3$），下肢长（$x_4$）、体重（$x_5$）、颈围（$x_6$）、胸围（$x_7$）、胸宽（$x_8$）

* 数据为相关系数矩阵。

```{r}

# 读取数据
  bodyMeasures <- rio::import(file = "Data/examp6.3.7.xlsx")
  class(x = bodyMeasures)
  anyNA(x = bodyMeasures)
  
# 提取不相似度矩阵并转换为距离矩阵
  dissimilarities <- 1 - as.matrix(x = bodyMeasures[, -1])
  dissimilarities <- as.dist(m = dissimilarities)
  
# 对变量的系统聚类：最小距离法
  cluster_min <- hclust(d = dissimilarities, method = "single")
  
# 绘制系统聚类图（Cluster Dendrogram）
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(cluster_min, labels = bodyMeasures$体型指标, check = TRUE,
       xlab = "Euclidean Distance", ylab = "Height", sub = "", 
       main = "Cluster Dendrogram with cluster = 2")
  rect.hclust(tree = cluster_min, k = 2, which = 1:2, border = c("red", "blue"))

  par(opar)

# 查看聚类结果
  table(cutree(tree = cluster_min, k = 2))
  
```

```{r}
  
# 对变量的系统聚类：最大距离法
  cluster_max <- hclust(d = dissimilarities, method = "complete")
  
# 绘制系统聚类图（Cluster Dendrogram）
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(cluster_max, labels = bodyMeasures$体型指标, check = TRUE,
       xlab = "Euclidean Distance", ylab = "Height", sub = "", 
       main = "Cluster Dendrogram with cluster = 2")
  rect.hclust(tree = cluster_max, k = 2, which = 1:2, border = c("red", "blue"))

  par(opar)

# 查看聚类结果
  table(cutree(tree = cluster_max, k = 2))

```

```{r}
  
# 对变量的系统聚类：类平均法
  cluster_avg <- hclust(d = dissimilarities, method = "average")
  
# 绘制系统聚类图（Cluster Dendrogram）
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(cluster_avg, labels = bodyMeasures$体型指标, check = TRUE,
       xlab = "Euclidean Distance", ylab = "Height", sub = "", 
       main = "Cluster Dendrogram with cluster = 2")
  rect.hclust(tree = cluster_avg, k = 2, which = 1:2, border = c("red", "blue"))

  par(opar)

# 查看聚类结果
  table(cutree(tree = cluster_avg, k = 2))

```

### 类的个数

如果能够分成若干个很分开的类，则类的个数就比较容易确定。反之，如果无论怎样分都很难分成明显分开的若干类，则类个数的确定可能就比较困难了。

确定类个数的常用方法：

* 给定一个阈值 $T$：通过观测树形图，给出一个你认为合适的阈值 $T$，要求类与类之间的距离要大于 $T$，有些样品可能会因此而归不了类或只能自成一类。这种方法有较强的主观性，这是它的不足之处。

* 观测样品的散点图

  - 如果样品只有两个（或三个）变量，则可通过观测数据的散点图（或旋转图）来主观确定类的个数。
  
  - 如果变量个数超过三个，则可对每一可能考虑的聚类结果，将所有样品的前两个（或三个）费希尔判别函数得分制作成散点图（或旋转图），目测类之间是否分离得较好。该图既能帮助我们评估聚类效果的好坏，也能帮助我们判断所定的类数目是否恰当。

* 使用统计量

  - $R^2$ 统计量
  
  - 半偏 $R^2$ 统计量
  
  - 伪 $R^2$ 统计量

总之，在确定类别的数量时绝不可仅仅依靠统计分析的结果，还必须结合相关领域的专业知识。

## 动态聚类法：$k$ 均值法

动态聚类法也称逐步聚类法，动态聚类法能够处理大样本数据集。

在系统聚类法中，对于那些先前已被“错误”分类的样品不再提供重新分类的机会，而动态聚类法（或称逐步聚类法）却允许样品从一个类移动到另一个类中。
  
动态聚类法的计算量要比建立在距离矩阵基础上的系统聚类法小得多。因此，使用动态聚类法计算机所能承受的样品数目 $n$ 要远远超过使用系统聚类法所能承受的 $n$。

动态聚类法的基本思想是，选择一批凝聚点或给出一个初始的分类，让样品按某种原则向凝聚点凝聚，对凝聚点进行不断的修改或迭代，直至分类比较合理或迭代稳定为止。类的个数 $k$ 需要事先指定。
 
* 选择初始凝聚点或给出初始分类的一种简单方法是采用随机抽选或随机分割样品的方法，可以要求凝聚点之间至少应间隔某个距离值。

* 动态聚类法只能用于对样品的聚类，而不能用于对变量的聚类。

动态聚类法有许多种方法，比较流行的动态聚类法是 $k$ 均值法，它是由麦奎因（MacQueen，1967）提出并命名的一种算法。

### $k$ 均值法的基本步骤

* 第一步：选择 $k$ 个样品作为初始凝聚点，或者将所有样品分成 $k$ 个初始类，然后将这 $k$ 个类的重心（均值）作为初始凝聚点。

* 第二步：对所有样品逐个归类，将每个样品归入凝聚点离它最近的那个类（通常采用欧氏距离），该类的凝聚点更新为这一类目前的均值，直至所有样品都归类为止。

* 重复第二步，直至所有的样品都不能再分配为止。

$k$ 均值法的最终聚类结果在一定程度上依赖于初始凝聚点或初始分类的选择。经验表明，聚类过程中的绝大多数重要变化均发生在第一次再分配中。

### $k$ 均值法的几点说明

* 由于 $k$ 均值法对凝聚点的初始选择有一定敏感性，故再试一下其他初始的凝聚点也许是个不错的想法。如果不同初始凝聚点的选择产生明显不同的最终聚类结果，或者迭代的收敛是极缓慢的，那么可能表明没有自然的类可以形成。

* $k$ 均值法有时也可用来改进系统聚类的结果。

  - 例如，先用类平均法聚类，然后将其各类的重心作为 $k$ 均值法的初始凝聚点重新聚类，这可使得系统聚类时错分的样品能有机会获得重新的分类。
  
  - $k$ 均值法能否有效地改善系统聚类不能一概而论，还应针对聚类的最终结果进行主观判断。  
 
在 R 中，$k$ 均值法的计算使用程序包 stats 中的函数 kmeans，其中：

* 参数 center：可接受规定的类别数 $k$，也可接受一个向量作为初始凝聚点。

* 参数 algorithm 可取 "Hartigan-Wong"、"Lloyd"、"Forgy"、"MacQueen"。

### $k$ 均值法的示例

```{r}

# 读取数据
  mydata <- rio::import(file = "Data/examp6.3.3.xlsx")    
  mydata_std <- datawizard::standardize(x = mydata)
 
# k 均值聚类：以列表的形式返回结果
# 聚成 5 类，由算法随机选择 5 行作为初始凝聚点 
  kms_mac <- kmeans(x = mydata_std[, -1], centers = 5, algorithm = "MacQueen")
  length(x = kms_mac)
  names(x = kms_mac)

# 查看聚类的结果
  table(kms_mac$cluster)

# 绘制 k 均值聚类图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)

  plot(x = 1:nrow(x = mydata_std), y = kms_mac$cluster, panel.first = grid(), type = "p", 
       col = kms_mac$cluster, pch = kms_mac$cluster, xlab = "样例编号", ylab = "类别",
       main = "k 均值聚类图")
  maptools::pointLabel(x = 1:nrow(x = mydata_std), y = kms_mac$cluster, 
                       labels = mydata_std$地区, cex = 0.8, col = kms_mac$cluster, 
                       method = "SANN", allowSmallOverlap = FALSE)
  par(opar)

```

```{r}

# 用散点图矩阵表现分类效果
  plot(x = mydata_std[, -1], col = kms_mac$cluster, pch = 20)

```

$k$ 均值聚类方法适用于各个类别中的样例个数相近且数据的空间形状接近凸集的情形。如果真实类别中的样例个数有很大差异，$k$ 均值聚类方法可能会将大的类别进行拆分。如果真实类别的空间形状不是球形或者椭球形，$k$ 均值聚类方法的效果可能也不好。

# 主成分分析

## 主成分分析概述

### 主成分分析的概念

主成分分析（Principal Component Analysis，PCA）由皮尔逊（Pearson，1901）首先引入，霍特林（Hotelling，1933）则进一步发展了主成分分析。

主成分分析是一种通过降维技术把多个变量转化为少数几个主成分（综合变量）的统计分析方法。这些主成分能够反映原始变量的绝大部分信息，主成分通常表示为原始变量的某种线性组合且各个主成分之间彼此不相关。

如何度量变量中包含的信息？ 如果变量取常数，就没有信息。变量变化范围越大，越不容易预知其取值，得到变量的观测值时所获得的信息量就大。

* 在主成分分析中，用方差衡量综合指标（主成分）所包含的原始信息的多少。 

* 在主成分分析中，通过寻找原始变量的线性组合作为综合指标，使得综合指标的方差最大且各个综合指标之间不相关。

### 主成分分析的应用  

* 在一些应用中，用少数的前几个主成分替代众多的原始变量，此时找到这些主成分就成了分析的目标。

  - 这种应用需要对这些前几个主成分给出一个符合实际背景和意义的解释。
  
* 在更多的另一些应用中，主成分只是要达到目的的一个中间结果或称中间步骤，而非最终目的本身。

  - 例如，主成分聚类、主成分回归、评估正态性、寻找异常值以及寻找原始变量间的多重共线性关系等。
  
  - 此时的主成分可不必给出解释。

## 总体的主成分

### 主成分的定义及解

设 $x=(x_1,x_2,\cdots,x_p)^\prime$ 是一个 $p$ 维向量，$E(x)=\mu$ 且 $V(x)=\Sigma$，则原始向量 $x$ 的第 $i$ 个主成分 $y_i=a_i^\prime{x}$ 是指在满足 $\lVert{a_i}\rVert=1$ 和 $Cov(y_k,y_i)=0$ 的条件下寻找 $a_i$，使得 $V(y_i)={a_i^\prime}{\Sigma}a_i$ 达到最大。可求得：$$y_i=t_{1i}x_1+t_{2i}x_2+\cdots+t_{pi}x_p$$

$y_i$ 称为主成分得分，其方差为 $\lambda_i$，$\lambda_i$ 是原始 $p$ 维向量 $x$ 协方差矩阵的第 $i$ 个特征根，$t_i=(t_{1i},t_{2i},\cdots,t_{pi})^\prime$ 是原始向量 $p$ 维向量 $x$ 协方差矩阵的第 $i$ 个特征向量。

### 主成分的几何意义
  
在几何上，$t_i$ 表明了第 $i$ 个主成分 $y_i$ 的方向，$y_i$ 是 $x$ 在 $t_i$ 上的投影值（其绝对值即为投影长度），$\lambda_i$ 是这些值的方差，它反映了 $t_i$ 上投影点的变异程度。

```{r}

# 主成分的几何意义
  img <- readPNG(source = "Pictures/主成分的几何意义.png")
  grid.raster(image = img)
  
```

该变换的几何意义是对原始的 $p$ 维向量做正交旋转，使得变换后得到的 $p$ 个主成分彼此无关。

### 主成分的性质

* 由于第 $i$ 个主成分 $y_i$ 的方差满足 $V(y_i)=\lambda_i$ 并且各个主成分之间彼此互不相关，因此主成分向量的方差满足 $V(y)=\Lambda$，$\Lambda$ 是一个由原始向量 $x$ 的协方差矩阵的特征根构成的对角矩阵。

* 各个主成分的方差之和等于原始 $p$ 维向量 $x$ 的协方差矩阵的方差，即

$$\sum_{i=1}^p{\lambda_i}=\sum_{i=1}^p{\sigma_{ii}}$$

或

$$\sum_{i=1}^{p} V\left(y_{i}\right)=\sum_{i=1}^{p} V\left(x_{i}\right)$$
  
  - 上式表明，经过变换后的各个主成分能够从整体上保留原始向量 $x$ 的全部信息。

* 总方差中属于第 $i$ 个主成分 $y_i$（或被 $y_i$ 所解释）的比例为：

$$\lambda_i\large{/}{\sum_{j=1}^p{\lambda_j}}$$

  - 这一比例称为主成分 $y_i$ 的贡献率。

  - 第一主成分 $y_1$ 的贡献率最大，表明它对原始 $p$ 维向量 $x$ 的解释能力最强，其后的主成分 $y_2,y_3,\cdots,y_p$ 的解释能力依次递减。

  - 主成分分析的目的就是为了减少变量的个数，因此一般是不会使用所有 $p$ 个主成分的，但是忽略一些具有较小方差的主成分不会给总方差带来大的影响。

* 前 $m$ 个主成分的贡献率之和：$${\sum_{i=1}^m{\lambda_i}}\large{/}{\sum_{j=1}^p{\lambda_j}}$$

称为主成分 $y_1,y_2,\cdots,y_m$ 的累计贡献率，它表示主成分 $y_1,y_2,\cdots,y_m$ 对原始 $p$ 维向量 $x$ 的总解释能力。

  - 通常取相对于 $p$ 较小的 $m$，使得累计贡献达到一个较高的百分比，如 $80\%\sim 90\%$。此时，就可使用 $(y_1,y_2,\cdots,y_m)$ 来代替 $(x_1,x_2,\cdots,x_p)$，从而达到降维的目的，而信息的损失却不多。

* 原始变量 $x_i$ 与主成分 $y_k$ 之间的相关系数：$$\rho(x_i, y_k)=\frac{\sqrt{\lambda_k}}{\sigma_{ii}}t_{ik}$$

  - 在实际应用中，通常我们只对 $x_i,\; i=1,2,\cdots,p$ 与 $y_k,\; k=1,2,\cdots,m$ 之间的相关系数感兴趣。

* 原始变量对主成分的影响：

$$y_k=t_{1k}x_1+t_{2k}x_2+\cdots+t_{pk}x_p$$

  - 称 $t_{ik}$ 为 $y_k$ 在 $x_i$ 上的载荷，它反映了 $x_i$ 对 $y_k$ 的重要程度。$t_{ik}$ 的绝对值越大，$x_i$ 对 $y_k$ 的重要程度越高。

  - $\rho(x_i, y_k)=\frac{\sqrt{\lambda_k}}{\sigma_{ii}}t_{ik}$ 与 $t_{ik}$ 具有相同的符号，且成正比。

  - 在解释主成分时，我们需要考察相关系数 $\rho(x_i, y_k)$，更为重要的是考察载荷 $t_{ik}$。

### 对原始变量线性组合含义的解释

* 从系数角度来说，取决于变量系数的符号和相对大小。

* 从相关系数角度来说，取决于线性组合与各变量相关系数的符号和相对大小。

```{r}

# 创建协方差矩阵
  mat <- matrix(data = c(16, 2, 30, 2, 1, 4, 30, 4, 100), nrow = 3, byrow = TRUE) 

# 计算协方差矩阵的特征根与特征向量
  eigen_vec <- eigen(x = mat)

# 提取特征根
  eigens <- eigen_vec$values
  eigens

# 特征特征向量
  eigenvectors <- eigen_vec$vectors
  eigenvectors

```
  
相应的主成分分别为：

* 第一主成分（对应的特征根为 $109.7934$）：$$y_1=0.305x_1+0.041x_2+0.951x_3$$

* 第二主成分（对应的特征根为 $6.46871$）：$$y_2=0.943x_1+0.120x_2-0.308x_3$$

* 第三主成分（对应的特征根为 $0.7378$）：$$y_3=-0.127x_1-0.992x_2+0.002x_3$$

* 方差大的主成分与方差大的原始变量有较密切的联系，而方差小的主成分与方差小的原始变量有较强的联系。

* 通常我们取前几个主成分，因此所取主成分会过于考虑方差大的原始变量，而对方差小的原始变量却考虑的不够。
 
* 原始变量的方差在取值上差异较大时，第一主成分的贡献率或前几个主成分的累计贡献率往往显得很大。

```{r}

# 计算主成分的累计贡献率
  cumsum(x = eigens)/sum(eigens)  
  
```
 
### 从相关阵出发求主成分

通常有两种情形不适合直接从协方差矩阵 $\Sigma$ 出发进行主成分分析。

* 一种是各变量的单位不全相同的情形。

* 另一种是各变量的单位虽相同，但各个变量的方差在数值上差异较大。

  - 在实际应用中，常表现为各变量数据间的数值大小相差较大。

为解决上述问题，需要对原始的 $p$ 维向量 $x$ 进行标准化变换。 

* 标准化后的 $p$ 维向量 $x^{*}$ 的协方差矩阵 $\Sigma$ 正是原始 $p$ 维向量 $x$ 的相关系数矩阵 $\boldsymbol {R}$。

  - 从相关阵 $\boldsymbol {R}$ 出发求主成分，主成分分析将均等地对待每一个原始变量。

从相关阵 $\boldsymbol {R}$ 出发的主成分有一个很好的性质，即相关阵特征根之和等于原始向量的个数 $p$，即

$$\sum_{i=1}^p{\lambda_i^{*}}=p$$

```{r}

# 对原始数据进行标准化
  A <- rbind(c(1, 2, 3, 4, 5), 
             c(2, 4, 7, 8, 9), 
             c(3, 7, 10, 15, 20), 
             c(4, 8, 15, 30, 20), 
             c(5, 9, 20, 20, 40))
  A_std <- datawizard::standardize(x = A)

# 验证：标准化后数据的协方差矩阵等价于原始数据的相关系数矩阵
  cov_Astd <- cov(A_std)
  cor_Astd <- cor(A_std, method = "pearson")
  all.equal(target = cov_Astd, current = cor_Astd)

```

```{r}

# 创建协方差矩阵
  mat <- matrix(data = c(16, 2, 30, 2, 1, 4, 30, 4, 100), nrow = 3, byrow = TRUE) 

# 从协方差矩阵计算相关系数矩阵
  cor_mat <- cor(x = mat, method = "pearson")

# 计算相关系数矩阵的特征根与特征向量
  eigen_vec <- eigen(x = cor_mat, only.values = FALSE)
  eigens <- eigen_vec$values
  near(x = sum(eigens), y = ncol(x = mat))

# 计算各个主成分的累计贡献率
  cumsum(x = eigens)/sum(eigens)
  
# 计算特征向量
  eigen_vec$vectors

```

相应的主成分分别为：

* 第一主成分，对应的特征根为 $2.9643$：$$y_1=0.5742x_1+0.58047x_2+0.5774x_3$$

* 第二主成分，对应的特征根为 $0.0357$：$$y_2=0.7911x_1-0.212x_2-0.5738x_3$$

* 第三主成分，对应的特征根为 $0$：$$y_3=0.2106x_1-0.7963x_2+0.5809x_3$$

一般来说，各原始变量方差之间的差异越大，从相关阵 $\boldsymbol {R}$ 出发与从协差阵 $\Sigma$ 出发得到的主成分结果就越不相同。标准化后的结论完全可能会发生很大的变化，因此标准化是非常重要的。

## 样本的主成分

* 用样本协差阵和样本相关阵代替总体协差阵和总体相关阵。

  - 所有观测值的平均主成分得分为零。

## 主成分应用中需注意的问题

* 在本身作为目标的主成分分析中，我们首先应保证所提取的前几个主成分的累计贡献率达到一个较高的水平，其次对这些被提取的主成分必须都能够给出符合实际背景且有实际意义的解释。

* 主成分的解释其含义一般多少带有点模糊性，不像原始变量的含义那么清楚确切，这是变量降维过程中必须付出的代价。因此，提取的主成分个数 $m$ 通常应明显小于原始变量个数 $p$（除非 $p$ 本身就很小），否则维数降低的“利”可能抵不过主成分含义不如原始变量清楚的“弊”。

* 如果原始变量之间具有较高的相关性，则前面少数几个主成分的累计贡献率通常就能达到一个较高水平。也就是说，此时的累计贡献率通常较易得到满足。

* 主成分分析的困难之处主要在于给出主成分的较好解释，所提取的主成分中如果有一个主成分解释不了，本身作为目的的整个主成分分析也就失败了。

总之，主成分要应用得成功，一是靠原始变量的合理选取，二是靠“运气”。

### 主成分的保留个数

应该保留多少个主成分要视具体情况而定，不能一概而论，最终一般还得依赖于主观判断。
  
单从保留信息量的角度出发，通常有以下几种选择主成分个数的方法：

* 保留的前几个主成分能使累计贡献率达到一个较高的比例（如 $80\%$），具体比例值需主观判断确定，这是最为推荐的方法。

* 当从 $S$ 或 $\Sigma$ 出发求主成分时，有一个经验规则就是只保留特征根大于其平均值 $\sum_{i=1}^p/p$ 或 $1$ 的主成分。这是一个粗略的经验规则，只宜作为选择主成分个数的初步参考。

* 另外一种能够帮助我们确定主成分个数的视觉工具，就是使用陡坡图。

* 最后，根据对主成分所对应的特征根进行显著性检验，该方法在实践中较少采用。

如果我们需要对主成分进行解释，则选用多少个主成分就还需考虑所选主成分是否都能作出成功的解释，有时可能会为此牺牲累计贡献率。

如果不需要对主成分作出解释（此时的主成分得分通常只是作为进入下一阶段分析的输入数据，即主成分仅是整个分析过程的中间结果），则主成分个数的选择一般更倾向于保持一个足够高的累计贡献率，除非需要画散点图。

取多少个主成分有时也要视作图或排序的需要而定。

* 当取三个和四个主成分都可行时，选取三个主成分有一大好处，就是可以利用三维旋转图对所有样品的三个主成分得分进行直观的比较分析。
  
* 当取两个和三个主成分都可行时，选取两个主成分的主要好处是，平面散点图可以比三维旋转图观测得更为清楚和方便，且可打印输出。
  
* 当取一个和两个主成分都可行时，取一个的优点是可以对各样品进行排序（这种排序必须是有实际意义的），取两个的优点是可以画散点图及保留更多的信息。
  
* 如果我们对样品的排序不感兴趣，则一般应考虑取两个主成分，哪怕第二主成分的贡献率明显偏低些，因为取一个主成分不利于作图。此外，通过对前两个或三个主成分的作图，还有助于从直觉上发现异常值、评估正态性以及进行其他的探索性分析等。

### 关于样本容量 $n$ 的大小

* 不同于判别分析，在主成分的计算过程中不涉及 $S$ 或 $R$ 的逆，故理论上允许 $n\leq p$。

* 一般（特别是在主成分本身作为目标的分析中）较理想的是能满足 $n$ 很大（如 $n\geq 50$）且 $n$ 至少是 $p$ 的 $5$ 倍，这样通常可使 $S$ 或 $R$ 的值比较稳定，分析结果一般也就不会随样本容量的变化而发生较大的改变，从而使结论更加可信。

### 关于异常值的影响

* 有时少数几个异常值就可对 $S$ 或 $R$ 的值产生较大甚至是非常大的影响。

* 遇到这种异常值通常可有两种处理方法：
  
  - 一种是从数据中找出异常值并直接剔除。
  
  - 另一种是采用 $\Sigma$ 或 $R$ 的稳健估计，而不是简单计算 $S$ 或 $R$，从而得到一个受异常值影响程度相对较小的估计。

### 关于时间序列数据

在绝大多数情况下下，时间序列数据 $x_1,x_2,\cdots,x_n$ 是彼此相关的，从而不再是一个简单随机样本。此时，由 $x_1,x_2,\cdots,x_n$ 计算得到的 $S$ 将不再是对 $\Sigma$ 的无偏估计，尤其是当 $x_1,x_2,\cdots,x_n$ 彼此间高度相关时更是如此。因此，直接从 $S$ 或 $R$ 出发进行的主成分分析是没有意义的。
  
* 处理方法是将原始数据变换为无相关的数据，如将股票的各个星期中的收盘价转换为周回报率，即

$$\text{本周回报率}=\frac{\text{本周五收盘价}-\text{上周五收盘价}}{\text{上周五收盘价}}$$

### 主成分用于聚类分析

* 主成分用于聚类的优势就在于能够从直观的散点图上进行（或许更有效、合理的）分类。

  - 其优势不在于用来计算样品间的距离。

  - 使用前几个主成分计算样品之间的距离一般不如使用所有的主成分计算距离精确。
  
  - 而使用所有的主成分计算距离相当于直接使用所有的原始变量来计算。

* 用于聚类的主成分在绝大多数场合下纯粹只是一个中间结果，此时一般无需对主成分进行解释。

* 如果希望用图形的方法来评估最终的聚类结果，则使用费希尔判别函数比使用主成分更为合适。

  - 费希尔判别得分散点图：能最大限度地显现出类之间的差异。

  - 主成分得分散点图：最大限度显现的却是样品之间的差异。  

* 从经验来看，在大多数的实际数据中，由主成分得分构成的散点图还是能够基本反映聚类效果的，只是相比费希尔判别的得分散点图一般要逊色一些。

  - 需注意，利用降维后的图形进行聚类不适合使用费希尔判别得分图。 
  
### 关于不同时期的主成分分析

* 一般来说，对于相同的原始变量，某个时期的主成分分析能成功未必意味着其他时期的主成分分析也能成功。

* 不同时期同样成功的主成分分析其主成分解释可能相同，也可能有差异。即使给出相同的解释，其主成分的具体内涵一般也不会完全相同，故不同时期的主成分之间一般是不可比较的。  

### 关于定性数据

* 不能对名义变量数据进行主成分分析，因为差值没有意义。

* 如果是有序变量数据，一般可将其转化为间隔变量数据，然后再进行主成分分析。例如：

  - 假设变量由低到高的等级依次是 $A,B,C,D,E$，如认为相邻等级的差异基本相同，则可转化为 $1,2,3,4,5$ 或 $5,4,3,2,1$。
  
* 上述转化方法一般也可用于其他专门用于间隔变量的统计方法，转化效果取决于我们对各相邻等级之间相对差异的认识程度。

### 对主成分综合得分方法的质疑

在多元数据分析中，国内流行一种通过建立主成分的综合评价函数（以贡献率为权数对前几个主成分加权求和）来对所有样品进行综合排名的方法。

* 该方法是对主成分的错误理解而产生的一种完全错误的方法，貌似合理实则毫无合理性可言，不应使用。

## 主成分分析示例

### 主成分分析的步骤

* 确定是否需要对数据进行标准化处理

* 计算样本的协方差阵或相关阵

* 计算 $S$ 或 $R$ 的特征根和特征向量

* 按主成分累积贡献率超过 $80\%$ 或其它满意的比例，确定主成分的个数

* 计算各个主成分得分

* 对分析结果做统计意义和实际意义的解释

实际计算时用 R 函数计算，这些步骤已经被整合在一些 R 函数中。

在 R 中，对原始数据进行主成分分析，使用程序包 stats 中的函数 princomp，其中：  

* 参数 cor 表示是否从相关矩阵出发计算主成分分析，默认设置为 cor = FALSE，即从协方差矩阵出发。

* 参数 scores 表示是否计算主成分得分，默认设置为  scores = TRUE。

函数 summary 用于输出主成分分析的概况，参数 loadings = TRUE 要求显示载荷系数，即主成分线性组合的系数，也即特征向量。

函数 loadings 能够单独输出载荷。

函数 predict 用于对训练数据或新数据计算主成分得分。注意：计算主成分得分时，都按训练数据的列均值进行中心化。

函数 screeplot 用于绘制从大到小排列的特征值的线图或条形图，称为陡坡图或碎石图，以便确定主成分个数。

如果取的是前两个主成分，使用函数 biplot 绘制双重散点图（biplot）。
  
### 示例 1：Violent Crime Rates by US State

变量说明：

* Murder：numeric	Murder arrests (per 100,000)

*	Assault：numeric Assault arrests (per 100,000)

* UrbanPop：numeric	Percent urban population

*	Rape：numeric	Rape arrests (per 100,000)

```{r}

# 读取数据
  data("USArrests", package = "datasets")
  class(x = USArrests)
  anyNA(x = USArrests)
  head(x = USArrests, n = 3)

```

```{r}

# 读取数据
  attach(what = USArrests)

# 计算相关系数矩阵
  corr.mat <- cor(x = USArrests, method = "pearson")
  corr.mat
  
# 绘制相关系数图：方法 1
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8)
  
  corrplot::corrplot(corr = corr.mat, is.corr = TRUE, method = "ellipse", type = "full",
                     bg = "lightgreen", mar = c(1, 2, 4, 1), addCoef.col = "blue", 
                     order = "FPC", addgrid.col = "black", addCoefasPercent = FALSE, 
                     outline = TRUE, title = "Plot for Correlation Matrix")
  
  par(opar)
  
  detach(name = USArrests)

```

```{r}

# 绘制相关系数图：方法 2
  library(corrgram)  
  
  opar <- par(no.readonly = TRUE)
  par(cex.main = 0.8)
  
  corrgram(x = corr.mat, type = "corr", order = TRUE, upper.panel = panel.pie, 
           lower.panel = panel.shade, text.panel = panel.txt, cex.labels = 1.5, 
           main = "Plot for Correlation Matrix") 
  
  par(opar)

```

```{r}
  
# 绘制相关系数网络图
  library(qgraph)

  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4))
  
  qgraph(input = corr.mat, cut = 0.3, details = TRUE, posCol = "darkgreen")
  
  par(opar)
  
```

相关系数矩阵与相关系数图表明，有些变量之间存在较强的相关性。也就是说，有些变量之间存在信息冗余，这就为数据降维提供了可能。对此，可采用主成分分析进行降维，在降维之后再根据研究目的做进一步地比较分析。

**用方差阵作主成分分析**

```{r}
 
# 读取数据
  attach(what = USArrests) 

# 主成分分析：以列表形式返回结果
  USArrests_std <- datawizard::standardize(x = USArrests)
  USArrests.pc <- princomp(x = USArrests_std, cor = FALSE, scores = TRUE)
  summary(object = USArrests.pc)

  length(x = USArrests.pc)
  names(x = USArrests.pc)

  detach(name = USArrests)

```

```{r}

# 提取观测数量
  USArrests.pc$n.obs

# 提取主成分的标准差，即各个特征根的平方根
  sdevs <- USArrests.pc$sdev
  sdevs

# 提取载荷阵，每列是对应特征根的一个特征向量，以下两条命令完全等价
  USArrests.pc$loadings
  stats::loadings(x = USArrests.pc)
  identical(x = USArrests.pc$loadings, y = stats::loadings(x = USArrests.pc))

# 提取质心，即原始数据中各列的平均值，以下两条命令完全等价  
  USArrests.pc$center
  colMeans(x = USArrests_std, na.rm = TRUE)
  near(x = USArrests.pc$center, y = colMeans(x = USArrests_std, na.rm = TRUE))

# 提取数据的缩放尺度
  USArrests.pc$scale

# 提取主成分得分
  scores <- USArrests.pc$scores
  head(x = scores, n = 3)

```

```{r}

# 绘制陡坡图（也称碎石图）：纵轴就是特征根或方差，用于帮助确定主成分个数

  # 绘制条形图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  screeplot(x = USArrests.pc, npcs = 4, type = "barplot", ylim = c(0, 3), xpd = FALSE,
            panel.first = grid(), col = rainbow(n = 8, alpha = 0.8), border = "gray",
            xlab = "Principal Components", main = "Screeplot for Principal Components")
  box()
  
  par(opar)
  
``` 

```{r}

# 绘制陡坡图（也称碎石图）：纵轴就是特征根或方差，用于帮助确定主成分个数

  # 绘制线型图  
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  screeplot(x = USArrests.pc, npcs = 4, pch = 20, type = "lines", ylim = c(0, 3), 
            xpd = FALSE, panel.first = grid(), col = "blue", 
            main = "Screeplot for Principal Components")
  box()
  
  par(opar)

```

```{r}

# 绘制陡坡图（也称碎石图）：纵轴就是特征根或方差，用于帮助确定主成分个数

  # 绘制线型图：方差累计比例图  
  cmu_prop <- cumsum(x = sdevs^2)/sum(sdevs^2)
  cmu_prop

  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(x = 1:4, y = cmu_prop, type = "o", pch = 20, col = "blue", panel.first = grid(), 
       xlab = "Principal Components", ylab = "Cumulative Proportion",
       main = "Screeplot for Principal Components")

  par(opar)
  
```

在双重散点图中，如果当某个变量与某个观测较为接近，就表示该变量在该观测上的取值较大。

```{r}

# 绘制双重散点图：方法 1
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, las = 1)
  
  biplot(x = USArrests.pc, scale = 0, panel.first = grid(), col = c("blue", "red"), 
         xlab = "First Principal Component", ylab = "Second Principal Component")
  abline(h = 0, v = 0, col = "gray", lty = 1)
  
  par(opar)
  
```

```{r}

# 绘制双重散点图：方法 2
  library(factoextra)
  fviz_pca_biplot(X = USArrests.pc, geom = "point", addEllipses = TRUE, repel = TRUE,
                  col.ind = as.character(x = c(rep(x = "Crimes", 20), rep(x = "Pop", 30))), 
                  palette = c("red", "black"), legend.title = "Components",
                  xlab = "First Principal Component", ylab = "Second Principal Component",
                  title = "双重散点图") +
    theme_bw()

```

```{r}

# 计算前两个主成分的主成分得分：方法 1
  scores_pc12 <- predict(object = USArrests.pc)[, 1:2]
  
# 针对前两个主成分绘制散点图
  ggplot(data = as.data.frame(x = scores_pc12), mapping = aes(x = Comp.1, y = Comp.2)) +
    geom_point(color = "blue") +
    labs(x = "First Principal Component", y = "Second Principal Component",
         title = "Scatter Plot for First Two Principal Components") +
    theme_bw()
  
```

```{r}

# 绘制第一、二主成分得分的散点图：自动添加标签
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(scores_pc12, panel.first = grid(), type = "p", pch = 20, col = "blue", 
       xlab = "First Principal Component", ylab = "Second Principal Component",
       main = "Scatter Plot for First Two Principal Components")
  maptools::pointLabel(x = scores_pc12[, 1], y = scores_pc12[, 2], 
                       labels = rownames(x = USArrests), 
                       cex = 0.8, col = "steelblue", method = "SANN")
  abline(h = 0, v = 0, col = "red", lty = 2)
  
  par(opar)

```

**用相关阵作主成分分析**

上述分析用相关阵重做一遍。

```{r}

# 读取数据
  attach(what = USArrests)  
  
# 计算相关系数矩阵
  corr.mat <- cor(USArrests, method = "pearson")
  corr.mat

# 计算相关系数矩阵的特征根与特征向量
  eigen_vec <- eigen(x = corr.mat, symmetric = TRUE)
  
  eigens <- eigen_vec$values  
  eigens

  eigenvectors <- eigen_vec$vectors
  eigenvectors
  
# 计算每个主成分对方差解释的贡献率
  eigens/sum(eigens)

# 计算主成分对方差解释的贡献率
  cumsum(x = eigens)/sum(eigens)

```

```{r}

# 根据主成分的累积贡献率，绘制陡坡图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(cumsum(x = eigens)/sum(eigens), panel.first = grid(), type = "o", pch = 20, 
       col = "blue", xlab = "Index of Principal Componensts", 
       ylab = "Cumulative Proportion", main = "Screeplot for Principal Components")
  par(opar)
  
```

```{r}

# 主成分分析：使用相关矩阵计算
  USArrests_pc <- princomp(x = USArrests, cor = TRUE, scores = TRUE)
  summary(object = USArrests_pc)

# 提取变量的个数
  USArrests_pc$n.obs

# 提取主成分的标准差，即各个特征根的平方根
  USArrests_pc$sdev
  near(x = USArrests_pc$sdev, y = sqrt(x = eigens))

# 提取载荷阵，每列是对应特征根的一个特征向量，以下两条命令完全等价
  USArrests_pc$loadings
  stats::loadings(x = USArrests_pc)
  identical(x = USArrests_pc$loadings, y = stats::loadings(x = USArrests_pc))

# 提取质心，即相关系数矩阵中各列的平均值，以下两条命令完全等价  
  USArrests_pc$center

# 提取数据的缩放尺度
  USArrests_pc$scale

# 提取主成分得分
  scores_pc12 <- USArrests_pc$scores
  head(x = scores_pc12, n = 3)

```

```{r}

# 针对主成分分析结果，绘制陡坡图（也称碎石图）：纵轴就是特征根或方差，用于帮助确定主成分个数

  # 绘制条形图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  screeplot(USArrests_pc, npcs = 4, type = "barplot", col = rainbow(n = 4, alpha = 0.8),
            border = "gray", panel.first = grid(), xpd = FALSE, ylim = c(0, 3),
            xlab = "Principal Componensts", main = "Scree Plot for Principal Components")
  box()
  
  par(opar)

```

```{r}

# 针对主成分分析结果，绘制陡坡图（也称碎石图）：纵轴就是特征根或方差，用于帮助确定主成分个数

  # 绘制线型图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  screeplot(USArrests_pc, npcs = 4, type = "lines", col = "blue", pch = 20, xpd = FALSE, 
            panel.first = grid(), main = "Scree Plot for Principal Components")
  box()
  
  par(opar)

```

```{r}

# 绘制双重散点图：方法 1
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  biplot(x = USArrests_pc, scale = 0, col = c("blue", "red"), panel.first = grid(), 
         xlab = "First Principal Component", ylab = "Second Principal Component",
         main = "Biplot")
  abline(h = 0, v = 0, col = "gray", lty = 1)
  
  par(opar)

```

```{r}

# 绘制双重散点图：方法 2
  library(factoextra)
  
  fviz_pca_biplot(X = USArrests_pc, axes = c(1, 2), geom = c("point", "text"), repel = TRUE,
                  addEllipses = TRUE, palette = c("blue", "darkgreen"), col.var = "red", 
                  col.ind = as.character(x = c(rep(x = "Crimes", 20), rep(x = "Pop", 30))), 
                  fill.ind = "red", fill.var = "blue", legend.title = "Components") +
    labs(x = "First Principal Component", y = "Second Principal Component",
         title = "Biplot") +
    theme_bw()
  
```  

```{r}

# 绘制载荷散点图
  USArrests_loadings <- USArrests_pc$loadings[, 1:2]
  USArrests_loadings <- as.data.frame(x = USArrests_loadings)
  attach(what = USArrests_loadings)
  
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(USArrests_loadings, type = "p", pch = 20, col = "blue", panel.first = grid(), 
       xlab = "First Principal Component", ylab = "Second Principal Component",
       main = "Scatter Plot for First Two Principal Components")
  maptools::pointLabel(x = Comp.1, y = Comp.2, labels = rownames(x = USArrests_loadings), 
                       col = "red", cex = 0.8, method = "SANN", allowSmallOverlap = FALSE)
  
  par(opar)

```  

### 示例 2：瑞士银行真假钞数据主成分分析

```{r}

# 读取数据
  

  
```


# 因子分析

## 因子分析概述
  
因子分析起源于 20 世纪初的 K.皮尔逊（Pearson）和 C.斯皮尔曼（Spearman）等学者为定义和测定智力所作的工作，主要是由对心理测量学有兴趣的科学家们培育和发展了因子分析。

因子分析的目标和用途与主成分分析极为类似，都是针对数据进行降维的方法，因子分析可视为是对主成分分析的拓展。但是相较于主成分分析，因子分析结果的可解释性要强得多。

### 因子分析与主成分分析的共同点：

* 二者具有相同的目的，都是要把多个变量的信息压缩到少数几个变量中，压缩后的结果在主成分分析中称为主成分，在因子分析中称为因子。

* 二者的理论基础相同，都是使用少数的变量概括原始变量中大部分的协方差信息。

### 因子分析与主成分分析的区别

因子分析与主成分分析的主要区别：

* 主成分分析涉及的只是一般的变量变换（正交旋转），它不能作为一个模型来描述，本质上主成分分析几乎不需要任何假定；而因子分析需要构造一个因子模型，并伴有几个关键性的假定。

* 在主成分分析中，主成分是原始变量的线性组合；而在因子分析中，原始变量是因子的线性组合，但因子却一般不能表示为原始变量的线性组合。

主成分分析: $x_{1}, x_{2}, \cdots, x_{p} \rightarrow y_{1}, y_{2}, \cdots, y_{m}$

$$\begin{array}{c}
y_{1}=t_{11} x_{1}+t_{21} x_{2}+\cdots+t_{p 1} x_{p}=\boldsymbol{t}_{1}^{\prime} \boldsymbol{x} \\
y_{2}=t_{12} x_{1}+t_{22} x_{2}+\cdots+t_{p 2} x_{p}=\boldsymbol{t}_{2}^{\prime} \boldsymbol{x} \\
\vdots \\
y_{m}=t_{1 m} x_{1}+t_{2 m} x_{2}+\cdots+t_{p m} x_{p}=\boldsymbol{t}_{m}^{\prime} \boldsymbol{x}
\end{array}$$

因子分析: $x_{1}, x_{2}, \cdots, x_{p} \rightarrow f_{1}, f_{2}, \cdots, f_{m}$   
  
$$\left\{\begin{array}{c}
x_{1}=\mu_{1}+a_{11} f_{1}+a_{12} f_{2}+\cdots+a_{1 m} f_{m}+\varepsilon_{1} \\
x_{2}=\mu_{2}+a_{21} f_{1}+a_{22} f_{2}+\cdots+a_{2 m} f_{m}+\varepsilon_{2} \\
\vdots \\
x_{p}=\mu_{p}+a_{p 1} f_{1}+a_{p 2} f_{2}+\cdots+a_{p m} f_{m}+\varepsilon_{p}
\end{array}\right.$$
 
* 在主成分分析中，强调的是用少数几个主成分解释总方差；而在因子分析中，强调的是用少数几个因子去描述协方差或相关系数。

* 主成分分析的解是唯一的（除非含有相同的特征根或特征向量为相反符号）；而因子的解可以有很多，表现得较为灵活（主要体现在因子旋转上），这种灵活性使得变量在降维之后更易得到解释，这是因子分析比需要对主成分作出解释的主成分分析有更广泛应用的一个重要原因。

* 主成分的个数不会因其所提取的主成分的个数的改变而变化，但因子分析中的因子数量往往会随模型中因子个数的不同而变化。

### 因子分析的示例

林登（Linden）根据他收集的来自 $139$ 名运动员的比赛数据，对二战后奥林匹克十项全能比赛的得分作了因子分析研究。

因子分析的研究结果显示，十项得分基本上可归结于运动员的手臂爆发力、短跑速度、腿部爆发力以及中长跑的耐力这四个维度，每个维度称为一个因子。

十项得分与四个因子之间的关系可以描述为如下的因子模型：

$$x_i = \mu_i + a_{i1}f_1+a_{i2}f_2+a_{i3}f_3+a_{i4}f_4+\epsilon_i,\quad i=1, 2, \cdots,10$$

其中：

* $f_1,f_2,f_3,f_4$ 分别表示四个因子，称为公共因子（简称因子）

* 系数 $a_{ij}$ 称为原始变量 $x_i$ 在公共因子 $f_j$ 上的载荷，$f_j$ 反映了该因子对原始变量 $x_i$ 的重要性

* $\mu_i$ 是变量 $x_i$ 的均值

* $\epsilon_i$ 是 $x_i$ 不能被四个公共因子解释的部分，可视为误差，称为特殊因子

## 正交因子模型 
 
### 数学模型

设有 $p$ 维可观测的随机向量 $x=(x_1,x_2,\cdots, x_p)^\prime$，其均值向量为 $\mu=(\mu_1,\mu_2,\cdots, \mu_p)^\prime$，协差阵为 $\Sigma=(\sigma{_ij})$。因子分析的一般模型为：

$$\left\{\begin{array}{c}
x_{1}=\mu_{1}+a_{11} f_{1}+a_{12} f_{2}+\cdots+a_{1 m} f_{m}+\varepsilon_{1} \\
x_{2}=\mu_{2}+a_{21} f_{1}+a_{22} f_{2}+\cdots+a_{2 m} f_{m}+\varepsilon_{2} \\
\vdots \\
x_{p}=\mu_{p}+a_{p 1} f_{1}+a_{p 2} f_{2}+\cdots+a_{p m} f_{m}+\varepsilon_{p}
\end{array}\right.$$  

其中，$f_1,f_2,\cdots,f_m$ 为公共因子，$\epsilon_1,\epsilon_2,\cdots,\epsilon_m$ 为特殊因子，它们都是不可观测的随机变量，也称隐变量。
 
* 由于公共因子出现在每一个原始变量的表达式中，可理解为原始变量共同具有的公共因素。

因子分析的一般模型可用矩阵表示为：$$\boldsymbol{x}=\boldsymbol{\mu}+\boldsymbol{Af}+\boldsymbol{\epsilon}$$

上式中 $\boldsymbol{f}=\left(f_{1}, f_{2}, \cdots, f_{m}\right)^{\prime}$ 称为公共因子向量，$\varepsilon=\left(\varepsilon_{1}, \varepsilon_{2}, \cdots, \varepsilon_{p}\right)^{\prime}$  为特殊因子向量，$\boldsymbol{A}=\left(a_{i j}\right): p \times m$ 称为因子载荷矩阵。 

通常假定：
 
$$\left\{\begin{array}{l}
E(\boldsymbol{f})=\mathbf{0} \\
E(\boldsymbol{\varepsilon})=\mathbf{0} \\
V(\boldsymbol{f})=\boldsymbol{I} \\
V(\varepsilon)=\boldsymbol{D}=\operatorname{diag}\left(\sigma_{1}^{2}, \sigma_{2}^{2},\cdots, \sigma_{p}^{2}\right) \\
\operatorname{Cov}(\boldsymbol{f}, \boldsymbol{\varepsilon})=E\left(\boldsymbol{f} \varepsilon^{\prime}\right)=\mathbf{0}
\end{array}\right.$$ 

其中：$V(\boldsymbol{f})=\boldsymbol{I}$ 表示公共因子之间是正交的，也即公共因子之间是彼此无关的。

该假定和上述关系式构成了正交因子模型。由上述假定可以看出，公共因子彼此不相关且具有单位方差，特殊因子也彼此不相关且和公共因子也不相关。 

### 正交因子模型的性质

**$x$ 的协差阵 $\Sigma$ 的分解**

$p$ 维向量 $x$ 的协差阵 $\boldsymbol\Sigma$ 可分解为：

$$\boldsymbol\Sigma=\boldsymbol{AA^\prime}+\boldsymbol{D}$$ 

如果 $\boldsymbol{A}$ 只有少数几列，则上述分解式揭示了 $\boldsymbol{\Sigma}$ 的一个简单结构。由于 $\boldsymbol{D}$ 是对角矩阵，故 $\boldsymbol{\Sigma}$ 的非对角线元素可由 $\boldsymbol{A}$ 的元素确定，即因子载荷完全决定了原始变量之间的协方差。具体有：

$$\sigma_{ij}=a_{i1} a_{j1}+a_{i2} a_{j2}+\cdots+a_{im} a_{jm}, \quad 1 \leq i \neq j \leq p$$
  
如果 $\boldsymbol{x}$ 为各分量已标准化了的随机向量，则 $\Sigma$ 就是相关阵 $\boldsymbol{R}$，即有
  
$$\boldsymbol{R}=\boldsymbol{A} \boldsymbol{A}^{\prime}+\boldsymbol{D}$$

相应地有

$$\rho_{i j}=a_{i 1} a_{j 1}+a_{i 2} a_{j 2}+\cdots+a_{i m} a_{j m}, \quad 1 \leq i \neq j \leq p$$

若取 $\boldsymbol{A}=\Sigma^{1/2}, \boldsymbol{D}=\mathbf{0}$，则有分解式

$$\Sigma=\Sigma^{1 / 2} \Sigma^{1 / 2}+\mathbf{0}$$
  
此时 $m=p$，没有达到降维目的，故所作的因子分析没有意义。
  
出于降维的需要, 我们常常希望 $m$ 要比 $p$ 小得多, 这样前述 $\boldsymbol{\Sigma}$ 的分解式通常只能近似成立, 即有
  
$$\Sigma \approx \boldsymbol{A} \boldsymbol{A}^{\prime}+\boldsymbol{D}$$

近似程度越好, 表明因子模型拟合得越好。

在因子数 $m$ 的选择上，我们既希望 $m$ 尽可能小又希望因子模型的拟合尽可能好，而这两个目标是彼此矛盾的，实践中我们应确定一个折中的、合理的 $m$。  

**因子载荷是不唯一的**

* 可以证明因子载荷矩阵 $A$ 是不唯一的，在实际应用中常常利用这一点，通过因子的旋转，使得新的因子有更好的实际意义。

**因子载荷矩阵 $A$ 的统计意义**
  
* $A$ 的元素

$\operatorname{Cov}(\boldsymbol{x}, \boldsymbol{f})=\operatorname{Cov}(\boldsymbol{A} \boldsymbol{f}+\boldsymbol{\varepsilon}, \boldsymbol{f})=\boldsymbol{A V}(\boldsymbol{f})+\operatorname{Cov}(\boldsymbol{\varepsilon}, \boldsymbol{f})=\boldsymbol{A}$  

或

$$\operatorname{Cov}\left(x_{i}, f_{j}\right)=a_{i j}, \quad i=1,2, \mathrm{~L}, p, \quad j=1,2, \cdots, m$$ 

上式表明，因子载荷矩阵中 $A$ 中的元素就是原始变量与因子的协方差矩阵中的元素。

若 $\boldsymbol{x}$ 为各分量已标准化了的随机向量, 则

$$\begin{aligned}
\rho\left(x_{i}, f_{j}\right) &=\frac{\operatorname{Cov}\left(x_{i}, f_{j}\right)}{\sqrt{V\left(x_{i}\right) V\left(f_{j}\right)}}=\operatorname{Cov}\left(x_{i}, f_{j}\right)=a_{i j} \\
i &=1,2, \cdots, p, \quad j=1,2, \cdots, m
\end{aligned}$$

上式表明，因子载荷矩阵 $A$ 中的元素还等于原始变量与因子的相关系数矩阵中的元素。
  
**$A$ 的行元素平方和**

* $p$ 维向量 $x$ 中的元素 $x_i$ 的方差可表示为：

$$\begin{aligned}
V\left(x_{i}\right) &=a_{i 1}^{2} V\left(f_{1}\right)+a_{i 2}^{2} V\left(f_{2}\right)+\cdots+a_{i m}^{2} V\left(f_{m}\right)+V\left(\varepsilon_{i}\right) \\
&=a_{i 1}^{2}+a_{i 2}^{2}+\cdots+a_{i m}^{2}+\sigma_{i}^{2}, \quad i=1,2, \cdots, p
\end{aligned}$$

令 

$$h_{i}^{2}=\sum_{j=1}^{m} a_{i j}^{2}, \quad i=1,2, \cdots, p, \quad h_{i}^{2}$$  

则 $h_{i}^{2}$ 可看成是全部公共因子 $f_{1}, f_{2}, \cdots, f_{m}$ 对变量 $x_{i}$ 的方差贡献，称为共性方差，它反映了全部公共因子对变量 $x_{i}$ 的总的影响，而 $\sigma_{i}^{2}$ 是特殊因子 $\varepsilon_{i}$ 对变量 $x_{i}$ 的方差贡献，称为特殊方差。

$\sigma_{ii}=h_{i}^{2}+\sigma_{i}^{2}, \quad i=1,2, \cdots, p$，它表示 $x_i$ 的方差可分解为共性方差与特殊方差之和。

当 $x$ 为各分量已标准化了的随机向量时，$\sigma_{ii}=1$，此时有

$$h_{i}^{2}+\sigma_{i}^{2}=1, \quad i=1,2, \cdots, p$$

也就是说，$A$ 的行元素平方和反映了全部因子对变量 $x_i$ 方差的联合贡献。

**$A$ 的列元素平方和**

$$\begin{aligned}
\sum_{i=1}^{p} V\left(x_{i}\right) &=\sum_{i=1}^{p} a_{i 1}^{2} V\left(f_{1}\right)+\cdots+\sum_{i=1}^{p} a_{i m}^{2} V\left(f_{m}\right)+\sum_{i=1}^{p} V\left(\varepsilon_{i}\right) \\
&=g_{1}^{2}+\cdots+g_{m}^{2}+\sum_{i=1}^{p} \sigma_{i}^{2}
\end{aligned}$$

其中

$$g_{j}^{2}=\sum_{i=1}^{p} a_{i j}^{2}, \quad j=1,2, \cdots, m$$

$g_{j}^{2}$ 反映了特定公共因子 $f_{j}$ 对整体 $p$ 维向量 $x$ 的影响，是衡量公共因子 $f_{j}$ 重要性的一个尺度，可视为公共因子 $f_{j}$ 对整体 $p$ 维向量 $x$ 的总方差贡献。

也就是说，$A$ 的列元素平方和反映了某个特定公共因子对整体 $p$ 维向量 $x$ 的联合贡献。

**$A$ 的各个元素的平方和**

因子载荷矩阵 $A$ 中的各个元素的平方和反映了全部公共因子对整体 $p$ 维向量 $x$ 总方差的累计贡献。

在正交因子模型中虽然全部因子可 $100\%$ 地解释原始 $p$ 维向量 $x$ 中各个分量之间的所有协方差或相关系数，但并不能保证这些因子一定能解释 $x_1,x_2,\cdots,x_p$ 总方差的多大比例。理论上该比例可以是较低的，甚至很低。

尽管如上所述，但在因子分析的许多实践中，因子模型在拟合得好的同时，公共因子所解释的总方差的累计比例往往也是较高的。正因如此，因子分析常常如同主成分分析那样用于分析样品之间的差异性。在此种应用中，公共因子所解释的总方差的累计比例需要达到一个较高的水平。
  
## 因子分析中模型参数的估计

因子分析模型中的未知参数是指载荷矩阵 $A$ 和特殊方差。对这些未知参数的估计，可采用主成分法、极大似然法、主因子法等。

### 主成分法
 
设样本协差阵 $S$ 的特征根依次为 $\lambda_1,\lambda_2,\cdots,\lambda_p$，其相应的正交单位特征向量依次为 $t_1,t_2,\cdots,t_p$。选取相对较小的因子数 $m$，并使得累计贡献率达到一个较高的百分比，则 $S$ 可近似分解为：

$$S\approx AA^\prime+D$$

其中，$A$ 和 $D$ 就是因子模型的一个主因子解。

对主因子解，$f_j$ 对 $x$ 的总方差贡献为 $\lambda_j$。当因子数 $m$ 增加时，原来因子的估计载荷保持不变。

$S\approx AA^\prime+D$ 的近似程度越好，表明因子模型拟合得越好。
  
当 $p$ 个原始变量的单位不同或虽单位相同但各变量的方差相差较大时，我们应首先对原始变量作标准化变换，也就是从 $R$ 出发求解。
  
由于因子分析中强调用少数几个因子来描述原始变量之间的协方差或相关系数而非原始变量自身的方差，所以相比于主成分分析，因子分析有更多的场合适合从 $R$ 出发。
  
主成分法与主成分分析有着很相似的名称，两者很容易混淆。虽然第 $j$ 个因子与第 $j$ 个主成分的解释完全相同，但主成分法与主成分分析本质上却是两个不同的概念。

### 主成分法的计算

假定原始向量 $\boldsymbol{x}$ 的各分量已作了标准化变换。如果随机向量 $\boldsymbol{x}$ 满足正交因子模型, 则有

$$\boldsymbol{R}=\boldsymbol{A} \boldsymbol{A}^{\prime}+\boldsymbol{D}$$

令

$$\boldsymbol{R}^{*}=\boldsymbol{R}-\mathrm{D}=\boldsymbol{A} \boldsymbol{A}^{\prime}$$

则称 $R^{*}$ 为 $x$ 的约相关矩阵。

$\boldsymbol{R}^{*}$ 中的第 $i$ 个对角线元素是 $h_{i}^{2}$，而不像 $\boldsymbol{R}$ 中是 $1$，非对角线元素与 $\boldsymbol{R}$ 中是完全一样的，并且 $\boldsymbol{R}^{*}$ 也是一个非负定矩阵。

约相关矩阵 $\boldsymbol{R}^{*}$ 可估计为：

$$\boldsymbol{R}^{*}=\hat{R}-\hat{D}$$

其中，$\hat{\boldsymbol{R}}=\left(r_{i j}\right), \hat{\boldsymbol{D}}=\operatorname{diag}\left(\hat{\sigma}_{1}^{2}, \hat{\sigma}_{2}^{2}, \cdots, \hat{\sigma}_{p}^{2}\right), \hat{h}_{i}^{2}=1-\hat{\sigma}_{i}^{2}$ 是 $h_{i}^{2}$ 的初始估计。

又设 $\hat{\boldsymbol{R}}^{*}$ 的前 $m$ 个特征根依次为 $\hat{\lambda}_{1}^{*} \geq \hat{\lambda}_{2}^{*} \geq \cdots \geq \hat{\lambda}_{m}^{*}>0$，相应的正交单位特征向量为 $\hat{\boldsymbol{t}}_{1}^{*}, \hat{\boldsymbol{t}}_{2}^{*}, \cdots, \hat{\boldsymbol{t}}_{m}^{*}$，则 $\boldsymbol{A}$ 的主因子解为：

$$\hat{\boldsymbol{A}}=\left(\sqrt{\hat{\lambda}_{1}^{*}} \hat{\boldsymbol{t}}_{1}^{*}, \sqrt{\hat{\lambda}_{2}^{*}} \hat{\boldsymbol{t}}_{2}^{*}, \cdots, \sqrt{\hat{\lambda}_{m}^{*}} \hat{\boldsymbol{t}}_{m}^{*}\right)$$

由此可更新 $h_{i}^{2}$ 的估计，同时更新 $\sigma_{i}^{2}$ 的估计，并将其作为 $\sigma_{i}^{2}$ 的最终估计。即，

$$\hat{\sigma}_{i}^{2}=1-\hat{h}_{i}^{2}=1-\sum_{j=1}^{m} \hat{a}_{i j}^{2}, \quad i=1,2, \cdots, p$$

特殊（或共性）方差的常用初始估计方法：

* 取 $\hat{\sigma}_{i}^{2}=1 / r_{ii}$，其中 $r_{ii}$ 是 $\hat{\boldsymbol{R}}^{-1}$ 的第 $i$ 个对角线元素，此时共性方差的估计为 $\hat{h}_{i}^{2}=1-\hat{\sigma}_{i}^{2}$，它是 $x_{i}$ 和其他 $p-1$ 个变量间（样本）复相关系数的平方。

  - 该初始估计方法最为常用，但一般要求 $\hat{\boldsymbol{R}}$ 是满秩的。如果 $\hat{\boldsymbol{R}}$ 不满秩，则可考虑下面的两种方法。

* 取 $\hat{h}_{i}^{2}=\max _{j \neq i}\left|r_{i j}\right|$，此时 $\hat{\sigma}_{i}^{2}=1-\hat{h}_{i}^{2}$。 

* 取 $\hat{h}_{i}^{2}=1$ 此时，$\hat{\sigma}_{i}^{2}=0$，有 $\hat{\boldsymbol{R}}^{*}=\hat{\boldsymbol{R}}$，由此得到的 $\hat{A}$ 是一个主成分解。

在 R 中，主成分法与极大似然法的因子分析使用程序包 psych 中的函数 fa，其中： 

* 参数 r：相关阵、协差阵或原始数据矩阵

* 参数 nfactors：提取的因子个数，默认为 1

* 参数 n.obs：观测数，参数 r 为相关系数矩阵时需要指定

* 参数 rotate：因子旋转方法，默认为 rotate = "oblimin"（斜交转轴法），常用的是 "varimax"（最大方差法）或 "none"（不旋转）

  - rotate 正交旋转可取 "none", "varimax", "quartimax", "bentlerT", "equamax", "varimin", "geominT", "bifactor" 
  
  - rotate 斜交旋转可取 "Promax", "promax", "oblimin", "simplimax", "bentlerQ, "geominQ" and "biquartimin", "cluster" 

* 参数 scores：设定是否需要计算因子得分，默认为 scores = "regression"

* 参数 fm：指定提取公共因子的方法，默认为 fm = "minres"（极小残差法）。此外还可以设置

  - fm = "ml"（极大似然法）、"pa"（主因子法）、"wls"（加权最小二乘法）、"gls"（广义最小二乘法）

* 参数 residuals：是否显示残差矩阵，默认设置 residuals = FALSE

* 参数 SMC：是否使用平方相关系数，默认设置 SMC = TRUE

在 R 中，进行因子分析之前可使用程序包 psych 中的函数 fa.parallel 来帮助确定合适的因子数量。

* 函数 fa.parallel 通过将原始数据与相同维度的随机数矩阵相比较来确定最合适的因子数量，所绘制的图形称为平行碎石图。

  - 参数 fm：表示所要使用的因子分析方法，可取 "minres"、"ml"、"uls"、"wls"、"gls"、"pa"，默认设置 fm = "minres"

  - 参数 fa：表示要使用哪种特征根，默认设置 fa = "both" 表示同时使用主成分特征根（pc）和主因子特征根（pa）。

  - 参数 SMC：表示使用平方相关系数估计特征根，默认设置 SMC = TRUE。  
  
### 示例 1：八项男子径赛运动记录

* $x_1$：100米（秒）		 
 
* $x_2$：200米（秒）		 
   
* $x_3$：400米（秒）  

* $x_4$：800米（秒）		 

* $x_5$：1500米（分）

* $x_6$：5000米（分）

* $x_7$：10000米（分）

* $x_8$：马拉松（分）

**使用极小残差法**

```{r}

# 读取数据
  sports <- rio::import(file = "Data/exec6.6.xlsx") 
  class(x = sports)
  anyNA(x = sports)

  sports_new <- tibble::column_to_rownames(.data = sports, var = "nation")
  class(x = sports_new)
  head(x = sports_new, n = 3)

```

```{r}

# 读取数据
  attach(what = sports_new)
  
# 绘制平行碎石图：用于确定因子数量
  library(psych)
  
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  fa.parallel(x = sports_new, fm = "minres", fa = "both", SMC = TRUE)
  panel.last = grid()
  par(opar)
  
  detach(name = sports_new)

```

```{r}

# 读取数据
  attach(what = sports_new)

# 计算相关矩阵
  corr.mat <- cor(x = sports_new, method = "pearson")
  corr.mat

# 因子分析：未旋转，从相关矩阵出发，以列表形式返回结果
  library(psych)
  
  sports.fa <- fa(r = sports_new, nfactors = 2, rotate = "none", fm = "minres")
  print(x = sports.fa)
  summary(object = sports.fa)

  detach(name = sports_new)

```  

```{r}

# 提取因子数量
  sports.fa$factors  

# 提取因子载荷矩阵及因子方差贡献率：列元素的平方和、因子所解释的总方差的比例及累计比例
  sports.fa$loadings

# 提取共性方差
  sports.fa$communalities

# 提取每个因子能够解释的全体变量的方差：特征根
  sports.fa$e.values

# 计算因子得分：以矩阵和数组的形式返回结果
  scores_fa <- sports.fa$scores
  head(x = scores_fa, n = 3)

# 提取残差矩阵：残差矩阵对角线上的元素就是特殊方差
  sports.fa$residual

# 检查因子模型对原始相关矩阵的拟合程度
  sports.fa$fit

# 检查非对角线上元素的再现程度：原始数据的再现程度
  sports.fa$fit.off

# 检查使用了哪种因子旋转方法
  sports.fa$method

# 提取观测数量：原始数据的样本容量
  sports.fa$n.obs

```  

```{r}

# 绘制因子得分散点图：方法 1
  scores_fa_df <- as.data.frame(x = scores_fa)
  attach(what = scores_fa_df)

  ggplot(data = scores_fa_df, mapping = aes(x = MR1, y = MR2)) +
    geom_point() +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    geom_vline(xintercept = 0, color = "red", linetype = "dashed") +
    labs(x = "Factor 1", y = "Factor 2", title = "Scatter Plot for Factors") +
    theme_bw()
  
  detach(name = scores_fa_df)

```

```{r}

# 读取数据
  attach(what = scores_fa_df)

# 绘制因子得分散点图：方法 2
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)

  plot(scores_fa_df, type = "p", col = "blue", pch = 20, panel.first = grid(), 
       xlab = "Factor 1", ylab = "Factor 2", main = "Scatter Plot for Factors")
  maptools::pointLabel(x = MR1, y = MR2, labels = rownames(x = scores_fa_df), 
                       col = "red", cex = 0.8, method = "SANN", allowSmallOverlap = FALSE)
  abline(h = 0, v = 0, col = "gray", lty = 1)
  
  par(opar)

  detach(name = scores_fa_df)

```

```{r}

# 读取数据
  fa_loadings <- loadings(sports.fa)
  items <- c("100米", "200米", "400米", "800米", "1500米", "5000米", "10000米", "马拉松")
  
# 用因子载荷阵前两个因子绘制散点图  
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(fa_loadings, type = "p", col = "blue", pch = 20, panel.first = grid(), 
       xlim = c(0.8, 1), ylim = c(-0.4, 0.6), xlab = "Factor 1", ylab = "Factor 2", 
       main = "Scatter Plot for Factor Loadings")
  text(fa_loadings, labels = items, col = "red", adj = -0.5, pos = 1, cex = 0.8)
  
  par(opar)

```

**使用极大似然法** 

设 $\boldsymbol{f} \sim N_{m}(\mathbf{0}, \boldsymbol{I}),\; \boldsymbol{\varepsilon} \sim N_{p}(\mathbf{0},\boldsymbol{D})$ 且相互独立，则由样本 $\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \cdots, \boldsymbol{x}_{n}$ 可计算得到 $(\boldsymbol{\mu}, \boldsymbol{A}, \boldsymbol{D})$ 的极大似然估计 $(\hat{\boldsymbol{\mu}}, \hat{\boldsymbol{A}}, \hat{\boldsymbol{D}})$。 其中，$\hat{\boldsymbol{\mu}}=\overline{\boldsymbol{x}}, \hat{\boldsymbol{A}}$ 和 $\hat{\boldsymbol{D}}$ 一般可用迭代方法求得。

```{r}

# 读取数据
  attach(what = sports_new)

# 绘制平行碎石图：用于确定因子数量
  library(psych)
  
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  fa.parallel(x = sports_new, fm = "ml", fa = "both", SMC = TRUE)
  panel.last = grid()

  par(opar)
  
  detach(name = sports_new)

```

```{r}

# 读取数据
  attach(what = sports_new)

# 计算相关矩阵
  corr.mat <- cor(sports_new, method = "pearson")
  
# 因子分析：使用极大似然法，未旋转，取因子个数为 2，以列表形式返回结果
  sports.fa <- fa(r = sports_new, nfactors = 2, rotate = "none", fm = "ml", 
                  scores = "tenBerge")  
  sports.fa
  summary(object = sports.fa)
  
  detach(name = sports_new)

```

```{r}

# 读取数据
  scores_fa <- sports.fa$scores
  scores_fa <- as.data.frame(x = scores_fa)
  attach(what = scores_fa)

# 绘制因子得分散点图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(scores_fa, type = "p", panel.first = grid(), col = "blue", pch = 20,
       xlab = "Factor 1", ylab = "Factor 2", main = "Scatter Plot for Factors")
  maptools::pointLabel(x = ML1, y = ML2, labels = rownames(x = scores_fa_df), cex = 0.8, 
                       col = "steelblue", method = "SANN", allowSmallOverlap = FALSE)
  abline(h = 0, v = 0, col = "gray", lty = 1)

  par(opar)
  
  detach(name = scores_fa)

```

```{r}

# 读取数据
  fa_loadings <- loadings(sports.fa)
  items <- c("100米", "200米", "400米", "800米", "1500米", "5000米", "10000米", "马拉松")
  
# 用因子载荷阵前两个因子绘制散点图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(fa_loadings, type = "p", col = "blue", pch = 20, panel.first = grid(), 
       xlim = c(0.7, 1), ylim = c(-0.4, 0.7), xlab = "Factor 1", ylab = "Factor 2", 
       main = "Scatter Plot for Factor Loadings")
  text(fa_loadings, labels = items, col = "red", adj = -0.5, pos = 1, cex = 0.8)
  
  par(opar)
  
```

## 因子旋转

因子的解释带有一定的主观性，我们常常通过旋转因子的方法来减少这种主观性且使之更易解释。

因子是否易于解释，很大程度上取决于因子载荷矩阵 $A$ 的元素结构。假设 $A$ 是从 $R$ 出发求得的，则有 $|a_{ij}|\leq 1$。

如果 $A$ 的所有元素都接近 $0$ 或 $\pm 1$，则模型的因子就易于解释。反之，如果 $A$ 的元素多数居中，不大不小，则对模型的因子往往就不易作出解释，此时应考虑进行因子旋转，使得旋转之后的载荷矩阵在每一列上元素的绝对值尽量地大小拉开，也就是尽可能多地使其中的一些元素接近于 $0$，另一些元素接近于 $\pm 1$。

```{r}

# 因子旋转
  img <- readPNG(source = "Pictures/Factor_Rotation.png")
  grid.raster(image = img)
  
```
  
* 因子的正交旋转不改变因子的共性方差，且共性方差为旋转后坐标点到原点的平方欧氏距离。

* 因子的正交旋转不改变 $m$ 个因子的累计贡献率。

当因子数 $m>2$ 时，我们一般就无法通过目测确定旋转，此时需要通过一种算法来给出正交矩阵 $T$。不同的算法构成了正交旋转的各种不同方法，在这些方法中使用最普遍的是最大方差旋转法。 

### 主成分法：使用最大方差旋转法

```{r}

# 读取数据
  attach(what = sports_new)

# 绘制平行碎石图：用于确定最合适的因子数量
  library(psych)

  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  fa.parallel(x = sports_new, fm = "minres", fa = "both", SMC = TRUE)

  detach(name = sports_new)

```

```{r}

# 读取数据
  attach(what = sports_new)

# 因子分析：使用主成分法，使用最大方差旋转法，从相关矩阵出发，以列表形式返回结果
  library(psych)
  sports.fa <- fa(r = sports_new, nfactors = 2, scores = "regression", rotate = "varimax",
                  residuals = TRUE, SMC = TRUE, fm = "minres")
  sports.fa
  summary(object = sports.fa)
  
  detach(name = sports_new)
  
```

```{r}

# 提取因子载荷矩阵及因子方差贡献率：列元素的平方和、因子所解释的总方差的比例及累计比例
  sports.fa$loadings

# 计算因子得分
  scores_fa <- sports.fa$scores
  head(x = scores_fa, n = 3)

```

```{r}

# 读取数据
  scores_fa <- as.data.frame(x = scores_fa)
  attach(what = scores_fa)

# 绘制因子得分散点图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(scores_fa, type = "p", panel.first = grid(), col = "blue", pch = 20,
       xlab = "Factor 1", ylab = "Factor 2", main = "Scatter Plot for Factors")
  maptools::pointLabel(x = MR1, y = MR2, labels = rownames(x = scores_fa_df), cex = 0.8, 
                       col = "steelblue", method = "SANN", allowSmallOverlap = FALSE)
  abline(h = 0, v = 0, col = "gray", lty = 1)

  par(opar)
  
  detach(name = scores_fa)

```

```{r}

# 读取数据
  fa_loadings <- loadings(sports.fa)
  items <- c("100米", "200米", "400米", "800米", "1500米", "5000米", "10000米", "马拉松")
  
# 用因子载荷阵前两个因子绘制散点图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(fa_loadings, type = "p", col = "blue", pch = 20, panel.first = grid(), 
       xlim = c(0.2, 1), ylim = c(0.2, 1), xlab = "Factor 1", ylab = "Factor 2", 
       main = "Scatter Plot for Factor Loadings")
  text(fa_loadings, labels = items, col = "red", adj = -0.5, pos = 1, cex = 0.8)
  
  par(opar)
  
```

### 极大似然法：使用最大方差旋转法

```{r}

# 读取数据
  attach(what = sports_new)

# 因子分析：使用极大似然法，使用最大方差旋转法，取因子个数为 2，以列表形式返回结果
  library(psych)
  sports.fa <- fa(r = sports_new, nfactors = 2, scores = "regression", residuals = TRUE, 
                  SMC = TRUE, fm = "ml", rotate = "varimax") 
  sports.fa
  summary(object = sports.fa)

  detach(name = sports_new)

``` 

```{r}

# 提取因子载荷矩阵及因子方差贡献率：列元素的平方和、因子所解释的总方差的比例及累计比例
  sports.fa$loadings
  
# 计算因子得分
  scores_fa <- sports.fa$scores

```

```{r}

# 读取数据
  scores_fa <- as.data.frame(x = scores_fa)
  attach(what = scores_fa)

# 绘制因子得分散点图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(scores_fa, type = "p", panel.first = grid(), col = "blue", pch = 20,
       xlab = "Factor 1", ylab = "Factor 2", main = "Scatter Plot for Factors")
  maptools::pointLabel(x = ML1, y = ML2, labels = rownames(x = scores_fa_df), cex = 0.8, 
                       col = "steelblue", method = "SANN", allowSmallOverlap = FALSE)
  abline(h = 0, v = 0, col = "gray", lty = 1)

  par(opar)
  
  detach(name = scores_fa)

```

```{r}

# 读取数据
  fa_loadings <- loadings(sports.fa)
  items <- c("100米", "200米", "400米", "800米", "1500米", "5000米", "10000米", "马拉松")
  
# 用因子载荷阵前两个因子绘制散点图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(fa_loadings, type = "p", col = "blue", pch = 20, panel.first = grid(), 
       xlim = c(0.2, 1), ylim = c(0.2, 1), xlab = "Factor 1", ylab = "Factor 2", 
       main = "Scatter Plot for Factor Loadings")
  text(fa_loadings, labels = items, col = "red", adj = -0.5, pos = 1, cex = 0.8)
  
  par(opar)
  
``` 
    
## 因子得分 

在前面的分析中，我们主要做了以下工作：

* 使用因子模型并根据样本选择合适的因子数量和估计出因子载荷矩阵，然后对因子给出合理的解释。
  
* 如果对因子难以做出解释或希望得到更好的解释，则进一步做因子旋转。
  
如果希望对因子做进一步的深入分析，如对降维后的各个样品进行比较和分析，则需要计算因子得分。计算因子得分有两种方法：

* 回归法：回归法也称汤姆森（Thompson，1951）因子得分。

* 采用类似于回归分析中加权最小二乘估计方法得到的因子得分称为称为巴特莱特（Bartlett，1937）因子得分。

* 回归法相比加权最小二乘法有着更高的估计精度，因而在实际应用中，回归法应用得最为广泛。

需要指出，计算因子得分时必须同时使用全部因子，不可将其中一个或两个单独拿出来使用，否则因子所含的信息量是不够的，以
致不足以代表原始变量。
    
# 对应分析
  
## 引言
  
对应分析（Correspondence analysis）又称为相应分析或 $R—Q$ 分析，是在因子分析的基础发展起来的一种多元统计分析方法。

对应分析主要通过分析由定性变量构成的列联表来揭示变量之间的关系。

随着计算机软件的应用，对应分析的方法在社会科学和自然科学领域都有着广泛的应用价值。特别是近年来在市场调查与研究中，有关市场细分、产品定位、品牌形象以及满意度研究等领域正得到越来越广泛的重视和应用。

对数据作对应分析之前，需要先了解因素间是否独立，如果因素之间相互独立，则没有必要进行对应分析。

## 行轮廓和列轮廓

### 对应矩阵

对应矩阵定义为：$$\boldsymbol{P}=\left(p_{i j}\right)=\left(n_{i j} / n\right)$$

行边缘频率构成的列向量:

$$\boldsymbol{r}=\boldsymbol{P} \mathbf{1}=\left(\begin{array}{c}
p_{1} \\
p_{2} \\
\vdots \\
p_{p .}
\end{array}\right)$$

其中 $\mathbf{1}=(1,1, \cdots, 1)^{\prime}: p \times 1$。

列边缘频率构成的行向量:

$$\boldsymbol{c}^{\prime}=\mathbf{1}^{\prime} \boldsymbol{P}=\left(p_{\cdot 1}, p_{\cdot 2}, \cdots, p_{\cdot q}\right)$$

其中，$$\mathbf{1}=(1,1, \cdots, 1)^{\prime}: q \times 1$$

### 行轮廓与列轮廓构成的矩阵

行轮廓矩阵

$$\boldsymbol{R}=\left(\begin{array}{c}
\boldsymbol{r}_{1}^{\prime} \\
\boldsymbol{r}_{2}^{\prime} \\
\vdots \\
\boldsymbol{r}_{p}^{\prime}
\end{array}\right)$$

列轮廓矩阵

$$\boldsymbol{C}=\left(\boldsymbol{c}_{1}, \boldsymbol{c}_{2}, \cdots, \boldsymbol{c}_{q}\right)$$

```{r}

# 读取数据
  mydata <- rio::import(file = "Data/examp9.2.1.xlsx")
  attach(what = mydata)

# 创建列联表
  mytab <- xtabs(frequency ~ status + psychhealth, data = mydata)
  mytab

# 检验列联表的行列是否独立
  chisq.test(mytab, correct = FALSE)

  detach(name = mydata)

```

```{r}

# 创建对应表：含行列轮廓
  gmodels::CrossTable(mytab, digits = 4, expected = TRUE, prop.r = TRUE, prop.c = TRUE, 
                      prop.t = TRUE, chisq = TRUE, prop.chisq = TRUE)

```  

## 总惯量
  
### 总惯量的定义

总惯量表示为：

$$\sum_{i=1}^{p} \sum_{j=1}^{q} \frac{\left(p_{i j}-p_{i \cdot} \cdot p_{\cdot j}\right)^{2}}{p_{i \cdot} p_{\cdot j}}$$

* 总惯量的值越大, 表明实际频率 $p_{ij}$ 与行、列变量之间独立情形下的期望频率 $p_{i.}p_{.j}$ 总体上差异就越大，也就是行、列变量之间越是不独立。

总惯量可作为行、列变量之间关联性的度量：

* 总惯量越大，表明行、列变量之间的关联性越强；反之，则越弱。

总惯量也可度量行轮廓之间的总变差和列轮廓之间的总变差。
  
* 总惯量越大，表明行轮廓之间的差异性就越大；反之，就越小。同理，总惯量越大，表明列轮廓之间的差异性就越大。
 
* 总惯量为零，意味着行变量与列变量基本上是独立的或者说是近似独立的。行变量与列变量独立，意味着总惯量接近于零。

* 行变量与列变量独立，或总惯量为零，一般都没有必要将这两个变量构造成列联表。 

### 总惯量的分解
  
$$\boldsymbol{P}-\boldsymbol{r} \boldsymbol{c}^{\prime}=\left(p_{i j}-p_{i \cdot} p_{\cdot j}\right)$$ 是对 $\boldsymbol{P}$ 的中心化, 对 $\boldsymbol{P}$ 的标准化是令 

$$\boldsymbol{Z}=\left(z_{i j}\right)$$ 

其中，

$$z_{i j}=\frac{p_{i j}-p_{i \cdot} \cdot p_{\cdot j}}{\sqrt{p_{i \cdot} \cdot p_{\cdot j}}}$$

记 $k=\operatorname{rank}(\boldsymbol{Z})$， 有 $k \leq \min \left(p-1,q-1\right)$。

设 $\lambda_{1}^{2} \geq \lambda_{2}^{2} \geq \cdots \geq \lambda_{k}^{2}>0$ 是 
$\boldsymbol{Z} \boldsymbol{Z}^{\prime}$ 的正特征根。

$$\text{总惯量}=\sum_{i=1}^{p} \sum_{j=1}^{q} \frac{\left(p_{i j}-p_{i \cdot} p_{\cdot j}\right)^{2}}{p_{i \cdot} \cdot p_{\cdot j}}=\sum_{i=1}^{p} \sum_{j=1}^{q} z_{i j}^{2}=\operatorname{tr}\left(\boldsymbol{Z} \boldsymbol{Z}^{\prime}\right)=\sum_{i=1}^{k} \lambda_{i}^{2}$$  

## 行、列轮廓的坐标  

### 数据的中心和变差

在第 $i$ 坐标轴上，$0$ 既可以看作是 $p$ 个行点坐标 $x_{1 i}, x_{2 i}, \cdots, x_{p i}$ 的中心, 也可看作是 $q$ 个列点 坐标 $y_{1 i}, y_{2 i}, \cdots, y_{q i}$ 的中心。

$$\sum_{j=1}^{p} p_{j \cdot} x_{ji}^{2}=\sum_{j=1}^{q} p_{\cdot j} y_{ji}^{2}=\lambda_{i}^{2}, \quad i=1,2, \cdots, k$$

称 $\lambda_{i}^{2}$  为第 $i$ 主惯量或第 $i$ 惯量，$i=1,2, \cdots, k$。

可见, 在第 $i$ 坐标轴上，$\lambda_{i}^{2}$ 既反映了 $p$ 个行点坐标 $x_{1 i}, x_{2 i}, \cdots, x_{p i}$ 的变差，也反映了 $q$ 个列点坐标 $y_{1 i}, y_{2 i}, \cdots, y_{q i}$ 的变差。
  
## 对应分析图
  
### 对应分析图的构建  
  
当 $\sum_{i=1}^{m}{\lambda_i}/\sum_{i=1}^{k}{\lambda_i}$ 足够大时，前 $m$ 维集中了足够多的关于列联表关联性或所有变差的信息，此时可将 $k$ 维坐标系降维成由其前 $m$ 维构成的坐标系进行对应分析。
  
* 出于作图的目的，我们通常取维数 $m=2$，偶尔取 $m=1$ 或 $m=3$。

* 可将各（中心化的）行轮廓和列轮廓在 $m$ 维坐标系上用点标出，并同时作到同一张图上。

### 行（列）点之间的距离
  
如果两个行（列）点越接近（远离），则表明相应的两个行（列）轮廓就越相似（不相似）。

### 行点和列点相近的含义

* 如果一个行点和一个列点相近，则表明行、列两个变量的相应类别组合发生的实际频数一般会高于这两个变量相互独立情形下的期望频数，也就意味着该行类别与该列类别相关联。

* 一般地，对于相近的行点和列点，它们离原点越远，其关联性就越强，也就是其类别组合的实际频数越会明显高于两变量独立情形下的期望频数。如果它们都在原点附近，则其关联性一般较弱、甚至可能几乎无关联性。
  
## 对应分析示例

对应分析使用程序包 ca 中的函数 ca。

### 示例 1

```{r}

# 绘制行轮廓马赛克图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  mosaicplot(x = mytab, shade = TRUE, margin = 1, xlab = "Sccial Status of Parents",
             ylab = "Psychological Health", main = "Cross Table")

  par(opar)
  
``` 

```{r}

# 绘制列轮廓马赛克图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  mosaicplot(x = mytab, shade = TRUE, margin = 2, xlab = "Sccial Status of Parents",
             ylab = "Psychological Health", main = "Cross Table")

  par(opar)

```

```{r}

# 绘制行列轮廓构成的马赛克图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  mosaicplot(x = mytab, shade = TRUE, margin = NULL, xlab = "Sccial Status of Parents",
             ylab = "Psychological Health", main = "Cross Table")

  par(opar)
  
```  
  
```{r}

# 读取数据
  attach(what = mydata)

# 对应分析：以列表的形式返回结果，包括（主）惯量、总惯量、贡献率及累计贡献率等
  library(ca)
  
  mydata.ca <- ca(obj = mytab)
  summary(object = mydata.ca)
  
  length(x = mydata.ca)
  names(x = mydata.ca)
  
  detach(name = mydata)

``` 

```{r}

# 提取奇异值：奇异值的平方就是（主）惯量值
  mydata.ca$sv
  
# 计算行点坐标：手动计算
  X <- mydata.ca$rowcoord %*% diag(x = mydata.ca$sv)
  
# 计算列点坐标：手动计算
  Y <- mydata.ca$colcoord %*% diag(x = mydata.ca$sv)

# 同时计算行点坐标与列点坐标
  coords <- cacoord(obj = mydata.ca, type = "principal")
  coords

``` 

```{r}

# 绘制对应分析图：使用行点坐标与列点坐标
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 0)
  
  plot(mydata.ca, lines = TRUE, mass = c(TRUE, TRUE), arrows = c(FALSE, FALSE),
       xlab = "First Inertia", ylab = "Second Inertia", main = "Corresponsence Plot")
  
  par(opar)
  
```

### 示例 2  

```{r}

# 读取数据
  USstates <- as.data.frame(x = state.x77)
  attach(what = USstates)  

# 对应分析 
  library(ca)
  USstates.ca <- ca(obj = USstates)
  summary(object = USstates.ca)
  
# 绘制二维对应分析图：使用行点坐标与列点坐标
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 0)
  
  plot(USstates.ca, lines = FALSE, mass = c(FALSE, FALSE), arrows = c(FALSE, FALSE),
       xlab = "First Inertia", ylab = "Second Inertia", main = "Corresponsence Plot")
  
  par(opar)
  
# 绘制三维维对应分析图
  plot3d.ca(x = USstates.ca)
  
  detach(name = USstates)

```

### 示例 3  

```{r}

# 读取数据
  data("caith", package = "MASS")
  
# 关联分析
  chisq.test(caith, correct = FALSE)

# 绘制马赛克图
  mosaicplot(caith, shade = TRUE, main = "Mosaic Plot")

# 对应分析
  library(ca)
  caith.ca <- ca(obj = caith)
  caith.ca

# 绘制二维对应分析图：使用行点坐标与列点坐标
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 0)
  
  plot(caith.ca, lines = TRUE, mass = c(TRUE, TRUE), arrows = c(FALSE, FALSE),
       xlab = "First Inertia", ylab = "Second Inertia", main = "Corresponsence Plot")
  
  par(opar)

```

从眼睛颜色与头发颜色关系看：

* 发色 fair，眼睛颜色多为 blue 和 light

* 发色 medium，眼睛颜色多为 medium

* 发色 black，眼睛颜色多 dark
  
### 示例 4

```{r}

# 创建数据
  mydata <- data.frame(herdsize = factor(rep(c('small', 'medium', 'large'), each = 3),
                                         levels = c('small', 'medium', 'large')),
                       disease = factor(rep(c('no', 'light', 'heavy'), 3),
                                        levels = c('no', 'light', 'heavy')),
                       counts = c(9, 5, 9,  18, 4, 19,  11, 88, 136))
  mydata

# 创建列联表
  mytab <- xtabs(counts ~ herdsize + disease, data = mydata)
  mytab

# 列联表独立性检验
  chisq.test(mytab, correct = TRUE)

# 对应分析
  library(ca)
  mydata.ca <- ca(obj = mytab)
  
# 绘制二维对应分析图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 0)
  
  plot(mydata.ca, lines = TRUE, mass = c(TRUE, TRUE), arrows = c(FALSE, FALSE),
       xlab = "First Inertia", ylab = "Second Inertia", main = "Corresponsence Plot")
  
  par(opar)  

```

图形解释:

* 第二因子作用很小。

* 病情轻、重很接近，与无并请方向相距很远，是合理的。

* 大型牛场倾向于有病；中、小型牛场倾向于无病。
  
# 典型相关分析 

变量之间的相关性的研究方法包括：

* 用相关系数衡量两个随机变量（或其样本）的线性相关性的方向和强弱。

* 对于一个因变量与多个自变量相关的情研究，使用回归分析建立自变量的线性组合使其最好地描述因变量，并用复相关系数的平方$R^2$ 描述因变量与一组自变量线性组合的相关性强弱。
  
## 典型相关的定义 

Hotelling 于 1935 年首先提出了典型相关（Canonical Correlations）的思想。

相关是指一对列变量 $x$ 与 $y$ 之间的相关性，如果将这种相关性一般化为一组列变量 $X=\{x1, x2,..., xn\}$ 与另一组列变量 $Y = \{y1, y2,..., ym\}$ 之间的相关性，这种相关性就称为典型相关。

```{r}

# 相关分析的基本架构
  img <- readPNG(source = "Pictures/相关分析的基本架构.png")
  grid.raster(image = img)
  
```
  
### 典型相关分析的应用场景

* 在工厂里，考察产品的 $q$ 个质量指标 $y_1,y_2,\cdots,y_q$ 与原材料的 $p$ 个质量指标 $x_1,x_2,\cdots,x_p$ 之间的相关关系。

* 牛肉、猪肉的价格与按人口平均的牛肉、猪肉的消费量之间的相关关系。

* 初一学生的阅读速度、阅读理解能力与数学运算速度、数学运算准确率才能之间的相关关系。

* 硕士研究生入学考试的各科成绩与本科阶段一些主要课程成绩之间的相关关系。

* 一组政府政策变量与一组经济目标变量之间的相关关系。

## 典型相关的原理
 
典型相关是找到一对权重向量 $a$ 和 $b$ 以最大化线性组合 $a^\prime{X}$ 与 $b^\prime{Y}$ 之间的相关系数，其中 $a^\prime{x_i}$ 与 $b^\prime{y_i}$ 称为第 $i$ 个观测样例的典型得分（canonical scores）。因此，多元数据也就可以在尽量不损失数据信息的条件下降维为由 $(x_i, y_i)$ 组成的二元数据。

在典型相关分析中，对每个观测样例来说，权重向量 $a$ 和 $b$ 都是相同的。

具体来说，可将线性变换表示为：
 
$$U=a_{1} x_{1}+a_{2} x_{2}+\cdots+a_{p} x_{p}$$ 
  
$$V=b_{1} y_{1}+b_{2} y_{2}+\cdots+b_{q} y_{q}$$  

$U$ 和 $V$ 称为典型变量。

为了计算 $U$ 和 $V$ 需要将数据划分为分块相关矩阵。

```{r}

# 分块相关矩阵
  img <- readPNG(source = "Pictures/分块相关矩阵.PNG")
  grid.raster(image = img)
  
```

矩阵 $B^{-1}C^{T}A^{-1}C$ 的特征根为 $\lambda_1>\lambda_2>\ldots>\lambda_p$，特征根对应的特征向量 $b_1,b_2,\ldots,b_p$ 就是随机向量 $Y$ 的系数 $b$，$A^{-1}C{b}$ 就是随机向量 $X$ 的系数。

$$\left[\operatorname{corr}\left(\boldsymbol{u}_{\boldsymbol{i}}, \boldsymbol{v}_{\boldsymbol{i}}\right)\right]^{2}=\lambda_{i}$$

### 典型变量的性质 

* 每一对典型变量 $U_i$ 和 $V_i$ $i=1,2,\ldots,p$ 的标准差为 $1$。

* $U_i$ 之间彼此不相关，$V_i$ 之间也彼此不相关，但是 $U_i$ 和 $V_i$ 之间是彼此相关的。

## 示例

### 示例 1

```{r}

# 读取数据
  data("LifeCycleSavings", package = "datasets")
  anyNA(x = LifeCycleSavings)

# 计算相关系数矩阵
  corr.mat <- cor(LifeCycleSavings, method = "pearson")
  corr.mat
  
# 绘制相关系数图
  library(corrgram)
  corrgram(x = LifeCycleSavings, type = "data", cor.method = "pearson", cex.labels = 1, 
           lower.panel = panel.shade, upper.panel = panel.pie, text.panel = panel.txt,  
           diag.panel = panel.minmax, main = "相关系数图") 

``` 

在进行典型相关分析之前，首先需要对数据进行标准化。

在 R 中，典型相关分析使用程序包 stats 中的函数 cancor。

在典型相关分析中，所保留的典型变量个数越少越好，最好能只保留一对典型变量。

* 如果一对也没有，说明两组变量之间不存在线性相关关系。

* 作为降维结果的各个典型变量的含义没有很好的解释。

```{r}

# 读取数据
  LifeCycleSavings_std <- datawizard::standardize(x = LifeCycleSavings)
  attach(what = LifeCycleSavings_std)
  
# 创建 X 矩阵 
  X <- LifeCycleSavings_std[, 2:3]
  
# 创建 Y 矩阵 
  Y <- LifeCycleSavings_std[, c(1, 4, 5)]  
  
# 典型相关分析：以列表显示返回结果
  LifeCycleSavings.cancor <- cancor(x = X, y = Y, xcenter = TRUE, ycenter = TRUE)  
  LifeCycleSavings.cancor
  
  detach(name = LifeCycleSavings_std)

```

```{r}

# 提取典型相关系数
  LifeCycleSavings.cancor$cor

# 提取 X 矩阵的系数
  LifeCycleSavings.cancor$xcoef

# 提取 Y 矩阵的系数
  LifeCycleSavings.cancor$ycoef

# 提取 X 矩阵的中心
  LifeCycleSavings.cancor$xcenter

# 提取 Y 矩阵的中心
  LifeCycleSavings.cancor$ycenter

```

输出结果表明：

* $z_{X_1}$ and $z_{Y_1}$ 之间的相关系数为 $0.8247966$，其中  
  
$$z_{u_1} = −0.08338007\times{x_{pop15}} + 0.06279282\times{x_{pop75}}$$

$$z_{v_1} = 0.03795363\times{sr} + 0.129546\times{dpi} + 0.01196908\times{ddpi}$$

```{r}

# 计算第一对（样本）典型变量得分
  xcoef <- as.numeric(x = LifeCycleSavings.cancor$xcoef[, 1])
  ycoef <- as.numeric(x = LifeCycleSavings.cancor$ycoef[, 1])
  u1 <- as.matrix(x = X) %*% xcoef
  v1 = as.matrix(x = Y) %*% ycoef

```   

```{r}

# 绘制典型变量之间的散点图：方法 1
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(x = u1, y = v1, panel.first = grid(), type = "p", col = "blue", 
       pch = 20, xlab = "First Canonical Variable", ylab = "First Canonical Variable", 
       main = "Scatter Plot for First Two Canonical Variables")
  maptools::pointLabel(x = u1, y = v1, labels = rownames(x = LifeCycleSavings), 
                       cex = 0.8, col = "steelblue", method = "SANN")
  abline(h = 0, v = 0, col = "gray50", lty = 2)
  
  par(opar)
  
```

```{r}

# 绘制典型变量之间的散点图：方法 2
  ggplot(mapping = aes(x = u1, y = v1)) +
    geom_point(color = "blue") +
    geom_hline(yintercept = 0, col = "gray50") +
    geom_vline(xintercept = 0, col = "gray50") +
    labs(x = "First Canonical Variable", y = "First Canonical Variable",
         title = "Scatter Plot for First Two Canonical Variables") +
    theme_bw()
  
```  

### 示例 2

某康复俱乐部对 $20$ 名中年人测量了三个生理指标：体重（x1）、腰围（x2）、脉搏（x3）和三个训练指标：引体向上（y1）、起坐次数（y2）、跳跃次数（y3）。

```{r}

# 读取数据
  health_train <- rio::import(file = "Data/examp10.3.1.csv")
  anyNA(x = health_train)
  attach(what = health_train)
  
# 分组计算相关系数矩阵
  corr.x <- cor(health_train[, 1:3], method = "pearson")
  corr.x
  
  corr.y <- cor(health_train[, 4:6], method = "pearson")
  corr.y

  detach(name = health_train)
  
```

典型相关分析还可以使用专门处理典型相关分析的程序包 CCA 中的函数 cc。

```{r}

# 读取数据
  health_train <- datawizard::standardize(x = health_train)
  attach(what = health_train)

# 典型相关分析：以列表显示返回结果
  library(CCA)  
  health_train.cancor <- cc(X = health_train[, 1:3], Y = health_train[, 4:6])
  health_train.cancor

  detach(name = health_train)

```   

```{r}
  
# 绘制典型变量之间的散点图：方法 1
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)
  
  plot(x = health_train.cancor$scores$xscores[, 1], 
       y =  health_train.cancor$scores$yscores[, 1],
       panel.first = grid(), type = "p", col = "blue", pch = 20,
       xlab = "First Canonical Variable", ylab = "First Canonical Variable", 
       main = "Scatter Plot for First Two Canonical Variables")
  maptools::pointLabel(x = health_train.cancor$scores$xscores[, 1], 
                       y =  health_train.cancor$scores$yscores[, 1],
                       labels = rownames(x = health_train), 
                       cex = 0.8, col = "steelblue", method = "SANN")
  abline(h = 0, v = 0, col = "gray50", lty = 2)
  
  par(opar)
  
```  

```{r}

# 绘制典型变量之间的散点图：方法 2
  ggplot(mapping = aes(x = health_train.cancor$scores$xscores[, 1], 
                       y = health_train.cancor$scores$yscores[, 1])) +
    geom_point(color = "blue") +
    geom_smooth(formula = y ~ x, method = "loess", color = "red", se = FALSE) +
    geom_hline(yintercept = 0, col = "gray50") +
    geom_vline(xintercept = 0, col = "gray50") +
    labs(x = "First Canonical Variable", y = "First Canonical Variable",
         title = "Scatter Plot for First Two Canonical Variables") +
    theme_bw()
  
```   
  
绘制所有原始变量与前两对典型变量得分的相关系数的散点图，横坐标是第一典型变量的权重，纵坐标是第二典型的权重。自变量为红色，因变量为蓝色。

```{r}

# 绘制所有原始变量与前两对典型变量得分的相关系数的散点图
  plt.cc(res = health_train.cancor, d1 = 1, d2 = 2, type = "v", int = 0.8)
  plt.cc(res = health_train.cancor, d1 = 1, d2 = 2, type = "i", int = 0.8)
  plt.cc(res = health_train.cancor, d1 = 1, d2 = 2, type = "b", int = 0.8)

```   

变量权重图的说明：

* 位于内圈之中的变量一般含有较小的权重，因此对典型相关分析的作用不大。
  
* 位于外圈之中的变量一般含有较大的权重，因此对典型相关分析具有较大的作用。

### 示例 3
 
在研究组织结构对“职业满意度”的影响时，作为其中一部分，邓讷姆（Dunham）调查了职业满意度与职业特性相关的程度。对从一大型零售公司各分公司选出的 $n=784$ 个行政人员，测量了 $p=5$ 个职业特性变量：用户反馈（x1）、任务重要性（x2）、任务多样性（x3）、任务特性（x4）及自主权（x5）和 $q=7$ 个职业满意度量：主管满意度（y1）、事业前景满意度（y2）、财务满意度（y3）、工作强度满意度（y4）、公司地位满意度（y5）、工种满意度（y6）及总体满意度（y7）。

```{r}

# 读取数据
  data("CHFLS", package = "HSAUR2")
  anyNA(x = CHFLS)
  
 # 绘制数据缺失值图 
  library(DescTools)
  PlotMiss(x = CHFLS, col = "red", bg = "steelblue", clust = FALSE, 
           main = "Plot for Missing Data")
  
# 创建不含缺失值的数据
  CHFLS_nona <- dplyr::filter(.data = CHFLS, A_edu != "NA") 
  anyNA(x = CHFLS_nona)
  
# 对数据标准化
  CHFLS_std <- datawizard::standardize(x = CHFLS_nona)
  
```

```{r}

# 读取数据
  attach(what = CHFLS_std)

# 典型相关分析：以列表显示返回结果
  library(CCA)  
  CHFLS.cancor <- cc(X = CHFLS_std[, c(2, 4, 6)], Y = CHFLS_std[, c(8, 10)])
  CHFLS.cancor
  
  detach(name = CHFLS_std)

```

```{r}

# 绘制典型变量之间的散点图：方法 1
  ggplot(mapping = aes(x = CHFLS.cancor$scores$xscores[, 1], 
                       y = CHFLS.cancor$scores$yscores[, 1])) +
    geom_point(mapping = aes(color = CHFLS_std$R_region)) +
    geom_smooth(formula = y ~ x, method = "loess", color = "red", se = FALSE) +
    geom_hline(yintercept = 0, col = "gray50") +
    geom_vline(xintercept = 0, col = "gray50") +
    labs(x = "First Canonical Variable", y = "First Canonical Variable",
         title = "Scatter Plot for First Two Canonical Variables", color = "Region") +
    theme_bw()

``` 

## 对典型变量系数的解释
  
对于如何最好地解释典型变量，一直存在争议：
 
* 使用主成分分析中的方法来解释系数，但是这种解释无法解决共线性问题。
  
* 计算典型变量与原始变量之间的相关系数，但是这种解释忽略了变量之间的联合分布。

# 多维标度分析

标度变换方法是由 Shepard 和 Kruskal 等人提出的。

## 多维标度的定义

多维标度（Multidimensional scaling）的首要目标：在最小化由降维导致的信息损失的条件下，将原始数据“拟合”到一个低维坐标系中。虽然主成分、因子分析也能够降维，但是关注的保留信息不同。多维标度分析关注的是保留数据中原有的距离信息或相似度信息。

* 例如： 在将数据维降维到 $1,2,3$ 维时，多维标度法会尽可能保持距离信息的不变。地图上的城市之间的距离并不是车程或时间距离，重新绘图使其反映两两之间的车程； 在仅知道一组城市彼此哪是最近，哪是次近等信息的情况下，作图表示这些关系。

```{r}

# 美国城市距离Kruskal stress 判断准则
  img <- readPNG(source = "Pictures/city_distance.png")
  grid.raster(image = img)

```

* “城市”可以换成“产品”、“品牌”、“指标”等。

在多维标度分析中，使用欧氏距离来测量数据点之间的相似度。由于数据的降维必然会损失原始数据中的部分信息，因此需要衡量降维后数据间的相似度与原始数据间的相似度的近似程度，这种近似程度的数值表示就称为 stress。
  
我们可以对 $N$ 个观测对象之间的相似度（欧式距离）进行排序，从而得到 $N(N-1)/2$ 个秩排序（rank order）。

* 如果仅使用秩排序进行几何展示，就称为非度量性多维标度（nonmetric multidimensional scaling）。

* 如果使用原始相似度的实际数值进行几何展示，就称为度量性多维标度（metric multidimensional scaling），也称为主坐标分析（principal coordinate analysis）。

## 计算方法  

对于 $N$ 个观测样例，可计算出 $N(N-1)/2$ 个两两之间的相似度（欧式距离），这些相似度构成了多维标度分析的基本数据。

假定这些相似度数据之间没有结（tie）的存在，即各个相似度都是不等的，那么可将相似度按照升序由小到大排序为：

$$S_{i_{1} k_{1}}<S_{i_{2} k_{2}}<\cdots<S_{i_{M} k_{M}}$$  

其中，$S_{i_{1} k_{1}}$ 为相似度最小的，也就意味着最不相似。下标 $i_{1}k_{1}$ 表示最不相似的那一对观测对象，即在相似性排序中等级为 $1$ 的观测对象。

我们的目标是在对数据进行降维后，降维后的数据点之间的相似度排序依然能够保持原始数据之间的排序。对此，Kruskal 提出了一个降维后数据排序偏离原始排序程度的指标，称为 stress：

$$\operatorname{Stress}(q)=\left\{\frac{\sum_{i<k}\left(d_{i k}^{(q)}-\hat{d}_{i k}^{(q)}\right)^{2}}{\sum_{i<k} \sum_{i k}\left[d_{i k}^{(q)}\right]^{2}}\right\}^{1 / 2}$$ 

Kruskal stress 判断准则为：

```{r}

# Kruskal stress 判断准则
  img <- readPNG(source = "Pictures/Kruskal stress 判断准则.png")
  grid.raster(image = img)
  
```

第二种对偏离度的度量是由 Takane 等人提出的，它是目前使用最多的准则：

$$\text { SStress }=\left[\frac{\sum_{i<k} \sum_{i<k}\left(d_{i k}^{2}-\hat{d}_{i k}^{2}\right)^{2}}{\sum_{i<k} \mathrm{~d}_{i k}^{4}}\right]^{1 / 2}$$

* SStress 的取值在 $0\sim{1}$ 之间，如果 $SStress < 0.1$ 就意味着降维后的数据能够很好地代表原始数据。

多维标度分析可归纳成以下两个步骤：
  
* 第一步：对 $N$ 个观测对象计算 $M=N(N-1)/2$ 个相似度（欧式距离），然后将这些相似度按照升序由小到大排序。

* 第二步：使用一个尝试性 $q$ 维点结构计算各点之间距离，不断迭代改进 $q$ 值，直至 $q$ 值能够得到最优 stress。

在 R 中，度量性多维标度分析使用程序包 stats 中的函数 cmdscale，其中：

* 参数 d 表示距离矩阵

* 参数 k 表示数据降维后的最大维度 

## 示例

```{r}

# 读取数据
  data("USArrests", package = "datasets")
  anyNA(x = USArrests)
  attach(what = USArrests)  
  
# 计算欧氏距离 
  dists <- dist(x = USArrests, method = "euclidean")  

# 多维标度分析：以列表形式返回结果
  USArrests.mds <- cmdscale(d = dists, k = 2, eig = TRUE)
  USArrests.mds
  
  detach(name = USArrests)

```  
  
```{r}

# 提取降维后的数据点：以矩阵的形式返回结果
  datapoints <- USArrests.mds$points
  dim(x = datapoints)
  
# 提取特征根：以向量的形式返回结果，向量长度等于原始数据中的行数
  eigens <- USArrests.mds$eig
  eigens

# 提取模型的拟合优度
  fits <- USArrests.mds$GOF
  fits
  
```

```{r}

# 绘制降维后的数据散点图：方法 1
  ggplot(data = as.data.frame(x = datapoints), mapping = aes(x = V1, y = V2)) +
    geom_point(color = "blue") +
    geom_hline(yintercept = 0, color = "gray50") +
    geom_vline(xintercept = 0, color = "gray50") +
    labs(x = "Coordinate 1", y = "Coordinate 2",
         title = "Two-dimensional MDS for USArrests") +
    theme_bw()
  
```  
  
```{r}

# 绘制降维后的数据散点图：方法 2
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)

  plot(datapoints, panel.first = grid(), type = "p", pch = 20, col = "blue", 
       xlab = "Coordinate 1", ylab = "Coordinate 2", 
       main = "Two-dimensional MDS for USArrests")
  maptools::pointLabel(x = datapoints[, 1], y = datapoints[, 2], cex = 0.8, col = "red", 
                       labels = rownames(x = USArrests), method = "SANN", 
                       allowSmallOverlap = FALSE)
  abline(h = 0, v = 0, col = "gray50", lty = 1)
  
  par(opar)

```
  
在 R 中，非度量性多维标度分析使用程序包 MASS 中的函数 isoMDS，其中：

* 参数 d 表示距离矩阵

* 参数 k 表示数据降维后的最大维度    
 
```{r}

# 读取数据
  data("voting", package = "HSAUR2")
  anyNA(x = voting)
  
# 非度量性多维标度分析  
  library(MASS)
  vote.mds <- isoMDS(d = voting, k = 2)
  
# 提取数据点：以矩阵的形式返回结果
  datapoints <- vote.mds$points
  
# 提取 stress：输出结果为百分比
  stress <- vote.mds$stress
  stress  

```    

```{r}

# 绘制降维后的数据散点图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)

  plot(datapoints, panel.first = grid(), type = "p", pch = 20, col = "blue", 
       xlab = "Coordinate 1", ylab = "Coordinate 2", 
       main = "Two-dimensional MDS for House of Representatives Voting")
  maptools::pointLabel(x = datapoints[, 1], y = datapoints[, 2], cex = 0.8, col = "red", 
                       labels = rownames(x = voting), method = "SANN", 
                       allowSmallOverlap = FALSE)
  abline(h = 0, v = 0, col = "gray50", lty = 1)
  
  par(opar)
  
```

```{r}

# 读取数据
  load(file = "Data/wwiileaders.RData")
  wwiileaders <- as.matrix(x = wwiileaders)
  
# 非度量性多维标度分析
  library(MASS)
  wwiileaders.mds <- isoMDS(d = wwiileaders, k = 2)
  
# 提取数据点：以矩阵的形式返回结果
  datapoints <- wwiileaders.mds$points

# 绘制降维后的数据散点图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)

  plot(datapoints, panel.first = grid(), type = "p", pch = 20, col = "blue", 
       xlab = "Coordinate 1", ylab = "Coordinate 2", 
       main = "Two-dimensional MDS for Leaders of II World War")
  maptools::pointLabel(x = datapoints[, 1], y = datapoints[, 2], cex = 0.8, col = "red", 
                       labels = rownames(x = wwiileaders), method = "SANN", 
                       allowSmallOverlap = FALSE)
  abline(h = 0, v = 0, col = "gray50", lty = 1)
  
  par(opar)  

```

```{r}

# 读取数据
  mydata <- rio::import(file = "Data/d3_1.xlsx")
  attach(what = mydata)
  
# 计算距离矩阵
  dists <- dist(x = mydata, method = "euclidean")

# 度量性多维标度分析  
  mydata.mds <- cmdscale(d = dists, k = 2, eig = TRUE)
  
# 提取数据点：以矩阵的形式返回结果
  datapoints <- mydata.mds$points
  

# 绘制降维后的数据散点图
  opar <- par(no.readonly = TRUE)
  par(cex = 0.8, mar = c(6, 6, 4, 4), las = 1)

  plot(datapoints, panel.first = grid(), type = "p", pch = 20, col = "blue", 
       xlab = "Coordinate 1", ylab = "Coordinate 2", 
       main = "Two-dimensional MDS for Consuming Expenditures of China")
  maptools::pointLabel(x = datapoints[, 1], y = datapoints[, 2], cex = 0.8, col = "red", 
                       labels = mydata$地区, method = "SANN", 
                       allowSmallOverlap = FALSE)
  abline(h = 0, v = 0, col = "gray50", lty = 1)
  
  par(opar)  
  
  detach(name = mydata)

``` 

# 联合分析
  
## 联合分析概述

联合分析（Conjoint analysis）是多元数据分析的主要工具之一，在社会学、生物统计学、数量心理学、市场营销、产品管理、运筹学等领域有着广泛的应用。联合分析尤其适用于市场营销研究，用于分析产品的多个属性对消费者购买决策的影响。

在设计产品时，人们总是希望了解产品的不同属性给消费者带来的不同效用，或者了解市场营销和广告策略的各个维度对产品销售的总体效用。例如，设计一款新汽车时，就希望知道消费者对汽车安全性、燃油经济性以及舒适性的偏好，以及汽车的这些属性对消费者最终购买决策的影响。 

## 试验设计

在联合分析中，产品被定义为产品各个维度属性的组合。产品属性的每一种组合称为一个激励（stimulus）或称为轮廓（剖面，profile）。

```{r}

# 读取数据
  icecream <- rio::import(file = "Data/icecream.xlsx")
  anyNA(x = icecream)
  
# 将数据转换为长数据
  icecream <- icecream %>% 
    pivot_longer(cols = starts_with("Individual"), names_to = "respondent", values_to = "rating") %>% 
    dplyr::rename("profile" = "Observations") %>% 
    mutate(profile = factor(x = profile), 
           respondent = factor(x = respondent),  
           Flavor = factor(x = Flavor), 
           Packaging = factor(x = Packaging), 
           Light = factor(x = Light), 
           Organic = factor(x = Organic)) 
  
# 查看数据维度
  dim(x = icecream)
  
# 查看数据
  slice_sample(.data = icecream, n = 3)

```  

从数据中可以看出：

* 维度 Flavor 有 aspberry、chocolate、strawberry、mango、vanilla 五种口味

* 维度 Packaging 有 homemade waffle、cone、pint 三种包装
 
* 维度 Light 有 low fat 和 not 两种脂肪含量

* 维度 Organic 有 organic 和 not 两种水平

因此产品的属性组和（profile）共计 $5\times 3 \times 2 \times 2 = 60$ 种。
  
为了确定消费者对各种 profile 的评价，需要做总计 $60$ 种的全因子试验（full factorial experiments）。但是，由于全因子试验过于复杂，因此我们需要从全因子试验中找出一个子集，即部分因子试验（fractional factorial experiments）。

使用程序包 radian 中的函数 doe 进行因子试验设计：

```{r}

# 创建产品属性
  attribute1 <- "Flavor; Raspberry; Chocolate; Strawberry; Mango; Vanilla"
  attribute2 <- "Package; Homemade waffle; Cone; Pint"
  attribute3 <- "Light; Low fat; No low fat"
  attribute4 <- "Organic; Organic; Not organic"
  
  attributes <- c(attribute1, attribute2, attribute3, attribute4)

# 生成试验设计
  library(radiant)
  exps <- doe(factors = attributes, seed = 123)
  summary(object = exps)
  names(x = exps)

```
  
**输出结果 Design efficiency 的解释**
  
* Trials：试验中 profile 的数量

* D-efficiency：试验的效果，它表示能够清楚获得试验效果的可能性。D-efficiency 的取值越大，试验效果越好。  

* Balanced：是否为平衡实验。如果试验中每个维度的每个水平出现的频数相等，称为平衡试验。

* 理想情况下，应该选择 D-efficiency 取值高（大于 $0.8$）且平衡的试验。

```{r}

# 试验的筛选
  exps_sub <- dplyr::filter(.data = exps$eff, Balanced == TRUE)
  exps_sub

```

```{r}

# 查看 trials = 30 的试验摘要信息
  summary(object = doe(factors = attributes, seed = 123, trials = 30))

```

## One respondent

### Estimate part-worths and importance weights

```{r}

# 筛选数据
  respondent1 <- dplyr::filter(.data = icecream, respondent == "Individual 1")
  dim(x = respondent1)
  head(x = respondent1, n = 3)

# 联合分析
  library(radiant.basics)
  conjoint_respondent1 <- conjoint(dataset = respondent1, 
                                   rvar = "rating", evar = names(x = respondent1)[2:5])
  summary(object = conjoint_respondent1)
  
# 对联合分析绘图
  plot(conjoint_respondent1) 

```

**输出结果的解释**

* part-worths 与 regression coefficient 给出的是相同的信息：与参考基准水平（基准水平设为 $0$）相比，每个维度的每个水平对产品效用的贡献。

### Profiles: predicted utilities

```{r}

# 预测产品效用：针对参加测试的产品
  profiles <- icecream %>% 
    dplyr::filter(respondent == "Individual 1") %>% 
    dplyr::select(Flavor, Packaging, Light, Organic)
  profiles

  predictions <- predict(object = conjoint_respondent1, pred_data = profiles)
  class(x = predictions)
  
  predictions <- predictions %>% 
    dplyr::arrange(desc(x = Prediction))
  
  predictions

```   

```{r}

# 预测产品效用：针对全部产品
  Flavor <- c("Raspberry", "Chocolate", "Mango", "Strawberry", "Vanilla")
  Organic <- c("Organic", "Not organic")
  
  profiles.all <- expand.grid(levels(x = icecream$Flavor), levels(x = icecream$Packaging),
                              levels(x = icecream$Light), levels(x = icecream$Organic)) %>%
    rename(Flavor = "Var1", Packaging = "Var2", Light = "Var3", Organic = "Var4")

  predict(object = conjoint_respondent1, pred_data = profiles.all) %>%
    dplyr::arrange(desc(x = Prediction)) 

```

## Many respondents

### Estimate part-worths and importance weights

```{r}

# 联合分析
  conjoint_allrespondents <- conjoint(dataset = icecream, rvar = "rating", 
                                      evar = c("Flavor", "Packaging", "Light", "Organic"))  
  summary(object = conjoint_allrespondents)

# 对联合分析绘图
  plot(x = conjoint_allrespondents)  

```

### Profiles: predicted utilities

```{r}

# 对全部数据进行预测
  predict(object = conjoint_allrespondents, pred_data = profiles.all) %>%
    dplyr::arrange(desc(x = Prediction))

```   

## Market simulation

```{r}

# 创建数据
  market_profiles <- profiles.all %>%
    slice(c(4, 16, 23, 38)) 
  market_profiles  

# 联合分析：针对全体
  conjoint_allrespondents <- conjoint(dataset = icecream, rvar = "rating", 
                                      evar = c("Flavor", "Packaging", "Light", "Organic"))

  predict(conjoint_allrespondents, market_profiles) %>%
    dplyr::arrange(desc(Prediction))
  
```

```{r}

# 联合分析：针对个体
  conjoint_perrespondent <- conjoint(icecream, rvar = "rating", 
                                     evar = c("Flavor", "Packaging", "Light", "Organic"), 
                                     by = "respondent")

  predict(conjoint_perrespondent, market_profiles) %>%
    dplyr::arrange(respondent, desc(Prediction))
 
```  
